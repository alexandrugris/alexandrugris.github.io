<!DOCTYPE html>
<html lang="en">

  <head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <title>Statistics, continued</title>
  <meta name="description" content="This post tackles basic algorithms for computing probability density functions and cumulative distribution functions, as well as generating random numbers ac...">

  <link rel="stylesheet" href="/assets/main.css">
  <link rel="canonical" href="https://alexandrugris.github.io/machine/learning/2017/03/11/MachineLearning-Notebook-2.html">
  <link rel="alternate" type="application/rss+xml" title="From The Trenches - The Code" href="/feed.xml">
  
  
</head>


  <body>

    <header class="site-header" role="banner">

  <div class="wrapper">

    <div class="site-nav"><a class="site-title" href="/">From The Trenches - The Code</a> </div>

    <nav class="site-nav">
      <span class="menu-icon">        
      </span>

      <div class="trigger">
        
          
          <a class="page-link" href="/about/">About</a>
          
        
          
        
          
        
          
          <a class="page-link" href="/sm/">Social Media</a>
          
        
          
        
          
        
      </div>
    </nav>

  </div>

</header>


    <main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title" itemprop="name headline">Statistics, continued</h1>
    <p class="post-meta"><time datetime="2017-03-11T14:15:16+02:00" itemprop="datePublished">Mar 11, 2017</time></p>
  </header>

  <div class="post-content" itemprop="articleBody">
    <p>This post tackles basic algorithms for computing probability density functions and cumulative distribution functions, as well as generating random numbers according to a distribution. I will compute the Gini index (G) and the entropy (H) coefficients, which are used to measure how much a distribution differs from the uniform distribution.</p>

<h3 id="generating-random-numbers-according-to-a-given-distribution">Generating random numbers according to a given distribution</h3>

<p>Let’s consider two distributions I will play arond with in this article:</p>

<ul>
  <li>
    <p><strong>Power law</strong> - sometimes referred as Pareto. Common in social systems or systems with strong network effects - wealth distribution, group size, productivity, website views. It models preferential attachment. Power law <a href="https://en.wikipedia.org/wiki/Probability_density_function">density function</a> is <code class="highlighter-rouge">p(a) ~= a / x^lambda</code> where <code class="highlighter-rouge">lambda</code> reflects the steepness of the fall. I am using <code class="highlighter-rouge">~=</code> because the integral on the (-infinity, +infinity) interval should be 1, thus the power law as expressed above is not really a density function. <a href="https://en.wikipedia.org/wiki/Power_law">Power Law, Wikipedia</a></p>
  </li>
  <li>
    <p><strong>Gaussian</strong> - common in physical systems, common in measurements or small random effect. <code class="highlighter-rouge">N(a, sigma) = C * e^[-(x-a)^2 / 2 * sigma^2]</code>, where <code class="highlighter-rouge">C</code> is a constant, <code class="highlighter-rouge">a</code> is the average and <code class="highlighter-rouge">sigma^2</code> the variance. 88% of the values fall between a+-sigma, 99.7% between a+-3*sigma. Z-scoring (<code class="highlighter-rouge">y=(x-a)/sigma</code>) brings the values to the N(0,1) standard form - useful as input for machine learning algorithms.</p>
  </li>
</ul>

<p>Let’s plot the <a href="https://en.wikipedia.org/wiki/Probability_density_function">Probability Density Function</a> and <a href="https://en.wikipedia.org/wiki/Cumulative_distribution_function">Cumulative Distribution Function</a> of a normally (Gaussian) distributed random variable:</p>

<div class="language-python highlighter-rouge"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="kn">as</span> <span class="nn">plt</span>

<span class="n">x_axis</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">10</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">1000</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">gaussian</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">a</span><span class="p">,</span> <span class="n">sigma</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">e</span><span class="o">**</span><span class="p">(</span><span class="o">-</span><span class="p">(</span><span class="n">x</span><span class="o">-</span><span class="n">a</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span> <span class="o">/</span> <span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="n">sigma</span> <span class="o">**</span> <span class="mi">2</span><span class="p">))</span>

<span class="k">def</span> <span class="nf">pdf</span><span class="p">(</span><span class="n">x_step</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">y</span> <span class="o">/</span> <span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="nb">sum</span><span class="p">(</span><span class="n">y</span><span class="p">)</span> <span class="o">*</span> <span class="n">x_step</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">cdf</span><span class="p">(</span><span class="n">y</span><span class="p">):</span>
    <span class="n">cdf_arr</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">empty_like</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>
    <span class="n">cdf_arr</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">y</span><span class="o">.</span><span class="n">size</span><span class="p">):</span>
        <span class="n">cdf_arr</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">cdf_arr</span><span class="p">[</span><span class="n">i</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="n">y</span><span class="p">[</span><span class="n">i</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
        
    <span class="k">return</span> <span class="n">cdf_arr</span> <span class="o">/</span> <span class="n">cdf_arr</span><span class="p">[</span><span class="n">cdf_arr</span><span class="o">.</span><span class="n">size</span> <span class="o">-</span> <span class="mi">1</span><span class="p">]</span>
    
<span class="k">def</span> <span class="nf">mean</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="nb">sum</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">sigma</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="mi">1</span><span class="o">/</span><span class="n">np</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="nb">sum</span><span class="p">((</span><span class="n">x</span><span class="o">-</span><span class="n">mean</span><span class="p">(</span><span class="n">x</span><span class="p">))</span><span class="o">**</span><span class="mi">2</span><span class="p">))</span>


<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x_axis</span><span class="p">,</span> <span class="n">pdf</span><span class="p">(</span><span class="n">x_axis</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">-</span> <span class="n">x_axis</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">gaussian</span><span class="p">(</span><span class="n">x_axis</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">)))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x_axis</span><span class="p">,</span> <span class="n">cdf</span><span class="p">(</span><span class="n">pdf</span><span class="p">(</span><span class="n">x_axis</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">-</span> <span class="n">x_axis</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">gaussian</span><span class="p">(</span><span class="n">x_axis</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">))))</span>
</code></pre>
</div>

<p><strong>Result:</strong></p>

<p><img src="https://alexandrugris.github.io/assets/ml_2_1.png" alt="PDF, CDF" /></p>

<p>In order to draw the PDF, one needs to normalize the function so that its integral is 1 on the (-infinite,+infinite) interval - see the <code class="highlighter-rouge">pdf</code> function above. I kept the <code class="highlighter-rouge">pdf</code>, <code class="highlighter-rouge">cdf</code>, mean and <code class="highlighter-rouge">sigma</code> functions generic, so that they can be applied to any distribution, although for the Gaussian, pdf and cdf analytic formulas already exist. The integrals are computed incrementally, thus errors tend to accumulate. For precise results or for production code analytic formulas should be preferred (if they exist).</p>

<p>Now, let’s compute a random variable distributed according to a specific probability density function.</p>

<p>Using the functions cdf and pdf from above, I will generate a set a randomly distributed numbers based on the power law distribution. First part generates the numbers. <code class="highlighter-rouge">func</code> - the function, <code class="highlighter-rouge">pdf_func</code> - probability density of func and <code class="highlighter-rouge">cdf_func</code> the cumulative distribution - please note that <code class="highlighter-rouge">cdf(func)</code> and <code class="highlighter-rouge">cdf(pdf(func))</code> produce similar results because <code class="highlighter-rouge">cdf</code> has normalization embedded.</p>

<div class="language-python highlighter-rouge"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">numpy.random</span> <span class="kn">as</span> <span class="nn">rnd</span>

<span class="n">x_axis</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">50</span><span class="p">,</span> <span class="mi">1000</span><span class="p">)</span>
<span class="n">func</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">/</span> <span class="p">(</span><span class="n">x_axis</span> <span class="o">+</span> <span class="mf">0.5</span><span class="p">)</span> <span class="o">**</span> <span class="mf">1.8</span>

<span class="n">pdf_func</span> <span class="o">=</span> <span class="n">pdf</span><span class="p">(</span><span class="n">x_axis</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">-</span> <span class="n">x_axis</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">func</span><span class="p">)</span>
<span class="n">cdf_func</span> <span class="o">=</span> <span class="n">cdf</span><span class="p">(</span><span class="n">func</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x_axis</span><span class="p">,</span> <span class="n">func</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x_axis</span><span class="p">,</span> <span class="n">pdf_func</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x_axis</span><span class="p">,</span> <span class="n">cdf</span><span class="p">(</span><span class="n">func</span><span class="p">))</span>
</code></pre>
</div>

<p><img src="https://alexandrugris.github.io/assets/ml_2_2.png" alt="PDF, CDF" /></p>

<div class="language-python highlighter-rouge"><pre class="highlight"><code>
<span class="k">def</span> <span class="nf">rand_cdf</span><span class="p">(</span><span class="n">array</span><span class="p">,</span> <span class="n">cdf_arr</span><span class="p">):</span>

    <span class="k">def</span> <span class="nf">inv_cdf</span><span class="p">(</span><span class="n">cdf_arr</span><span class="p">,</span> <span class="n">cnt</span><span class="p">):</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">empty</span><span class="p">(</span><span class="n">cnt</span><span class="p">)</span>
        <span class="n">prev_cdf_idx</span> <span class="o">=</span> <span class="mi">1</span>
    
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">cnt</span><span class="p">):</span>
            <span class="n">next_val</span> <span class="o">=</span> <span class="nb">float</span><span class="p">(</span><span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">/</span> <span class="nb">float</span><span class="p">(</span><span class="n">cnt</span><span class="p">)</span>
            
            <span class="k">while</span> <span class="n">cdf_arr</span><span class="p">[</span><span class="n">prev_cdf_idx</span><span class="p">]</span> <span class="o">&lt;</span> <span class="n">next_val</span><span class="p">:</span>
                <span class="n">prev_cdf_idx</span> <span class="o">=</span> <span class="n">prev_cdf_idx</span> <span class="o">+</span> <span class="mi">1</span>
                
            <span class="c"># a more accurate version would consider a linear interpolation</span>
            <span class="c"># between prev_cdf_idx - 1 and prev_cdf_idx</span>
            <span class="n">x</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">prev_cdf_idx</span> <span class="o">-</span> <span class="mi">1</span>
        
        <span class="k">return</span> <span class="n">x</span> <span class="o">/</span> <span class="n">cdf_arr</span><span class="o">.</span><span class="n">size</span>
    
    <span class="n">scale</span> <span class="o">=</span> <span class="mi">10000</span>
    
    <span class="n">cdf_inv</span> <span class="o">=</span> <span class="n">inv_cdf</span><span class="p">(</span><span class="n">cdf_arr</span><span class="p">,</span> <span class="n">scale</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">cdf_inv</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">array</span> <span class="o">*</span> <span class="p">(</span><span class="n">scale</span> <span class="o">-</span> <span class="mf">1.0</span> <span class="o">/</span> <span class="n">scale</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">integer</span><span class="p">)]</span> 

<span class="n">rand_0_1</span> <span class="o">=</span> <span class="n">rnd</span><span class="o">.</span><span class="n">random</span><span class="p">(</span><span class="mi">1000</span><span class="p">)</span>
<span class="n">rand_distributed_arr</span> <span class="o">=</span> <span class="n">rand_cdf</span><span class="p">(</span><span class="n">rand_0_1</span><span class="p">,</span> <span class="n">cdf_func</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">hist</span><span class="p">(</span><span class="n">rand_distributed_arr</span><span class="p">)</span>

</code></pre>
</div>
<p><img src="https://alexandrugris.github.io/assets/ml_2_3.png" alt="PDF, CDF" /></p>

<p>The <code class="highlighter-rouge">rand_cdf</code> takes as parameters two arrays - a uniform randomly distributed array with values between 0 and 1 and the CDF of the function I want my resulting random numbers to be distributed on. Then it computes the inverse of the CDF and uses the random numbers as indices in this array. Please see the CDF^(-1) below:</p>

<p><img src="https://alexandrugris.github.io/assets/ml_2_4.png" alt="Inverse CDF" /></p>

<p>As one can notice, x axis is between (0,1) and y axis between (0, 1000), where 1000 is the size of the random number array. In plain English, it basically reads: “for any number received as parameter between 0 and 0.8, I will output a very small number. For any number higher than 0.8, I will output a larger number.”. As my input is uniformly distributed between (0,1), The probability of outputting a smaller number is much higher than outputting a larger number. How much larger? It is precisely based on the CDF received as input.</p>

<h3 id="entropy-and-gini-index">Entropy and Gini index</h3>

<p><a href="https://en.wikipedia.org/wiki/Entropy_(information_theory)">Entropy</a></p>

<p>Entropy measures the amount of information in signals being transferred over a communication channel. A rare signal bares more information than a more frequent one and since the signals are considered to be independent of each other, they can be summed up to estimate the total amount of information. Because of these, we can choose <code class="highlighter-rouge">log(1/p) = log(p^-1) = -log(p)</code> for scoring the level of information in a signal which appears with probability <code class="highlighter-rouge">p</code>. We define entropy to be the averaged level of information in categories of a categorical feature.</p>

<p>Thus, considering a categorical feature with p1..pn probabilities of occurence:</p>

<p><code class="highlighter-rouge">Entropy = H = Sum(-pi log(pi))</code> and it is smaller than <code class="highlighter-rouge">log(n)</code> where n is the number of categories. H for a uniform distribution is <code class="highlighter-rouge">log(n)</code> and it is the maximum entropy.</p>

<p>According to <a href="https://en.wikipedia.org/wiki/Entropy_(information_theory)">Wikipedia</a>:</p>

<blockquote>
  <p>Entropy is a measure of unpredictability of the state, or equivalently, of its average information content. To get an intuitive understanding of these terms, consider the example of a political poll. Usually, such polls happen because the outcome of the &gt;poll is not already known. In other words, the outcome of the poll is relatively unpredictable, and actually performing the poll and learning the results gives some new information; these are just different ways of saying that the a priori entropy of the &gt;poll results is large. Now, consider the case that the same poll is performed a second time shortly after the first poll. Since the result of the first poll is already known, the outcome of the second poll can be predicted well and the results should not &gt;contain much new information; in this case the a priori entropy of the second poll result is small relative to that of the first.</p>
</blockquote>

<div class="highlighter-rouge"><pre class="highlight"><code>H of uniform distribution - m elements =&gt;
probability p = 1/m  = -Sum(1/p * log(p)) = -p * (1/p) * log(p) = log(1/p) = log(m)
</code></pre>
</div>

<p>The formulas above translate to the following python code:</p>

<div class="language-python highlighter-rouge"><pre class="highlight"><code><span class="k">def</span> <span class="nf">entropy</span><span class="p">(</span><span class="n">histogram</span><span class="p">):</span>
    <span class="n">mask</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">ma</span><span class="o">.</span><span class="n">masked_values</span><span class="p">(</span><span class="n">histogram</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">,</span> <span class="n">copy</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
    
    <span class="k">if</span> <span class="nb">type</span><span class="p">(</span><span class="n">mask</span><span class="o">.</span><span class="n">mask</span><span class="p">)</span> <span class="ow">is</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">:</span>
        <span class="n">histogram</span><span class="p">[</span><span class="n">mask</span><span class="o">.</span><span class="n">mask</span><span class="p">]</span> <span class="o">=</span> <span class="mf">1e-4</span> <span class="c"># so we don't have 1/0</span>
    
    <span class="n">prb</span> <span class="o">=</span> <span class="n">histogram</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="nb">sum</span><span class="p">(</span><span class="n">histogram</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="nb">sum</span><span class="p">(</span><span class="n">prb</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">log2</span><span class="p">(</span> <span class="mi">1</span> <span class="o">/</span> <span class="n">prb</span> <span class="p">))</span>

<span class="k">def</span> <span class="nf">max_entropy</span><span class="p">(</span><span class="n">histogram</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">log2</span><span class="p">(</span><span class="n">histogram</span><span class="o">.</span><span class="n">size</span><span class="p">)</span>
</code></pre>
</div>

<p>Example 1:</p>

<p>Compute entropy for a completely skewed distribution of 5 items, [1, 0, 0, 0, 0]</p>

<div class="language-python highlighter-rouge"><pre class="highlight"><code><span class="n">histogram</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="nb">float</span><span class="p">)</span>
<span class="k">print</span> <span class="p">(</span><span class="s">"Entropy: {} ; Max Entropy: {}"</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">entropy</span><span class="p">(</span><span class="n">histogram</span><span class="p">),</span> <span class="n">max_entropy</span><span class="p">(</span><span class="n">histogram</span><span class="p">)))</span>
</code></pre>
</div>

<p>With output:</p>

<div class="highlighter-rouge"><pre class="highlight"><code>Entropy: 0.0058899223994331295 ; Max Entropy: 2.321928094887362
</code></pre>
</div>

<p>Example 2:</p>

<p>Compute entropy for a gaussian distrbuted random variable split into 5 categories.</p>

<div class="language-python highlighter-rouge"><pre class="highlight"><code><span class="n">histogram</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">histogram</span><span class="p">(</span><span class="n">rand_gaussian_arr</span><span class="p">,</span> <span class="mi">5</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
<span class="k">print</span> <span class="p">(</span><span class="s">"Entropy: {} ; Max Entropy: {}"</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">entropy</span><span class="p">(</span><span class="n">histogram</span><span class="p">),</span> <span class="n">max_entropy</span><span class="p">(</span><span class="n">histogram</span><span class="p">)))</span>
</code></pre>
</div>

<p>With output:</p>

<div class="highlighter-rouge"><pre class="highlight"><code>Entropy: 1.8286832347936723 ; Max Entropy: 2.321928094887362
</code></pre>
</div>

<p>Example 3:</p>

<p>Plot the chart for the evolution of the entropy index for a set of probabilities [p, 1-p]</p>

<div class="language-python highlighter-rouge"><pre class="highlight"><code><span class="n">x_axis</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mf">0.01</span><span class="p">,</span> <span class="mf">0.99</span><span class="p">,</span> <span class="mi">100</span><span class="p">)</span>
<span class="n">e</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">empty_like</span><span class="p">(</span><span class="n">x_axis</span><span class="p">)</span>
<span class="n">e_idx</span> <span class="o">=</span> <span class="mi">0</span>

<span class="n">probs</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">empty</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>

<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">x_axis</span><span class="p">:</span>    
    <span class="n">probs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="n">i</span> 
    <span class="n">probs</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="mf">1.0</span> <span class="o">-</span> <span class="n">probs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">e</span><span class="p">[</span><span class="n">e_idx</span><span class="p">]</span> <span class="o">=</span> <span class="n">entropy</span><span class="p">(</span><span class="n">probs</span><span class="p">)</span>
    
    <span class="k">print</span> <span class="p">(</span><span class="n">probs</span><span class="p">,</span> <span class="n">e</span><span class="p">[</span><span class="n">e_idx</span><span class="p">])</span>
    
    <span class="n">e_idx</span> <span class="o">=</span> <span class="n">e_idx</span> <span class="o">+</span> <span class="mi">1</span>
    
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x_axis</span><span class="p">,</span> <span class="n">e</span><span class="p">)</span>
</code></pre>
</div>

<p>With the output:</p>

<p><img src="https://alexandrugris.github.io/assets/ml_2_5.png" alt="Entropy" /></p>

<p><a href="https://en.wikipedia.org/wiki/Gini_coefficient">Gini index</a></p>

<p>Gini index is referred also as categorical variance. It was initially introduced to “represent the income or wealth distribution of a nation’s residents, and is the most commonly used measure of inequality.” [Wikipedia]. We define it as a measurement for the average level of error for the method of the proportional classifier (proportional clasifier: given a feature which appears with a frequency f, its probability equals its frequency).</p>

<p>E.g., consider a category with frequency <code class="highlighter-rouge">p = 20%</code>, the average error is <code class="highlighter-rouge">p*(1-p) = 20% * 80% = 16%</code>. <code class="highlighter-rouge">Gmax = (m-1) / m</code> and is obtained for a uniform distribution. <code class="highlighter-rouge">G = sum( p_i * (1 - p_i)</code> ).</p>

<p>Same as before:</p>

<div class="language-python highlighter-rouge"><pre class="highlight"><code><span class="k">def</span> <span class="nf">gini_idx</span><span class="p">(</span><span class="n">histogram</span><span class="p">):</span>
    <span class="n">prb</span> <span class="o">=</span> <span class="n">histogram</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="nb">sum</span><span class="p">(</span><span class="n">histogram</span><span class="p">)</span>
    <span class="k">return</span> <span class="mf">1.0</span> <span class="o">-</span> <span class="n">np</span><span class="o">.</span><span class="nb">sum</span><span class="p">(</span><span class="n">prb</span> <span class="o">*</span> <span class="n">prb</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">gini_idx_max</span><span class="p">(</span><span class="n">histogram</span><span class="p">):</span>
    <span class="k">return</span> <span class="p">(</span><span class="n">histogram</span><span class="o">.</span><span class="n">size</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">/</span> <span class="n">histogram</span><span class="o">.</span><span class="n">size</span>
</code></pre>
</div>

<p>For Gaussian (5 elements):</p>

<div class="highlighter-rouge"><pre class="highlight"><code>histogram = np.histogram(rand_gaussian_arr, 5)[0]
print ("Gini: {} ; Max Gini: {}".format(gini_idx(histogram), gini_idx_max(histogram)))

Gini: 0.674266 ; Max Gini: 0.8
</code></pre>
</div>

<p>For skewed distribution:</p>

<div class="highlighter-rouge"><pre class="highlight"><code>histogram = np.array([1, 0, 0, 0, 0], dtype=np.float)
print ("Gini: {} ; Max Gini: {}".format(gini_idx(histogram), gini_idx_max(histogram)))
Gini: 0.0 ; Max Gini: 0.8
</code></pre>
</div>

<p>Evolution of Gini index (G) for a set of probabilities (p, 1-p):</p>

<p><img src="https://alexandrugris.github.io/assets/ml_2_6.png" alt="Gini Index" /></p>

<p>Mean, median, entropy index (H) and Gini index (G) are different aggregate indices (summaries) for a distribution. Because our sample size may not be large enough, they are subject to measurement errors. Therefore, if we need to improve our knowledge of the data, we can compute their distribution as well - which is Gaussian. If we take the mean as the parameter we want to investigate, we obtain the mean of the mean and the stddev of mean. But any other index can be considered. Two outstanding methods are bootstrapping and K-fold crossvalidation.</p>

<h3 id="bootstrapping">Bootstrapping</h3>

<p>Bootstrapping is a resampling method which works like this:</p>

<ol>
  <li>Consider M random trials and an entity set with N entities</li>
  <li>Randomly extract and put back N entities from the entity set =&gt; on average only <code class="highlighter-rouge">(e-1) / e = 63.2%</code> will be selected in a trial. (p of an entity not to be drawn = 1 - 1/N, we have N trials =&gt; p of not drawn for N trials is (1 - 1/N)^N -&gt; 1/e = 36% of all entities)</li>
  <li>Compute the desired index for the trial</li>
  <li>Summarize for all trials the indices into the mean / stddev form; plot a histogram.</li>
</ol>

<p>Here is the code for computing mean variation through bootstrapping. We will consider a power-law distribution for start, with 20 extractions.</p>

<p>Initialization:</p>

<div class="language-python highlighter-rouge"><pre class="highlight"><code><span class="n">x_axis</span>          <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">50</span><span class="p">,</span> <span class="mi">1000</span><span class="p">)</span>
<span class="n">powerlaw</span>        <span class="o">=</span> <span class="mi">1</span> <span class="o">/</span> <span class="p">(</span><span class="n">x_axis</span> <span class="o">+</span> <span class="mf">1.0</span><span class="p">)</span> <span class="o">**</span> <span class="mf">1.2</span>
<span class="n">rand_0_1</span>        <span class="o">=</span> <span class="n">rnd</span><span class="o">.</span><span class="n">random</span><span class="p">(</span><span class="mi">20</span><span class="p">)</span> <span class="c"># 20 extractions</span>
<span class="n">rand_powerlaw_arr</span> <span class="o">=</span> <span class="n">rand_cdf</span><span class="p">(</span><span class="n">rand_0_1</span><span class="p">,</span> <span class="n">cdf</span><span class="p">(</span><span class="n">powerlaw</span><span class="p">))</span>

<span class="n">plt</span><span class="o">.</span><span class="n">hist</span><span class="p">(</span><span class="n">rand_powerlaw_arr</span><span class="p">)</span> <span class="c"># just check the distribution is power-law-ish</span>
</code></pre>
</div>

<p><img src="https://alexandrugris.github.io/assets/ml_2_7.png" alt="First distribution" /></p>

<div class="language-python highlighter-rouge"><pre class="highlight"><code><span class="c"># prepare space for mean for 500 trials</span>
<span class="n">means</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">empty</span><span class="p">(</span><span class="mi">500</span><span class="p">)</span>

<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">means</span><span class="o">.</span><span class="n">size</span><span class="p">):</span> <span class="c">#500 trials</span>
    <span class="n">bootstrapped</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">empty_like</span><span class="p">(</span><span class="n">rand_powerlaw_arr</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">rand_powerlaw_arr</span><span class="o">.</span><span class="n">size</span><span class="p">):</span>
        <span class="n">bootstrapped</span><span class="p">[</span><span class="n">j</span><span class="p">]</span> <span class="o">=</span> <span class="n">rand_powerlaw_arr</span><span class="p">[</span><span class="nb">int</span><span class="p">(</span><span class="n">rnd</span><span class="o">.</span><span class="n">random</span><span class="p">()</span> <span class="o">*</span> <span class="n">rand_powerlaw_arr</span><span class="o">.</span><span class="n">size</span><span class="p">)]</span>
        
    <span class="n">means</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">bootstrapped</span><span class="p">)</span>
    
<span class="n">plt</span><span class="o">.</span><span class="n">hist</span><span class="p">(</span><span class="n">means</span><span class="p">)</span>

<span class="k">print</span><span class="p">(</span><span class="s">"Mean of distribution: {:.4f}, Mean boostrapped: {:.4f}, Stddev mean bootstrapped {:.4f} "</span>
      <span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">rand_powerlaw_arr</span><span class="p">),</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">means</span><span class="p">),</span> <span class="n">np</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="n">means</span><span class="p">)))</span>
</code></pre>
</div>

<p>And the output - shows the distribution of the mean is Gaussian:</p>

<p><img src="https://alexandrugris.github.io/assets/ml_2_8.png" alt="First distribution" /></p>

<div class="highlighter-rouge"><pre class="highlight"><code>Mean of distribution: 0.1402, Mean boostrapped: 0.1408, Stddev mean bootstrapped 0.0346 
</code></pre>
</div>

<p>Intrestingly enough, as the initial sample is quite narrow (20 extractions), the mean and std-dev of mean varies significantly between successive runs of the algorithm.</p>

  </div>

  
</article>

      </div>
    </main>

    <footer class="site-footer">

  <div class="wrapper">

    <h2 class="footer-heading">From The Trenches - The Code</h2>

    <div class="footer-col-wrapper">
      <div class="footer-col footer-col-1">
        <ul class="contact-list">
          <li>
            
              From The Trenches - The Code
            
            </li>
            
            <li><a href="mailto:alexandru.gris2006@gmail.com">alexandru.gris2006@gmail.com</a></li>
            
        </ul>
      </div>

      <div class="footer-col footer-col-2">
        <ul class="social-media-list">
          
          <li>
            <a href="https://github.com/alexandrugris"><span class="icon icon--github"><svg viewBox="0 0 16 16" width="16px" height="16px"><path fill="#828282" d="M7.999,0.431c-4.285,0-7.76,3.474-7.76,7.761 c0,3.428,2.223,6.337,5.307,7.363c0.388,0.071,0.53-0.168,0.53-0.374c0-0.184-0.007-0.672-0.01-1.32 c-2.159,0.469-2.614-1.04-2.614-1.04c-0.353-0.896-0.862-1.135-0.862-1.135c-0.705-0.481,0.053-0.472,0.053-0.472 c0.779,0.055,1.189,0.8,1.189,0.8c0.692,1.186,1.816,0.843,2.258,0.645c0.071-0.502,0.271-0.843,0.493-1.037 C4.86,11.425,3.049,10.76,3.049,7.786c0-0.847,0.302-1.54,0.799-2.082C3.768,5.507,3.501,4.718,3.924,3.65 c0,0,0.652-0.209,2.134,0.796C6.677,4.273,7.34,4.187,8,4.184c0.659,0.003,1.323,0.089,1.943,0.261 c1.482-1.004,2.132-0.796,2.132-0.796c0.423,1.068,0.157,1.857,0.077,2.054c0.497,0.542,0.798,1.235,0.798,2.082 c0,2.981-1.814,3.637-3.543,3.829c0.279,0.24,0.527,0.713,0.527,1.437c0,1.037-0.01,1.874-0.01,2.129 c0,0.208,0.14,0.449,0.534,0.373c3.081-1.028,5.302-3.935,5.302-7.362C15.76,3.906,12.285,0.431,7.999,0.431z"/></svg>
</span><span class="username">alexandrugris</span></a>

          </li>
          

          
          <li>
            <a href="https://twitter.com/alexandrugris"><span class="icon icon--twitter"><svg viewBox="0 0 16 16" width="16px" height="16px"><path fill="#828282" d="M15.969,3.058c-0.586,0.26-1.217,0.436-1.878,0.515c0.675-0.405,1.194-1.045,1.438-1.809c-0.632,0.375-1.332,0.647-2.076,0.793c-0.596-0.636-1.446-1.033-2.387-1.033c-1.806,0-3.27,1.464-3.27,3.27 c0,0.256,0.029,0.506,0.085,0.745C5.163,5.404,2.753,4.102,1.14,2.124C0.859,2.607,0.698,3.168,0.698,3.767 c0,1.134,0.577,2.135,1.455,2.722C1.616,6.472,1.112,6.325,0.671,6.08c0,0.014,0,0.027,0,0.041c0,1.584,1.127,2.906,2.623,3.206 C3.02,9.402,2.731,9.442,2.433,9.442c-0.211,0-0.416-0.021-0.615-0.059c0.416,1.299,1.624,2.245,3.055,2.271 c-1.119,0.877-2.529,1.4-4.061,1.4c-0.264,0-0.524-0.015-0.78-0.046c1.447,0.928,3.166,1.469,5.013,1.469 c6.015,0,9.304-4.983,9.304-9.304c0-0.142-0.003-0.283-0.009-0.423C14.976,4.29,15.531,3.714,15.969,3.058z"/></svg>
</span><span class="username">alexandrugris</span></a>

          </li>
          
        </ul>
      </div>

      <div class="footer-col footer-col-3">
        <p>Alexandru Gris - Personal Blog
</p>
      </div>
    </div>

  </div>

</footer>


  </body>

</html>
