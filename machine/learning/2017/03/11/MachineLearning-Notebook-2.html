<!DOCTYPE html>
<html lang="en">

  <head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <title>My Machine Learning Notebook - Part 2 (Regression)</title>
  <meta name="description" content="This is the second part of my Machine Learning notebook, following the Udemy course “Machine Learning A-Z™: Hands-On Python &amp;amp; R”.">

  <link rel="stylesheet" href="/assets/main.css">
  <link rel="canonical" href="https://alexandrugris.github.io/machine/learning/2017/03/11/MachineLearning-Notebook-2.html">
  <link rel="alternate" type="application/rss+xml" title="From The Trenches - The Code" href="/feed.xml">
  
  
</head>


  <body>

    <header class="site-header" role="banner">

  <div class="wrapper">

    <div class="site-nav"><a class="site-title" href="/">From The Trenches - The Code</a> </div>

    <nav class="site-nav">
      <span class="menu-icon">        
      </span>

      <div class="trigger">
        
          
          <a class="page-link" href="/about/">About</a>
          
        
          
        
          
        
          
          <a class="page-link" href="/sm/">Social Media</a>
          
        
          
        
          
        
      </div>
    </nav>

  </div>

</header>


    <main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title" itemprop="name headline">My Machine Learning Notebook - Part 2 (Regression)</h1>
    <p class="post-meta"><time datetime="2017-03-11T14:15:16+02:00" itemprop="datePublished">Mar 11, 2017</time></p>
  </header>

  <div class="post-content" itemprop="articleBody">
    <p>This is the second part of my Machine Learning notebook, following the Udemy course “Machine Learning A-Z™: Hands-On Python &amp; R”.</p>

<h3 id="simple-linear-regression-with-plot">Simple Linear Regression With Plot</h3>

<p>Let’s draw a plot with the following convesion:</p>
<ul>
  <li>Pink dots - training set X</li>
  <li>Blue line - regression line on the train set</li>
  <li>Red dots - values to predict (test set)</li>
  <li>Green dots - predicted values for the test set (situated on the blue line)</li>
</ul>

<p>We consider sets of <code class="highlighter-rouge">y</code>s and <code class="highlighter-rouge">x</code>s, derived from experiments. Assuming that <code class="highlighter-rouge">y = ax + b</code>,  what value for <code class="highlighter-rouge">a</code> and <code class="highlighter-rouge">b</code> minimize the error of <code class="highlighter-rouge">sum ((yi-(axi + b))^2)</code>? <code class="highlighter-rouge">b</code> is called intercept and <code class="highlighter-rouge">a</code> is the slope.</p>

<div class="language-python highlighter-rouge"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">pandas</span> <span class="kn">as</span> <span class="nn">pd</span>
<span class="n">dataset</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s">".</span><span class="se">\\</span><span class="s">Data</span><span class="se">\\</span><span class="s">Salary_Data.csv"</span><span class="p">)</span>

<span class="n">X</span> <span class="o">=</span> <span class="n">dataset</span><span class="o">.</span><span class="n">iloc</span><span class="p">[:,</span> <span class="p">:</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">values</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">dataset</span><span class="o">.</span><span class="n">iloc</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">values</span>

<span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span>
<span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">test_size</span> <span class="o">=</span> <span class="mf">0.2</span><span class="p">)</span>

<span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">LinearRegression</span>

<span class="n">regressor</span> <span class="o">=</span> <span class="n">LinearRegression</span><span class="p">()</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>

<span class="n">y_pred</span> <span class="o">=</span> <span class="n">regressor</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>

<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="kn">as</span> <span class="nn">plt</span>

<span class="c"># scatter plot</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s">"pink"</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s">"red"</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s">"lightgreen"</span><span class="p">)</span>

<span class="c"># line plot</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">regressor</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_train</span><span class="p">),</span> <span class="n">color</span><span class="o">=</span><span class="s">"blue"</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s">"Salary vs Experience"</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s">"Years"</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s">"Salary"</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</code></pre>
</div>

<p><img src="https://alexandrugris.github.io/assets/ml_3_1.png" alt="Results" /></p>

<p>The implementation of simple linear regression using the ordinary least squares method is straight forward:</p>

<div class="language-python highlighter-rouge"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>

<span class="k">def</span>  <span class="nf">ols_train_predict</span><span class="p">(</span><span class="n">_X_train</span><span class="p">,</span> <span class="n">_Y_train</span><span class="p">,</span> <span class="n">_X_test</span><span class="p">):</span>
    <span class="s">"""
    f = sum( (Y - (aX+b)) ** 2) == minimum &lt;=&gt;
    df / da = 0 and df / db = 0 
    """</span>
    
    <span class="c"># secure the same dimensions</span>
    <span class="n">X_train</span> <span class="o">=</span> <span class="n">_X_train</span><span class="o">.</span><span class="n">flatten</span><span class="p">()</span>
    <span class="n">Y_train</span> <span class="o">=</span> <span class="n">_Y_train</span><span class="o">.</span><span class="n">flatten</span><span class="p">()</span>
    <span class="n">X_test</span> <span class="o">=</span> <span class="n">_X_test</span><span class="o">.</span><span class="n">flatten</span><span class="p">()</span>

    <span class="n">xi_2_sum</span>    <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="nb">sum</span><span class="p">(</span><span class="n">X_train</span> <span class="o">*</span> <span class="n">X_train</span><span class="p">)</span>
    <span class="n">xi_sum</span>      <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="nb">sum</span><span class="p">(</span><span class="n">X_train</span><span class="p">)</span>
    <span class="n">xi_yi_sum</span>   <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="nb">sum</span><span class="p">(</span><span class="n">X_train</span> <span class="o">*</span> <span class="n">Y_train</span><span class="p">)</span>

    <span class="n">N</span> <span class="o">=</span> <span class="n">X_train</span><span class="o">.</span><span class="n">size</span>
    <span class="n">yi_sum</span>      <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="nb">sum</span><span class="p">(</span><span class="n">Y_train</span><span class="p">)</span>

    <span class="c"># we now have to solve the following equations:</span>
    <span class="c"># a * xi_2_sum + b * xi_sum - xi_yi_sum = 0</span>
    <span class="c"># a * xi_sum + b * N  - yi_sum = 0</span>

    <span class="n">a</span> <span class="o">=</span> <span class="p">(</span><span class="n">xi_yi_sum</span> <span class="o">*</span> <span class="n">N</span> <span class="o">-</span> <span class="n">xi_sum</span> <span class="o">*</span> <span class="n">yi_sum</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="n">N</span> <span class="o">*</span> <span class="n">xi_2_sum</span> <span class="o">-</span> <span class="n">xi_sum</span> <span class="o">*</span> <span class="n">xi_sum</span><span class="p">)</span>
    <span class="n">b</span> <span class="o">=</span> <span class="p">(</span><span class="n">yi_sum</span> <span class="o">-</span> <span class="n">a</span> <span class="o">*</span> <span class="n">xi_sum</span><span class="p">)</span> <span class="o">/</span> <span class="n">N</span>

    <span class="k">return</span> <span class="n">X_test</span> <span class="o">*</span> <span class="n">a</span> <span class="o">+</span> <span class="n">b</span>

<span class="n">Y</span> <span class="o">=</span> <span class="n">ols_train_predict</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">X_train</span><span class="p">)</span>

<span class="c"># line plot</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">Y</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s">"orange"</span><span class="p">)</span>
</code></pre>
</div>

<p>The basic idea for all least squares methods is that the function <code class="highlighter-rouge">S=sum((yi-f(xi, B))^2)</code> is minimized where <code class="highlighter-rouge">B</code> is a vector of variables we need to identify. In the particular case of linear regression, <code class="highlighter-rouge">B = [a, b]</code> and <code class="highlighter-rouge">f(xi, B) = axi + b</code>. Minimized means the conditions <code class="highlighter-rouge">dS / dB[i] = 0</code>. In the case above, we have a system of two equations with two unknowns, <code class="highlighter-rouge">d(sum ((yi-(axi + b))^2))/da = 0</code> and <code class="highlighter-rouge">d(sum ((yi-(axi + b))^2))/db = 0</code>.</p>

<p>Results:</p>

<p><img src="https://alexandrugris.github.io/assets/ml_3_5.png" alt="Dataset" /></p>

<p>Note:</p>

<p>I kept the function above simple, but it is not numerically friendly. Numbers <code class="highlighter-rouge">xi_2_sum</code>, <code class="highlighter-rouge">xi_yi_sum</code>, etc. might get very high. Therefore some rearangement of the computation should be performed for production code.</p>

<p>Another method for obtaining similar results is to use the corellation coefficient for determining <code class="highlighter-rouge">a</code> and <code class="highlighter-rouge">b</code>. <code class="highlighter-rouge">a = R * stddev(y) / stddev(x), b = mean(y) - a * mean(x)</code> where <code class="highlighter-rouge">R = dot(z_score(x), z_score(y)) / sizeof(x)</code> and <code class="highlighter-rouge">z_score(x) = (x - mean(x)) / stddev(x)</code>. This is clearly simpler to read and also provides more context to the results of the method - corellation coefficient (<code class="highlighter-rouge">R</code>) is between -1 and 1, with 0 meaning there is no corellation. This correlation coefficient is also known as the <a href="https://en.wikipedia.org/wiki/Pearson_correlation_coefficient">Pearson coefficient</a></p>

<p>Here is the python code:</p>

<div class="language-python highlighter-rouge"><pre class="highlight"><code>
<span class="c"># 50 points, x between 10, 100, a = 2, b = 126, variation of y = 20</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">m</span><span class="o">.</span><span class="n">generate_noisy_linear_data</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">100</span><span class="p">,</span> <span class="mi">50</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">126</span><span class="p">,</span> <span class="mi">20</span><span class="p">)</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">100</span><span class="p">,</span> <span class="mi">50</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>

<span class="c"># z-score</span>
<span class="n">x_</span> <span class="o">=</span> <span class="n">m</span><span class="o">.</span><span class="n">scale_std_dev</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="n">y_</span> <span class="o">=</span> <span class="n">m</span><span class="o">.</span><span class="n">scale_std_dev</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>

<span class="n">correlation</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span> <span class="o">/</span> <span class="n">x_</span><span class="o">.</span><span class="n">size</span><span class="p">)</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">x_</span> <span class="p">,</span> <span class="n">y_</span><span class="p">)</span>

<span class="k">print</span><span class="p">(</span><span class="n">correlation</span><span class="p">)</span>

<span class="n">a</span> <span class="o">=</span> <span class="n">correlation</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="n">y</span><span class="p">)</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="n">b</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">y</span><span class="p">)</span> <span class="o">-</span> <span class="n">a</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

<span class="k">print</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">)</span>
</code></pre>
</div>

<p>Giving very close results for <code class="highlighter-rouge">corellation=0.994670853834</code>, <code class="highlighter-rouge">a = 2.01623520593</code> and <code class="highlighter-rouge">b = 125.614041068</code>.</p>

<h3 id="multiple-linear-regression">Multiple linear regression</h3>

<p>Formula: <code class="highlighter-rouge">y = b + a1 * x1 + a2 * x2 + a3 * x3 + ...</code></p>

<p>Assumptions that need to be checked first:</p>

<ul>
  <li>Linearity</li>
  <li>Homoscedasticity (<a href="https://en.wikipedia.org/wiki/Homoscedasticity">all random variables in the sequence or vector have the same finite variance</a>)</li>
  <li>Multivariate normality of error distribution - <a href="https://en.wikipedia.org/wiki/Multivariate_normal_distribution">https://en.wikipedia.org/wiki/Multivariate_normal_distribution</a></li>
  <li>Independence of errors</li>
  <li>Lack of multicolinearity - <a href="https://en.wikipedia.org/wiki/Multicollinearity">is a phenomenon in which two or more predictor variables in a multiple regression model are highly correlated, meaning that one can be linearly predicted from the others with a substantial degree of accuracy</a></li>
</ul>

<p>For categorial variables, we need to transform them in dummy binary variables, but <em>not include all of them in the regression model - dummy variable trap</em>. The idea is that if we include all the categories in the multiple regression we break the rule of lack of multicolinearity. Afterall, if a value does not belong to a category, it will belong to another. The rule is “always omit one dummy variable”.</p>

<p>Methods for building a multiple regression model:</p>

<ul>
  <li>
    <p><em>All variables in</em> - not really recommended because of the noise some variables might bring to the prediction. Some variables might simply be bad predictors and they would only pollute the model space. Useful if you have prior knowledge of the problem domain and you know that these variables matter.</p>
  </li>
  <li>
    <p><em>Backward elimination</em> - Steps:</p>
  </li>
</ul>

<ol>
  <li>Select a significance level to stay in the model (SL = 0.05, for instance),</li>
  <li>Fit the model with all possible predictors (all variables in),</li>
  <li>Consider the predictor with the highest <a href="https://en.wikipedia.org/wiki/P-value">P-value</a>.</li>
  <li>If P &gt; SL, go to next step. Otherwise finish. Step 5. Fit the model without this predictor and go to Step 3.</li>
</ol>

<p>In the end we have a model in which all variables have their P-value less than our chosen significance level.</p>

<ul>
  <li><em>Forward selection</em> - Steps:</li>
</ul>

<ol>
  <li>Select a significance level to enter the model. E.g. SL = 0.05.</li>
  <li>Fit all simple regression models y ~ xi. Select the one with the lowest P-value</li>
  <li>Construct all possible models with this variable AND with another predictor added to this one.</li>
  <li>Select the predictor with the lowest P-value. If P-value &lt; SL, go to 3. Else finish and keep the previous model.</li>
</ol>

<ul>
  <li><em>Bidirectional elimination (stepwise regression)</em> - Steps:</li>
</ul>

<ol>
  <li>Select a significance level to enter and a significance level to say.</li>
  <li>Perform next step from forward selection.</li>
  <li>Perform all steps from backward elimiation.</li>
</ol>

<p>The model is considered finished when no more variables can enter or stay.</p>

<ul>
  <li><em>Score comparison</em></li>
</ul>

<ol>
  <li>Build several models</li>
  <li>Select the one that fits best a specific criterion</li>
</ol>

<p><em>P-value (Wikipedia):</em></p>

<blockquote>
  <p>In statistical hypothesis testing, the p-value is the probability for a given statistical model that, when the null hypothesis is true, the statistical summary (such as the sample mean difference between two compared groups) would be the same as or more extreme than the actual observed results.
The p-value is defined informally as the probability of obtaining a result equal to or “more extreme” than what was actually observed, when the null hypothesis is true.</p>
</blockquote>

<p><em>Null hypothesis (Wikipedia)</em></p>

<blockquote>
  <p>The term “null hypothesis” is a general statement or default position that there is no relationship between two measured phenomena, or no association among groups. Rejecting or disproving the null hypothesis—and thus concluding that there are grounds for believing that there is a relationship between two phenomena (e.g. that a potential treatment has a measurable effect)—is a central task in the modern practice of science; the field of statistics gives precise criteria for rejecting a null hypothesis.
The null hypothesis is generally assumed to be true until evidence indicates otherwise.</p>
</blockquote>

<p>TODO in a future post: how to compute the P-value</p>

<h3 id="multiple-linear-regression---the-code">Multiple linear regression - the code</h3>

<p>The dataset:</p>

<p><img src="https://alexandrugris.github.io/assets/ml_3_2.png" alt="Dataset" /></p>

<p>Preparing the data:</p>

<div class="language-python highlighter-rouge"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="kn">as</span> <span class="nn">pd</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="kn">as</span> <span class="nn">plt</span>

<span class="c"># load data</span>
<span class="n">dataset</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s">".</span><span class="se">\\</span><span class="s">Data</span><span class="se">\\</span><span class="s">50_Startups.csv"</span><span class="p">)</span>

<span class="n">X</span> <span class="o">=</span> <span class="n">dataset</span><span class="o">.</span><span class="n">iloc</span><span class="p">[:,</span> <span class="p">:</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">values</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">dataset</span><span class="o">.</span><span class="n">iloc</span><span class="p">[:,</span> <span class="mi">4</span><span class="p">]</span><span class="o">.</span><span class="n">values</span>

 <span class="c"># encode categorical data to numbers</span>
<span class="kn">from</span> <span class="nn">sklearn.preprocessing</span> <span class="kn">import</span> <span class="n">LabelEncoder</span>

<span class="c"># transforms categorical data from strings to numbers (our State column)</span>
<span class="n">X</span><span class="p">[:,</span> <span class="mi">3</span><span class="p">]</span> <span class="o">=</span> <span class="n">LabelEncoder</span><span class="p">()</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">X</span><span class="p">[:,</span> <span class="mi">3</span><span class="p">])</span>

<span class="c"># since we don't want in our model to have order between categories,</span>
<span class="c"># we need to create dummy variables, one column per each category</span>
<span class="kn">from</span> <span class="nn">sklearn.preprocessing</span> <span class="kn">import</span> <span class="n">OneHotEncoder</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">OneHotEncoder</span><span class="p">(</span><span class="n">categorical_features</span><span class="o">=</span><span class="p">[</span><span class="mi">3</span><span class="p">])</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">X</span><span class="p">)</span><span class="o">.</span><span class="n">toarray</span><span class="p">()</span>

<span class="c"># Remove the dummy variable trap - we remove the first colum, which is equivalent to "California"</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">X</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">:]</span>

<span class="c"># split data into training and test</span>
<span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span>
<span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
</code></pre>
</div>

<p>And the output:</p>

<p><img src="https://alexandrugris.github.io/assets/ml_3_3.png" alt="Dataset" /></p>

<p>For the actual regression:</p>

<div class="language-python highlighter-rouge"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">LinearRegression</span>

<span class="n">regressor</span> <span class="o">=</span> <span class="n">LinearRegression</span><span class="p">()</span>
<span class="n">regressor</span> <span class="o">=</span> <span class="n">regressor</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>

<span class="n">y_pred</span> <span class="o">=</span> <span class="n">regressor</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
</code></pre>
</div>

<p><em>** Backward Elimination **</em></p>

<div class="language-python highlighter-rouge"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">statsmodels.formula.api</span> <span class="kn">as</span> <span class="nn">sm</span>
<span class="c"># add a 1 column to X. our formula for linear regression is </span>
<span class="c"># y = b0 + b1 * x1 + b2 * x2 ... &lt;=&gt; y = b0 * x0 + b1 * x1 + ... where x0 == [1, ..]</span>
<span class="c"># we add the column in the beginning so it is easier to interpret the result</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="mi">50</span><span class="p">,</span><span class="mi">1</span><span class="p">)),</span> <span class="n">X</span><span class="p">,</span> <span class="n">axis</span> <span class="o">=</span> <span class="mi">1</span><span class="p">)</span>
</code></pre>
</div>

<p><img src="https://alexandrugris.github.io/assets/ml_3_4.png" alt="New X" /></p>

<p>To remember: the lower the P-value, the more statistically relevant the independent variable is going to be in our regression model.</p>

<div class="language-python highlighter-rouge"><pre class="highlight"><code><span class="c"># the optimum matrix of features that contains the </span>
<span class="c"># optimum set of variables to predict the outcome - setatistically significant</span>
<span class="c"># initially add all columns, all rows</span>
<span class="n">X_opt</span> <span class="o">=</span> <span class="n">X</span><span class="p">[:,</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">])]</span>

<span class="c"># use the ordinary least squares algorithm</span>
<span class="n">regressor_ols</span> <span class="o">=</span> <span class="n">sm</span><span class="o">.</span><span class="n">OLS</span><span class="p">(</span><span class="n">endog</span> <span class="o">=</span> <span class="n">y</span><span class="p">,</span> <span class="n">exog</span> <span class="o">=</span> <span class="n">X_opt</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">()</span>
<span class="n">regressor_ols</span><span class="o">.</span><span class="n">summary</span><span class="p">()</span>
</code></pre>
</div>

<p>We get the following table, with the P-values in the P&gt;t column. <code class="highlighter-rouge">const, x1, ... x5</code> are the coefficients <code class="highlighter-rouge">b0, b1, .. b5</code> specified above.</p>

<div class="highlighter-rouge"><pre class="highlight"><code>|         |       coef|    std err|          t|        P&gt;t|      [95.0% Conf. Int.]|
|----------------------------------------------------------------------------------|
|const    |  5.013e+04|   6884.820|      7.281|      0.000|      3.62e+04   6.4e+04|
|x1       |   198.7888|   3371.007|      0.059|      0.953|     -6595.030  6992.607|
|x2       |   -41.8870|   3256.039|     -0.013|      0.990|     -6604.003  6520.229|
|x3       |     0.8060|      0.046|     17.369|      0.000|         0.712     0.900|
|x4       |    -0.0270|      0.052|     -0.517|      0.608|        -0.132     0.078|
|x5       |     0.0270|      0.017|      1.574|      0.123|        -0.008     0.062|
</code></pre>
</div>

<p>According to algorithm, we remove x2 and redo the ordinary least square fit.</p>

<p>In the end, if we continue to run the backwards elimination algorithm, we notice that the only variable that significantly influences the profit is the R&amp;D Spent. :)</p>

<p>Without library support, we compute the multiple regression line according to the following.</p>

<p>We consider the line <code class="highlighter-rouge">Y = b0 + b1 * x1 + b2 * x2 + ...</code>.</p>

<p><em>Theorem:</em> The regression line has the following form: <code class="highlighter-rouge">Y - y_mean = sum(bj * (xj - xj_mean))</code>, where <code class="highlighter-rouge">bj</code> solve the following system of equations: <code class="highlighter-rouge">cov(y, xj) = sum(bm * cov(xm, xj))</code> <a href="http://www.real-statistics.com/multiple-regression/least-squares-method-multiple-regression/">here</a></p>

<p>We are going to do precisely this. The <a href="https://en.wikipedia.org/wiki/Covariance">covariation</a> between two vectors is expressed through the following function:</p>

<div class="language-python highlighter-rouge"><pre class="highlight"><code><span class="c">####################################</span>
<span class="c">### multiple regression</span>
<span class="c"># y = b0 + x1 * b1 + x2 * b2 + x3 * b3 ...</span>
<span class="c"># line has the form</span>
<span class="c"># (y - y_mean) = sum(b(x - x_mean))</span>

<span class="k">def</span> <span class="nf">cov</span><span class="p">(</span><span class="n">_x</span><span class="p">,</span> <span class="n">_y</span><span class="p">):</span>

    <span class="n">x</span> <span class="o">=</span> <span class="n">_x</span><span class="o">.</span><span class="n">flatten</span><span class="p">()</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">_y</span><span class="o">.</span><span class="n">flatten</span><span class="p">()</span>

    <span class="k">if</span> <span class="n">x</span><span class="o">.</span><span class="n">size</span> <span class="o">!=</span> <span class="n">y</span><span class="o">.</span><span class="n">size</span><span class="p">:</span>
        <span class="k">raise</span> <span class="nb">Exception</span><span class="p">(</span><span class="s">"x and y should have the same size"</span><span class="p">)</span>

    <span class="n">mean_x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">mean_y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>

    <span class="n">N</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">size</span>

    <span class="k">return</span> <span class="p">(</span><span class="mi">1</span> <span class="o">/</span> <span class="n">N</span><span class="p">)</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="nb">sum</span><span class="p">((</span><span class="n">x</span> <span class="o">-</span> <span class="n">mean_x</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span><span class="n">y</span> <span class="o">-</span> <span class="n">mean_y</span><span class="p">))</span>
</code></pre>
</div>

<p>However, we are not going to use it but rather use directly numpy’s matrices for faster computation.</p>

<div class="language-python highlighter-rouge"><pre class="highlight"><code><span class="k">def</span> <span class="nf">cov_matrix</span><span class="p">(</span><span class="n">_y</span><span class="p">,</span> <span class="n">_x</span><span class="p">):</span>
    
    <span class="k">if</span> <span class="n">_x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">!=</span> <span class="n">_y</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]:</span>
        <span class="k">raise</span> <span class="nb">Exception</span><span class="p">(</span><span class="s">"Shapes do not match"</span><span class="p">)</span>

    <span class="c"># make sure we use matrix multiplication, not array multiplication</span>
    <span class="n">_xm</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">matrix</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">_x</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">repeat</span><span class="p">(</span><span class="n">_x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">axis</span> <span class="o">=</span> <span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">_x</span><span class="o">.</span><span class="n">shape</span><span class="p">))</span>
    <span class="n">_ym</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">matrix</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">_y</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">repeat</span><span class="p">(</span><span class="n">_y</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">axis</span> <span class="o">=</span> <span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">_y</span><span class="o">.</span><span class="n">shape</span><span class="p">))</span>

    <span class="k">return</span> <span class="p">((</span><span class="n">_x</span> <span class="o">-</span> <span class="n">_xm</span><span class="p">)</span><span class="o">.</span><span class="n">T</span> <span class="o">*</span> <span class="p">(</span><span class="n">_y</span> <span class="o">-</span> <span class="n">_ym</span><span class="p">))</span> <span class="o">*</span> <span class="mi">1</span> <span class="o">/</span> <span class="n">_x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>

<span class="k">def</span> <span class="nf">compute_b0_bn</span><span class="p">(</span><span class="n">ym</span><span class="p">,</span> <span class="n">Xm</span><span class="p">):</span>
    
    <span class="k">if</span> <span class="n">ym</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">!=</span> <span class="mi">1</span><span class="p">:</span>
        <span class="k">raise</span> <span class="nb">Exception</span> <span class="p">(</span><span class="s">"ym should be a vector with shape [n, 1]"</span><span class="p">)</span>
        
    <span class="k">if</span> <span class="n">Xm</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">!=</span> <span class="n">ym</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]:</span>
        <span class="k">raise</span> <span class="nb">Exception</span> <span class="p">(</span><span class="s">"Xm should have the same amount of lines as ym"</span><span class="p">)</span>
    
    <span class="n">C_y_x</span> <span class="o">=</span> <span class="n">cov_matrix</span><span class="p">(</span><span class="n">ym</span><span class="p">,</span> <span class="n">Xm</span><span class="p">)</span>
    <span class="n">C_x_x</span> <span class="o">=</span> <span class="n">cov_matrix</span><span class="p">(</span><span class="n">Xm</span><span class="p">,</span> <span class="n">Xm</span><span class="p">)</span>

    <span class="n">b1_bn</span>  <span class="o">=</span> <span class="n">C_x_x</span><span class="o">.</span><span class="n">I</span> <span class="o">*</span> <span class="n">C_y_x</span>
    
    <span class="n">x_mean</span>  <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">matrix</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">Xm</span><span class="p">,</span> <span class="n">axis</span> <span class="o">=</span> <span class="mi">0</span><span class="p">))</span>
    <span class="n">y_mean</span>  <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">ym</span><span class="p">)</span>
    
    <span class="n">b0</span> <span class="o">=</span> <span class="o">-</span><span class="n">x_mean</span> <span class="o">*</span> <span class="n">b1_bn</span> <span class="o">+</span> <span class="n">y_mean</span>
    
    <span class="k">return</span> <span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="nb">float</span><span class="p">(</span><span class="n">b0</span><span class="p">),</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">b1_bn</span><span class="p">)</span><span class="o">.</span><span class="n">flatten</span><span class="p">())</span>


<span class="n">Xm</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">matrix</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
<span class="n">ym</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">matrix</span><span class="p">(</span><span class="n">y</span><span class="o">.</span><span class="n">reshape</span><span class="p">((</span><span class="n">y</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="mi">1</span><span class="p">)))</span>

<span class="n">ret</span> <span class="o">=</span> <span class="n">compute_b0_bn</span><span class="p">(</span><span class="n">ym</span><span class="p">,</span> <span class="n">Xm</span><span class="p">)</span>
</code></pre>
</div>

<p>In order to test my function, I have compared its results with the results of algorithm from the library. They are identical.</p>

<div class="language-python highlighter-rouge"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">LinearRegression</span>

<span class="n">regressor</span> <span class="o">=</span> <span class="n">LinearRegression</span><span class="p">()</span>
<span class="n">regressor</span> <span class="o">=</span> <span class="n">regressor</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="n">coef</span> <span class="o">=</span> <span class="n">regressor</span><span class="o">.</span><span class="n">coef_</span>
<span class="n">intercept</span> <span class="o">=</span> <span class="n">regressor</span><span class="o">.</span><span class="n">intercept_</span>
</code></pre>
</div>

<p><img src="https://alexandrugris.github.io/assets/ml_3_6.png" alt="Identical results between library and own function" /></p>

  </div>

  
</article>

      </div>
    </main>

    <footer class="site-footer">

  <div class="wrapper">

    <h2 class="footer-heading">From The Trenches - The Code</h2>

    <div class="footer-col-wrapper">
      <div class="footer-col footer-col-1">
        <ul class="contact-list">
          <li>
            
              From The Trenches - The Code
            
            </li>
            
            <li><a href="mailto:alexandru.gris2006@gmail.com">alexandru.gris2006@gmail.com</a></li>
            
        </ul>
      </div>

      <div class="footer-col footer-col-2">
        <ul class="social-media-list">
          
          <li>
            <a href="https://github.com/alexandrugris"><span class="icon icon--github"><svg viewBox="0 0 16 16" width="16px" height="16px"><path fill="#828282" d="M7.999,0.431c-4.285,0-7.76,3.474-7.76,7.761 c0,3.428,2.223,6.337,5.307,7.363c0.388,0.071,0.53-0.168,0.53-0.374c0-0.184-0.007-0.672-0.01-1.32 c-2.159,0.469-2.614-1.04-2.614-1.04c-0.353-0.896-0.862-1.135-0.862-1.135c-0.705-0.481,0.053-0.472,0.053-0.472 c0.779,0.055,1.189,0.8,1.189,0.8c0.692,1.186,1.816,0.843,2.258,0.645c0.071-0.502,0.271-0.843,0.493-1.037 C4.86,11.425,3.049,10.76,3.049,7.786c0-0.847,0.302-1.54,0.799-2.082C3.768,5.507,3.501,4.718,3.924,3.65 c0,0,0.652-0.209,2.134,0.796C6.677,4.273,7.34,4.187,8,4.184c0.659,0.003,1.323,0.089,1.943,0.261 c1.482-1.004,2.132-0.796,2.132-0.796c0.423,1.068,0.157,1.857,0.077,2.054c0.497,0.542,0.798,1.235,0.798,2.082 c0,2.981-1.814,3.637-3.543,3.829c0.279,0.24,0.527,0.713,0.527,1.437c0,1.037-0.01,1.874-0.01,2.129 c0,0.208,0.14,0.449,0.534,0.373c3.081-1.028,5.302-3.935,5.302-7.362C15.76,3.906,12.285,0.431,7.999,0.431z"/></svg>
</span><span class="username">alexandrugris</span></a>

          </li>
          

          
          <li>
            <a href="https://twitter.com/alexandrugris"><span class="icon icon--twitter"><svg viewBox="0 0 16 16" width="16px" height="16px"><path fill="#828282" d="M15.969,3.058c-0.586,0.26-1.217,0.436-1.878,0.515c0.675-0.405,1.194-1.045,1.438-1.809c-0.632,0.375-1.332,0.647-2.076,0.793c-0.596-0.636-1.446-1.033-2.387-1.033c-1.806,0-3.27,1.464-3.27,3.27 c0,0.256,0.029,0.506,0.085,0.745C5.163,5.404,2.753,4.102,1.14,2.124C0.859,2.607,0.698,3.168,0.698,3.767 c0,1.134,0.577,2.135,1.455,2.722C1.616,6.472,1.112,6.325,0.671,6.08c0,0.014,0,0.027,0,0.041c0,1.584,1.127,2.906,2.623,3.206 C3.02,9.402,2.731,9.442,2.433,9.442c-0.211,0-0.416-0.021-0.615-0.059c0.416,1.299,1.624,2.245,3.055,2.271 c-1.119,0.877-2.529,1.4-4.061,1.4c-0.264,0-0.524-0.015-0.78-0.046c1.447,0.928,3.166,1.469,5.013,1.469 c6.015,0,9.304-4.983,9.304-9.304c0-0.142-0.003-0.283-0.009-0.423C14.976,4.29,15.531,3.714,15.969,3.058z"/></svg>
</span><span class="username">alexandrugris</span></a>

          </li>
          
        </ul>
      </div>

      <div class="footer-col footer-col-3">
        <p>Alexandru Gris - Personal Blog
</p>
      </div>
    </div>

  </div>

</footer>


  </body>

</html>
