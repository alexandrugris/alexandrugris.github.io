<!DOCTYPE html>
<html lang="en">

  <head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <title>P-Values And Hypothesis Testing</title>
  <meta name="description" content="This post is about p-values and hypothesis testing. What they are, why they are needed, how to compute them and how to use them. It also includes a worked ex...">

  <link rel="stylesheet" href="/assets/main.css">
  <link rel="canonical" href="https://alexandrugris.github.io/maths/2017/05/14/p-values-hypothesis-testing.html">
  <link rel="alternate" type="application/rss+xml" title="From The Trenches - The Code" href="/feed.xml">
  
  
</head>


  <body>

    <header class="site-header" role="banner">

  <div class="wrapper">

    <div class="site-nav"><a class="site-title" href="/">From The Trenches - The Code</a> </div>

    <nav class="site-nav">
      <span class="menu-icon">        
      </span>

      <div class="trigger">

        <a class="page-link" href="https://alexandrugris.github.io">Home</a>

        
          
          <a class="page-link" href="/about/">About</a>
          
        
          
        
          
        
          
          <a class="page-link" href="/sm/">Social Media</a>
          
        
          
        
      </div>
    </nav>

  </div>

</header>


    <main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title" itemprop="name headline">P-Values And Hypothesis Testing</h1>
    <p class="post-meta"><time datetime="2017-05-14T14:15:16+03:00" itemprop="datePublished">May 14, 2017</time></p>
  </header>

  <div class="post-content" itemprop="articleBody">
    <p>This post is about p-values and hypothesis testing. What they are, why they are needed, how to compute them and how to use them. It also includes a worked example, how to validate that an A/B test indeed produces a significant outcome. The article follows quite closely a chapter from “Data Science from Scratch” by Joel Grus and it is annotated with my own research and observations.</p>

<h2 id="definitions">Definitions</h2>

<ol>
  <li>
    <p><em>Null hypothesis (H0)</em> - the hypothesis we want to test against. Equivalent to “there is nothing out of the ordinary”. For a coin toss, for instance, the hypothesis that the coin is fair. For a medicine, that it has no more effect than a sugar pill.</p>
  </li>
  <li>
    <p><em>Alternative hypothesis (H1)</em> - the hypothesis we want to test for. Something is happening, the coin is unfair or that the medicine has a significant effect.</p>
  </li>
  <li>
    <p><em>Significance (probability)</em> - how willing are we to make a false positive claim, to reject H0 even if it is true. Due to historical reasons and scientific prudence, it is usually set to 1% or 5%.</p>
  </li>
  <li>
    <p><em>Type 1 Error</em> - false positive. Reject H0 even if it is true. Say that there is something when there isn’t.</p>
  </li>
  <li>
    <p><em>Type 2 Error</em> - the opposite of type 1; failure to reject the null hypothesis even though there is something.</p>
  </li>
  <li>
    <p><em>Power of the test</em> - probability of not making a type 2 error.</p>
  </li>
  <li>
    <p><em>P-value</em> - instead of choosing bounds based on some probability cutoff (99% or 95%), we compute the probability— assuming H0 is true—that we would see a value at least as extreme as the one we actually observed. <a href="https://en.wikipedia.org/wiki/P-value">wikipedia</a></p>
  </li>
</ol>

<p>Before going forward, I would like to point out these two links:</p>
<ul>
  <li><a href="https://www.boundless.com/users/233402/textbooks/openintro-statistics/">OpenIntro Statistics Book</a></li>
  <li><a href="https://www.boundless.com/statistics/">OpenIntro Statistics Hub</a></li>
</ul>

<h2 id="the-maths">The maths</h2>

<p>Let’s consider the following problem: a software routine is known to return the result of a complex computation in approx. 1.2 seconds. A software engineer tries to optimize the routine and obtained an average over 100 runs of 1.05 seconds, with a standard deviation of 0.5s. Has the routine been improved?</p>

<p><em>Answer:</em></p>

<ul>
  <li>H0: the software performs just as it did before.</li>
  <li>H1: the improvement had an effect (not saying wether improvement or worsening).</li>
</ul>

<p>If we assume H0, that means the mean of the sample should be close enough to the mean of the total distribution.</p>

<p>We define the standard error of the mean for the population <code class="highlighter-rouge">SEM = population standard deviation / SQRT(populatin size)</code></p>

<p>Since we have enough samples in our population, over 30, we can approximate to <code class="highlighter-rouge">SEM = sample standard deviation / SQRT(population size)</code> which leads to <code class="highlighter-rouge">SEM = 0.5/SQRT(100) = 0.5/10 = 0.05</code>.</p>

<p>We have <code class="highlighter-rouge">Z-score = (population mean - sample mean) / SEM = (1.2 - 1.05) / 0.05 = 3</code>, which means that our result (sample mean) is 3 standard deviations from the mean of the population, which means we have a <code class="highlighter-rouge">p-value = 0.3%</code> chance to observe a result as extreme or more extreme to the one we observed, which means we can consider that the improvement worked - we considered an extreme result on both the positive and the negative side (a two-tailed test).</p>

<p>Now, let’s assume the H1 were <em>the software response has improved (lowered)</em>, a one-tailed test, our p-value would have been <code class="highlighter-rouge">0.015%</code>.</p>

<p>Assuming we would have had 10 samples, not 100, the sample mean would not have been normally distributed, but t-distributed and we would have had to use the <em>T-distribution with n-1 degrees of freedom</em> to find the p-values. T-score is computed exactly the same as the Z-score, but the p-values are computed based on the associated T-distribution with n-1 degrees of freedom.</p>

<h2 id="code-example---unfair-coin">Code example - unfair coin</h2>

<p>We are going to test if a coin is unfair (slightly biased towards the head, with a <code class="highlighter-rouge">p(head) = 0.55</code>). We are going to use two sample sizes and see how the sample size affects the results.</p>

<p>Before that, here is some prerequisite code:</p>

<p><em>Notes:</em></p>

<ul>
  <li>
    <p>The coin toss is represented by a <code class="highlighter-rouge">Binomial(n, p)</code> distribution, where <code class="highlighter-rouge">n</code> is the number of trials and <code class="highlighter-rouge">p</code> the probability of hitting head.</p>
  </li>
  <li>
    <p>A common rule of thumb is that if both <code class="highlighter-rouge">n*p</code> and <code class="highlighter-rouge">n * (1 – p)</code> are greater than <code class="highlighter-rouge">5</code>, the binomial distribution may be approximated by the normal distribution.</p>
  </li>
</ul>

<div class="language-python highlighter-rouge"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">math</span>

<span class="c">#The binomial distribution with </span>
<span class="c">#parameters n and p is the discrete probability distribution of the number of </span>
<span class="c">#successes in a sequence of n independent experiments</span>
<span class="k">def</span> <span class="nf">normal_approximation_to_binomial</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="n">p</span><span class="p">):</span>    
    <span class="s">"""finds mu and sigma corresponding to a Binomial(n, p)"""</span> 

    <span class="k">if</span> <span class="n">n</span> <span class="o">*</span> <span class="n">p</span> <span class="o">&lt;</span> <span class="mi">5</span> <span class="ow">or</span> <span class="n">n</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">p</span><span class="p">)</span> <span class="o">&lt;</span> <span class="mi">5</span><span class="p">:</span>
        <span class="k">raise</span> <span class="nb">Exception</span><span class="p">(</span><span class="s">"Cannot be approximated by a normal distribution"</span><span class="p">)</span>
    
    <span class="n">mu</span> <span class="o">=</span> <span class="n">p</span> <span class="o">*</span> <span class="n">n</span>    
    <span class="n">sigma</span> <span class="o">=</span> <span class="n">math</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">p</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">p</span><span class="p">)</span> <span class="o">*</span> <span class="n">n</span><span class="p">)</span>    
    <span class="k">return</span> <span class="n">mu</span><span class="p">,</span> <span class="n">sigma</span>

<span class="k">def</span> <span class="nf">normal_probability_below</span><span class="p">(</span><span class="n">value</span><span class="p">,</span> <span class="n">mu</span><span class="p">,</span> <span class="n">sigma</span><span class="p">):</span>
    <span class="s">""" Considering the normal distribution of coin tosses and the central limit theorem,
    computes the probability of a value to be below a certain given value. Same as normal_cdf"""</span>
    <span class="k">return</span> <span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="n">math</span><span class="o">.</span><span class="n">erf</span><span class="p">((</span><span class="n">value</span> <span class="o">-</span> <span class="n">mu</span><span class="p">)</span> <span class="o">/</span> <span class="n">math</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span> <span class="o">/</span> <span class="n">sigma</span><span class="p">))</span> <span class="o">/</span> <span class="mi">2</span>     

<span class="k">def</span> <span class="nf">normal_probability_above</span><span class="p">(</span><span class="n">value</span><span class="p">,</span> <span class="n">mu</span><span class="p">,</span> <span class="n">sigma</span><span class="p">):</span>
    <span class="k">return</span> <span class="mi">1</span> <span class="o">-</span> <span class="n">normal_probability_below</span><span class="p">(</span><span class="n">value</span><span class="p">,</span> <span class="n">mu</span><span class="p">,</span> <span class="n">sigma</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">normal_probability_between</span><span class="p">(</span><span class="n">v1</span><span class="p">,</span> <span class="n">v2</span><span class="p">,</span> <span class="n">mu</span><span class="p">,</span> <span class="n">sigma</span><span class="p">):</span>
    <span class="n">v1</span><span class="p">,</span> <span class="n">v2</span> <span class="o">=</span> <span class="p">(</span><span class="n">v2</span><span class="p">,</span> <span class="n">v1</span><span class="p">)</span> <span class="k">if</span> <span class="n">v1</span> <span class="o">&gt;</span> <span class="n">v2</span> <span class="k">else</span> <span class="p">(</span><span class="n">v1</span><span class="p">,</span> <span class="n">v2</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">normal_probability_below</span><span class="p">(</span><span class="n">v2</span><span class="p">,</span> <span class="n">mu</span><span class="p">,</span> <span class="n">sigma</span><span class="p">)</span> <span class="o">-</span> <span class="n">normal_probability_below</span><span class="p">(</span><span class="n">v1</span><span class="p">,</span> <span class="n">mu</span><span class="p">,</span> <span class="n">sigma</span><span class="p">)</span>


<span class="c">#Inverse through binary search; not optimal for more than a few values</span>
<span class="k">def</span> <span class="nf">interval_probability_centered</span><span class="p">(</span><span class="n">p</span><span class="p">,</span> <span class="n">mu</span><span class="p">,</span> <span class="n">sigma</span><span class="p">):</span>
    <span class="s">"""
    returns the interval (lower, upper) of values for which the probability 
    of the result to be in it is equal to p.
    """</span>
    <span class="n">hw</span> <span class="o">=</span> <span class="mi">9</span> <span class="o">*</span> <span class="n">sigma</span> <span class="c"># half width of intw</span>
    <span class="n">interval_low</span><span class="p">,</span> <span class="n">interval_high</span> <span class="o">=</span> <span class="n">mu</span> <span class="o">-</span> <span class="n">hw</span><span class="p">,</span> <span class="n">mu</span> <span class="o">+</span> <span class="n">hw</span>
    <span class="n">current_prob</span> <span class="o">=</span> <span class="mf">1.0</span>

    <span class="k">while</span> <span class="nb">abs</span><span class="p">(</span><span class="n">current_prob</span> <span class="o">-</span> <span class="n">p</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mf">1e-12</span><span class="p">:</span>
        <span class="n">hw</span> <span class="o">/=</span> <span class="mi">2</span>
        <span class="k">if</span> <span class="n">p</span> <span class="o">&lt;</span> <span class="n">current_prob</span><span class="p">:</span>
            <span class="n">interval_low</span><span class="p">,</span> <span class="n">interval_high</span> <span class="o">=</span> <span class="n">interval_low</span> <span class="o">+</span> <span class="n">hw</span><span class="p">,</span> <span class="n">interval_high</span> <span class="o">-</span> <span class="n">hw</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">interval_low</span><span class="p">,</span> <span class="n">interval_high</span> <span class="o">=</span> <span class="n">interval_low</span> <span class="o">-</span> <span class="n">hw</span><span class="p">,</span> <span class="n">interval_high</span> <span class="o">+</span> <span class="n">hw</span>

        <span class="n">current_prob</span> <span class="o">=</span> <span class="n">normal_probability_between</span><span class="p">(</span><span class="n">interval_low</span><span class="p">,</span> <span class="n">interval_high</span><span class="p">,</span> <span class="n">mu</span><span class="p">,</span> <span class="n">sigma</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">interval_low</span><span class="p">,</span> <span class="n">interval_high</span>

<span class="k">def</span> <span class="nf">normal_upper_bound</span><span class="p">(</span><span class="n">p</span><span class="p">,</span> <span class="n">mu</span><span class="p">,</span> <span class="n">sigma</span><span class="p">):</span>
    <span class="s">""" interval (-oo, value) for which sum of probabilities == p. Equivalent to inverse_normal_cdf"""</span>   

    <span class="n">delta</span> <span class="o">=</span> <span class="mi">9</span> <span class="o">*</span> <span class="n">sigma</span>
    <span class="n">intw_high</span> <span class="o">=</span> <span class="n">mu</span> <span class="o">+</span> <span class="n">delta</span>
    <span class="n">current_prob</span> <span class="o">=</span> <span class="mf">1.0</span>
    <span class="k">while</span> <span class="nb">abs</span><span class="p">(</span><span class="n">current_prob</span> <span class="o">-</span> <span class="n">p</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mf">1e-12</span><span class="p">:</span>

        <span class="n">delta</span> <span class="o">/=</span> <span class="mi">2</span>

        <span class="k">while</span> <span class="n">p</span> <span class="o">&lt;</span> <span class="n">current_prob</span><span class="p">:</span>
            <span class="n">intw_high</span> <span class="o">-=</span> <span class="n">delta</span>
            <span class="n">current_prob</span> <span class="o">=</span> <span class="n">normal_probability_below</span><span class="p">(</span><span class="n">intw_high</span><span class="p">,</span> <span class="n">mu</span><span class="p">,</span> <span class="n">sigma</span><span class="p">)</span>

        <span class="k">while</span> <span class="n">p</span> <span class="o">&gt;</span> <span class="n">current_prob</span><span class="p">:</span>
            <span class="n">intw_high</span> <span class="o">+=</span> <span class="n">delta</span>
            <span class="n">current_prob</span> <span class="o">=</span> <span class="n">normal_probability_below</span><span class="p">(</span><span class="n">intw_high</span><span class="p">,</span> <span class="n">mu</span><span class="p">,</span> <span class="n">sigma</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">intw_high</span>

<span class="k">def</span> <span class="nf">normal_lower_bound</span><span class="p">(</span><span class="n">p</span><span class="p">,</span> <span class="n">mu</span><span class="p">,</span> <span class="n">sigma</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">normal_upper_bound</span><span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="n">p</span><span class="p">,</span> <span class="n">mu</span><span class="p">,</span> <span class="n">sigma</span><span class="p">)</span>

</code></pre>
</div>

<p>Let’s run some tests with the code above:</p>

<div class="language-python highlighter-rouge"><pre class="highlight"><code><span class="n">normal_approximation_to_binomial</span><span class="p">(</span><span class="mi">1000</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">)</span>
<span class="n">Out</span><span class="p">[</span><span class="mi">47</span><span class="p">]:</span> <span class="p">(</span><span class="mf">500.0</span><span class="p">,</span> <span class="mf">15.811388300841896</span><span class="p">)</span>

<span class="n">normal_approximation_to_binomial</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">)</span>
<span class="n">Out</span><span class="p">[</span><span class="mi">49</span><span class="p">]:</span> <span class="p">(</span><span class="mf">50.0</span><span class="p">,</span> <span class="mf">5.0</span><span class="p">)</span>
</code></pre>
</div>

<p>Considering the coin fair (<code class="highlighter-rouge">p=0.5</code>), the average is 500 heads in case of 1000 trials and 50 heads in case of 100 trials - obviously. The dispersion (sigma), if we consider it as a percentage of the number of trials, is much higher in the case of 100 trials versus the 1000. For 1000 trials, the distribution is significantly narrower.</p>

<p>Assuming 100 trials,</p>

<div class="language-python highlighter-rouge"><pre class="highlight"><code><span class="n">normal_probability_below</span><span class="p">(</span><span class="mi">50</span><span class="p">,</span> <span class="n">mu</span><span class="p">,</span> <span class="n">sigma</span><span class="p">)</span>
<span class="n">Out</span><span class="p">[</span><span class="mi">53</span><span class="p">]:</span> <span class="mf">0.5</span>

<span class="n">normal_probability_between</span><span class="p">(</span><span class="mi">45</span><span class="p">,</span> <span class="mi">55</span><span class="p">,</span> <span class="n">mu</span><span class="p">,</span> <span class="n">sigma</span><span class="p">)</span>
<span class="n">Out</span><span class="p">[</span><span class="mi">54</span><span class="p">]:</span> <span class="mf">0.6826894921370861</span>
</code></pre>
</div>

<p>50% of the results will be below 50 (first line) and the probability for the result (no. of observed heads) to be between 45 and 55 is 0.68.</p>

<div class="language-python highlighter-rouge"><pre class="highlight"><code><span class="n">interval_probability_centered</span><span class="p">(</span><span class="mf">0.95</span><span class="p">,</span> <span class="n">mu</span><span class="p">,</span> <span class="n">sigma</span><span class="p">)</span>
<span class="n">Out</span><span class="p">[</span><span class="mi">55</span><span class="p">]:</span> <span class="p">(</span><span class="mf">40.20018007732688</span><span class="p">,</span> <span class="mf">59.79981992267312</span><span class="p">)</span>
</code></pre>
</div>

<p>For the same <code class="highlighter-rouge">n=100</code> trials, the number of observed heads will be between 40 and 60 for 95% of the experiment runs.</p>

<p>First hypothesis - coin is biased, but we don’t specify towards which side:</p>

<div class="language-python highlighter-rouge"><pre class="highlight"><code><span class="c"># if coin not biased, considering the mu and sigma computed above, for 100 trials and p(head) = 0.5</span>
<span class="n">low</span><span class="p">,</span> <span class="n">high</span> <span class="o">=</span> <span class="n">interval_probability_centered</span><span class="p">(</span><span class="mf">0.95</span><span class="p">,</span> <span class="n">mu</span><span class="p">,</span> <span class="n">sigma</span><span class="p">)</span>

<span class="c"># consider the coin biased towards the head, with p(head) = 0.55</span>
<span class="n">mu_biased</span><span class="p">,</span> <span class="n">sigma_biased</span> <span class="o">=</span> <span class="n">normal_approximation_to_binomial</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="mf">0.55</span><span class="p">)</span>
</code></pre>
</div>

<p>Now we run the following test: what is the probability that we will get a value between <code class="highlighter-rouge">low</code> and <code class="highlighter-rouge">high</code> as the margins of 95% confidence if the coin is unbiased, if we consider the coin biased. As we see below, the higher the number of trials, the higher the confidence we have in rejecting the null hypothesis. Power of the test below is the probability of not making a type 2 error, in which we fail to reject H0 even though it’s false.</p>

<div class="language-python highlighter-rouge"><pre class="highlight"><code><span class="n">normal_probability_between</span><span class="p">(</span><span class="n">low</span><span class="p">,</span> <span class="n">high</span><span class="p">,</span> <span class="n">mu_biased</span><span class="p">,</span> <span class="n">sigma_biased</span><span class="p">)</span>
<span class="n">Out</span><span class="p">[</span><span class="mi">57</span><span class="p">]:</span> <span class="mf">0.8312119922463654</span>

<span class="mi">1</span> <span class="o">-</span> <span class="n">normal_probability_between</span><span class="p">(</span><span class="n">low</span><span class="p">,</span> <span class="n">high</span><span class="p">,</span> <span class="n">mu_biased</span><span class="p">,</span> <span class="n">sigma_biased</span><span class="p">)</span> <span class="c"># power of the test</span>
<span class="n">Out</span><span class="p">[</span><span class="mi">58</span><span class="p">]:</span> <span class="mf">0.16878800775363456</span>
</code></pre>
</div>

<p>Obviously, we cannot discharge the null hypothesis. p-value = 0.83 and the power of the test only 0.168. However, if we run the code for <code class="highlighter-rouge">n=1000</code> and <code class="highlighter-rouge">p_biased(head) = 0.55</code>, we get a different picture due to the more narrow distribution. We still cannot reject the null hypothesis, though.</p>

<div class="language-python highlighter-rouge"><pre class="highlight"><code><span class="n">normal_probability_between</span><span class="p">(</span><span class="n">low</span><span class="p">,</span> <span class="n">high</span><span class="p">,</span> <span class="n">mu_biased</span><span class="p">,</span> <span class="n">sigma_biased</span><span class="p">)</span>
<span class="n">Out</span><span class="p">[</span><span class="mi">62</span><span class="p">]:</span> <span class="mf">0.11345221890056983</span>

<span class="mi">1</span> <span class="o">-</span> <span class="n">normal_probability_between</span><span class="p">(</span><span class="n">low</span><span class="p">,</span> <span class="n">high</span><span class="p">,</span> <span class="n">mu_biased</span><span class="p">,</span> <span class="n">sigma_biased</span><span class="p">)</span> <span class="c"># power of the test</span>
<span class="n">Out</span><span class="p">[</span><span class="mi">63</span><span class="p">]:</span> <span class="mf">0.8865477810994302</span>
</code></pre>
</div>

<p>If we had 2000 trials on the other hand,</p>

<div class="language-python highlighter-rouge"><pre class="highlight"><code><span class="n">normal_probability_between</span><span class="p">(</span><span class="n">low</span><span class="p">,</span> <span class="n">high</span><span class="p">,</span> <span class="n">mu_biased</span><span class="p">,</span> <span class="n">sigma_biased</span><span class="p">)</span>
<span class="n">Out</span><span class="p">[</span><span class="mi">65</span><span class="p">]:</span> <span class="mf">0.005787749170240164</span>

<span class="mi">1</span> <span class="o">-</span> <span class="n">normal_probability_between</span><span class="p">(</span><span class="n">low</span><span class="p">,</span> <span class="n">high</span><span class="p">,</span> <span class="n">mu_biased</span><span class="p">,</span> <span class="n">sigma_biased</span><span class="p">)</span> <span class="c"># power of the test</span>
<span class="n">Out</span><span class="p">[</span><span class="mi">66</span><span class="p">]:</span> <span class="mf">0.9942122508297598</span>
</code></pre>
</div>
<p>We can pretty confidently reject the null hypothesis.</p>

<p>Now we assume the hypothesis we want to test is a little bit more specific - the coin is biased towards the head. The difference from the previous scenario is that now we make a one-sided test. In the previous case we were less specific, thus the two-sided test has wider margins.</p>

<p>Considering 1000 trials,</p>

<div class="language-python highlighter-rouge"><pre class="highlight"><code><span class="n">hi</span> <span class="o">=</span> <span class="n">normal_upper_bound</span><span class="p">(</span><span class="mf">0.95</span><span class="p">,</span> <span class="n">mu</span><span class="p">,</span> <span class="n">sigma</span><span class="p">)</span>

<span class="n">normal_probability_below</span><span class="p">(</span><span class="n">hi</span><span class="p">,</span> <span class="n">mu_biased</span><span class="p">,</span> <span class="n">sigma_biased</span><span class="p">)</span>
<span class="n">Out</span><span class="p">[</span><span class="mi">74</span><span class="p">]:</span> <span class="mf">0.06362100214309152</span>

<span class="mi">1</span> <span class="o">-</span> <span class="mf">0.06362100214309152</span>
<span class="n">Out</span><span class="p">[</span><span class="mi">75</span><span class="p">]:</span> <span class="mf">0.9363789978569085</span>
</code></pre>
</div>

<p>While we still cannot confidently reject the null hypothesis with this test, but it is obviously a more powerful one.</p>

<h2 id="p-values">P-values:</h2>

<p>Instead of choosing bounds based on some probability cutoff, we compute the probability, assuming H0 is true, that we would see a value at least as extreme as the one we actually observed. From wikipedia,</p>

<p><img src="https://upload.wikimedia.org/wikipedia/commons/3/3a/P-value_in_statistical_significance_testing.svg" alt="pvalue" /></p>

<div class="language-python highlighter-rouge"><pre class="highlight"><code><span class="k">def</span> <span class="nf">two_sided_p_value</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">mu</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">sigma</span><span class="o">=</span><span class="mi">1</span><span class="p">):</span>    
    <span class="k">if</span> <span class="n">x</span> <span class="o">&gt;=</span> <span class="n">mu</span><span class="p">:</span>
        <span class="c"># if x is greater than the mean, the tail is what's greater than x        </span>
        <span class="k">return</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">normal_probability_above</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">mu</span><span class="p">,</span> <span class="n">sigma</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="c"># if x is less than the mean, the tail is what's less than x        </span>
        <span class="k">return</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">normal_probability_below</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">mu</span><span class="p">,</span> <span class="n">sigma</span><span class="p">)</span>

<span class="n">upper_p_value</span> <span class="o">=</span> <span class="n">normal_probability_above</span>
<span class="n">lower_p_value</span> <span class="o">=</span> <span class="n">normal_probability_below</span> 
</code></pre>
</div>

<p>The first thing that is obvious is that the <code class="highlighter-rouge">two_sided_p_value</code> features a multiplication by 2, whereas the single-sided functions don’t. We talked about two sided versus one sided tests in the previuos chapter. There is one very important result to consider: we can only decide to opt for a single-sided test <em>IF</em> we didn’t look at the data before. That means, that the hypothesis we want to test is not derived from our understanding of the data, but rather is a pure theoretical (blind) speculation. If we already have an idea of what the data is about, we need to opt for the two sided test. This comes from the fact that the data might be already unintentionally biased towards our result (or in the opposite side of our result) by precisely half of our confidence interval.</p>

<p>Here is a more detailed explanation: <a href="https://www.boundless.com/users/233402/textbooks/openintro-statistics/foundations-for-inference-4/hypothesis-testing-35/two-sided-hypothesis-testing-with-p-values-174-13787/">hypothesis testing with p-values</a></p>

<p>Let’s use the p-values. While in the previous chapter we estimated the validity of the test for a certain hypothesis, now we are looking at actual results. The problem sounds like: assuming that we obtain <code class="highlighter-rouge">530</code> tails in a <code class="highlighter-rouge">1000</code> trials experiment, is the coin biased?</p>

<div class="language-python highlighter-rouge"><pre class="highlight"><code><span class="n">two_sided_p_value</span><span class="p">(</span><span class="mf">529.5</span><span class="p">,</span> <span class="n">mu</span><span class="p">,</span> <span class="n">sigma</span><span class="p">)</span> <span class="c"># continuity correction</span>
<span class="n">Out</span><span class="p">[</span><span class="mi">76</span><span class="p">]:</span> <span class="mf">0.06207721579598857</span>
</code></pre>
</div>

<p>Unfortunately we cannot dismiss the null hypothesis. Had we observed <code class="highlighter-rouge">531</code> tails, then the result would have looked different.</p>

<p><em>Note:</em> decreasing <code class="highlighter-rouge">530</code> by <code class="highlighter-rouge">0.5</code> in the test above is called continuity correction. It basically asserts that <code class="highlighter-rouge">p(530)</code> is better estimated by the average of <code class="highlighter-rouge">p(529.5)</code> and <code class="highlighter-rouge">p(530.5)</code> than by the average of <code class="highlighter-rouge">p(530)</code> and <code class="highlighter-rouge">p(531)</code>. <a href="https://en.wikipedia.org/wiki/Continuity_correction">Continuity Correction - Wikipedia</a></p>

<h2 id="example---ab-testing">Example - A/B testing</h2>

<p>Let’s put all these things together in a worked example - computing the success of an A/B test campaign.</p>

<p>Let’s assume we have a banner for which we want to make a change and we want to understand if the change brings more clicks and thus profit. Our null hypothesis is “no, the change does not impact the number of clicks”.</p>

<p>We split the clients in two cohorts (<code class="highlighter-rouge">A</code> and <code class="highlighter-rouge">B</code>). For cohort <code class="highlighter-rouge">A</code> we keep the old banner, for cohort <code class="highlighter-rouge">B</code> we have the banner changed. After <code class="highlighter-rouge">N(A)</code> views for banner <code class="highlighter-rouge">A</code> we have <code class="highlighter-rouge">n(A)</code> click-throughs and after <code class="highlighter-rouge">N(B)</code> views for banner <code class="highlighter-rouge">B</code> we have <code class="highlighter-rouge">n(B)</code> click-throughs. Let’s assume <code class="highlighter-rouge">N(A) == 1000</code>, <code class="highlighter-rouge">N(B) == 1000</code>, <code class="highlighter-rouge">n(A) == 180</code>, <code class="highlighter-rouge">n(B) == 200</code>. Is <code class="highlighter-rouge">B</code> a better campaign?</p>

<p>Obviously we have a binomial variable with two posible outcomes: click (1) or not click (0), with a probability to click of <code class="highlighter-rouge">p</code>. Thus, for one trial, we have:</p>

<div class="highlighter-rouge"><pre class="highlight"><code>mu = p #expected value
sigma = sqrt(1/2 * ((0 - p)^2 + (1 - p)^2)) = sqrt(p * (1 - p))
</code></pre>
</div>

<p>For <code class="highlighter-rouge">N</code> trials we apply the central limit theorem which states:</p>

<div class="highlighter-rouge"><pre class="highlight"><code>mu_clt = 1/N * (x0 + x1 +... + xN) = 1/N * (1 + 0 + 0 + ... +1) = n/N = mu = p
sigma_clt = sigma / sqrt(N) = sqrt(p * (1 - p)) / sqrt(N) = sqrt(p * (1 - p) / N)
</code></pre>
</div>

<p>Now we want to test that distribution for <code class="highlighter-rouge">A</code> is the same as distribution for <code class="highlighter-rouge">B</code>, that is <code class="highlighter-rouge">p(A) == p(B)</code> - null hypothesis.</p>

<p><code class="highlighter-rouge">p(A) == p(B)</code> means <code class="highlighter-rouge">p(A) - p(B) == 0</code> or, more precisely, has <code class="highlighter-rouge">mu == 0</code>. But <code class="highlighter-rouge">p(A) - p(B)</code> <a href="https://en.wikipedia.org/wiki/Normal_distribution#Operations_on_normal_deviates">is a normally distributed random variable</a>, thus</p>

<div class="highlighter-rouge"><pre class="highlight"><code>mu(p(A) - p(B)) = mu(A) - mu(B)
sigma(p(A) - p(B)) = sqrt(sigma(A) ^ 2 + sigma(B) ^ 2)
</code></pre>
</div>

<p>If we are to <a href="http://stattrek.com/statistics/dictionary.aspx?definition=Normal_random_variable"><code class="highlighter-rouge">z-score</code></a> this distribution, we should obtain a distribution with <code class="highlighter-rouge">mu == 0</code> and <code class="highlighter-rouge">sigma == 1</code> - which is a mathematical expression of our null hypothesis. If our experimental <code class="highlighter-rouge">p(A) - p(B)</code> is within the constraints of this distribution, then <code class="highlighter-rouge">B</code> most likely does not provide any improvement (or worsening) over <code class="highlighter-rouge">A</code>. Now we can use our <code class="highlighter-rouge">two_sided_p_value</code> to test our hypothesis.</p>

<p>The code:</p>

<div class="language-python highlighter-rouge"><pre class="highlight"><code><span class="k">def</span> <span class="nf">abtest_estimated_params_trial</span><span class="p">(</span><span class="n">N</span><span class="p">,</span> <span class="n">n</span><span class="p">):</span>
    <span class="n">mu</span> <span class="o">=</span> <span class="n">n</span> <span class="o">/</span> <span class="n">N</span>
    <span class="n">sigma</span> <span class="o">=</span> <span class="n">math</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">mu</span> <span class="o">*</span> <span class="p">(</span> <span class="mi">1</span><span class="o">-</span> <span class="n">mu</span><span class="p">)</span> <span class="o">/</span> <span class="n">N</span><span class="p">)</span>
    <span class="k">return</span> <span class="p">(</span><span class="n">mu</span><span class="p">,</span> <span class="n">sigma</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">pA_minus_pB</span><span class="p">(</span><span class="n">Na</span><span class="p">,</span> <span class="n">na</span><span class="p">,</span> <span class="n">Nb</span><span class="p">,</span> <span class="n">nb</span><span class="p">):</span>
    <span class="n">muA</span><span class="p">,</span> <span class="n">sigmaA</span> <span class="o">=</span> <span class="n">abtest_estimated_params_trial</span><span class="p">(</span><span class="n">Na</span><span class="p">,</span> <span class="n">na</span><span class="p">)</span>
    <span class="n">muB</span><span class="p">,</span> <span class="n">sigmaB</span> <span class="o">=</span> <span class="n">abtest_estimated_params_trial</span><span class="p">(</span><span class="n">Nb</span><span class="p">,</span> <span class="n">nb</span><span class="p">)</span>
    
    <span class="c"># the following number should be normally distributed with mu = 0 and sigma == 1</span>
    <span class="k">return</span> <span class="p">(</span><span class="n">muA</span> <span class="o">-</span> <span class="n">muB</span><span class="p">)</span> <span class="o">/</span> <span class="n">math</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">sigmaA</span> <span class="o">**</span> <span class="mi">2</span> <span class="o">+</span> <span class="n">sigmaB</span> <span class="o">**</span> <span class="mi">2</span><span class="p">)</span>

<span class="n">two_sided_p_value</span><span class="p">(</span><span class="n">pA_minus_pB</span><span class="p">(</span><span class="mi">1000</span><span class="p">,</span> <span class="mi">200</span><span class="p">,</span> <span class="mi">1000</span><span class="p">,</span> <span class="mi">150</span><span class="p">),</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
</code></pre>
</div>

<p>With the results:</p>

<div class="language-python highlighter-rouge"><pre class="highlight"><code><span class="n">two_sided_p_value</span><span class="p">(</span><span class="n">pA_minus_pB</span><span class="p">(</span><span class="mi">1000</span><span class="p">,</span> <span class="mi">200</span><span class="p">,</span> <span class="mi">1000</span><span class="p">,</span> <span class="mi">250</span><span class="p">),</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">Out</span><span class="p">[</span><span class="mi">6</span><span class="p">]:</span> <span class="mf">0.007313777221710671</span> <span class="c"># &gt; 0.05, we can safely reject the null hypothesis</span>

<span class="n">two_sided_p_value</span><span class="p">(</span><span class="n">pA_minus_pB</span><span class="p">(</span><span class="mi">1000</span><span class="p">,</span> <span class="mi">200</span><span class="p">,</span> <span class="mi">1000</span><span class="p">,</span> <span class="mi">350</span><span class="p">),</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">Out</span><span class="p">[</span><span class="mi">7</span><span class="p">]:</span> <span class="mf">2.531308496145357e-14</span> <span class="c"># &gt; 0.5, we can safely reject the null hypothesis</span>

<span class="n">two_sided_p_value</span><span class="p">(</span><span class="n">pA_minus_pB</span><span class="p">(</span><span class="mi">1000</span><span class="p">,</span> <span class="mi">200</span><span class="p">,</span> <span class="mi">1000</span><span class="p">,</span> <span class="mi">210</span><span class="p">),</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">Out</span><span class="p">[</span><span class="mi">8</span><span class="p">]:</span> <span class="mf">0.5796241923602059</span> <span class="c"># cannot reject the null</span>

<span class="n">two_sided_p_value</span><span class="p">(</span><span class="n">pA_minus_pB</span><span class="p">(</span><span class="mi">1000</span><span class="p">,</span> <span class="mi">200</span><span class="p">,</span> <span class="mi">1000</span><span class="p">,</span> <span class="mi">180</span><span class="p">),</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">Out</span><span class="p">[</span><span class="mi">9</span><span class="p">]:</span> <span class="mf">0.25414197654223614</span> <span class="c"># cannot reject the null</span>

<span class="n">two_sided_p_value</span><span class="p">(</span><span class="n">pA_minus_pB</span><span class="p">(</span><span class="mi">1000</span><span class="p">,</span> <span class="mi">200</span><span class="p">,</span> <span class="mi">1000</span><span class="p">,</span> <span class="mi">170</span><span class="p">),</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">Out</span><span class="p">[</span><span class="mi">10</span><span class="p">]:</span> <span class="mf">0.08382984264631776</span> <span class="c"># still cannot reject the null</span>

<span class="n">two_sided_p_value</span><span class="p">(</span><span class="n">pA_minus_pB</span><span class="p">(</span><span class="mi">1000</span><span class="p">,</span> <span class="mi">200</span><span class="p">,</span> <span class="mi">1000</span><span class="p">,</span> <span class="mi">150</span><span class="p">),</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">Out</span><span class="p">[</span><span class="mi">11</span><span class="p">]:</span> <span class="mf">0.003189699706216853</span> <span class="c"># we can safely reject the null</span>
</code></pre>
</div>


  </div>

  
</article>

      </div>
    </main>

    <footer class="site-footer">

  <div class="wrapper">

    <h2 class="footer-heading">From The Trenches - The Code</h2>

    <div class="footer-col-wrapper">
      <div class="footer-col footer-col-1">
        <ul class="contact-list">
          <li>
            
              From The Trenches - The Code
            
            </li>
            
            <li><a href="mailto:alexandru.gris2006@gmail.com">alexandru.gris2006@gmail.com</a></li>
            
        </ul>
      </div>

      <div class="footer-col footer-col-2">
        <ul class="social-media-list">
          
          <li>
            <a href="https://github.com/alexandrugris"><span class="icon icon--github"><svg viewBox="0 0 16 16" width="16px" height="16px"><path fill="#828282" d="M7.999,0.431c-4.285,0-7.76,3.474-7.76,7.761 c0,3.428,2.223,6.337,5.307,7.363c0.388,0.071,0.53-0.168,0.53-0.374c0-0.184-0.007-0.672-0.01-1.32 c-2.159,0.469-2.614-1.04-2.614-1.04c-0.353-0.896-0.862-1.135-0.862-1.135c-0.705-0.481,0.053-0.472,0.053-0.472 c0.779,0.055,1.189,0.8,1.189,0.8c0.692,1.186,1.816,0.843,2.258,0.645c0.071-0.502,0.271-0.843,0.493-1.037 C4.86,11.425,3.049,10.76,3.049,7.786c0-0.847,0.302-1.54,0.799-2.082C3.768,5.507,3.501,4.718,3.924,3.65 c0,0,0.652-0.209,2.134,0.796C6.677,4.273,7.34,4.187,8,4.184c0.659,0.003,1.323,0.089,1.943,0.261 c1.482-1.004,2.132-0.796,2.132-0.796c0.423,1.068,0.157,1.857,0.077,2.054c0.497,0.542,0.798,1.235,0.798,2.082 c0,2.981-1.814,3.637-3.543,3.829c0.279,0.24,0.527,0.713,0.527,1.437c0,1.037-0.01,1.874-0.01,2.129 c0,0.208,0.14,0.449,0.534,0.373c3.081-1.028,5.302-3.935,5.302-7.362C15.76,3.906,12.285,0.431,7.999,0.431z"/></svg>
</span><span class="username">alexandrugris</span></a>

          </li>
          

          
          <li>
            <a href="https://twitter.com/alexandrugris"><span class="icon icon--twitter"><svg viewBox="0 0 16 16" width="16px" height="16px"><path fill="#828282" d="M15.969,3.058c-0.586,0.26-1.217,0.436-1.878,0.515c0.675-0.405,1.194-1.045,1.438-1.809c-0.632,0.375-1.332,0.647-2.076,0.793c-0.596-0.636-1.446-1.033-2.387-1.033c-1.806,0-3.27,1.464-3.27,3.27 c0,0.256,0.029,0.506,0.085,0.745C5.163,5.404,2.753,4.102,1.14,2.124C0.859,2.607,0.698,3.168,0.698,3.767 c0,1.134,0.577,2.135,1.455,2.722C1.616,6.472,1.112,6.325,0.671,6.08c0,0.014,0,0.027,0,0.041c0,1.584,1.127,2.906,2.623,3.206 C3.02,9.402,2.731,9.442,2.433,9.442c-0.211,0-0.416-0.021-0.615-0.059c0.416,1.299,1.624,2.245,3.055,2.271 c-1.119,0.877-2.529,1.4-4.061,1.4c-0.264,0-0.524-0.015-0.78-0.046c1.447,0.928,3.166,1.469,5.013,1.469 c6.015,0,9.304-4.983,9.304-9.304c0-0.142-0.003-0.283-0.009-0.423C14.976,4.29,15.531,3.714,15.969,3.058z"/></svg>
</span><span class="username">alexandrugris</span></a>

          </li>
          
        </ul>
      </div>

      <div class="footer-col footer-col-3">
        <p>Alexandru Gris - Personal Blog
</p>
      </div>
    </div>

  </div>

</footer>


  </body>

</html>
