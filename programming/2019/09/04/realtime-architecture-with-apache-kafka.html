<!DOCTYPE html>
<html lang="en">

  <head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <title>Streaming Architecture Using Apache Kafka And Redis</title>
  <meta name="description" content="This post describes an architecture based on Apache Kafka and Redis that can be applied to building high performing, resilient streaming systems. It applies ...">

  <link rel="stylesheet" href="/assets/main.css">
  <link rel="canonical" href="https://alexandrugris.github.io/programming/2019/09/04/realtime-architecture-with-apache-kafka.html">
  <link rel="alternate" type="application/rss+xml" title="From The Trenches - The Code" href="/feed.xml">
  
  
</head>


  <body>

    <header class="site-header" role="banner">

  <div class="wrapper">

    <div class="site-nav"><a class="site-title" href="/">From The Trenches - The Code</a> </div>

    <nav class="site-nav">
      <span class="menu-icon">        
      </span>

      <div class="trigger">

        <a class="page-link" href="https://alexandrugris.github.io">Home</a>

        
          
          <a class="page-link" href="/about/">About</a>
          
        
          
        
          
        
          
          <a class="page-link" href="/sm/">Social Media</a>
          
        
          
        
      </div>
    </nav>

  </div>

</header>


    <main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title" itemprop="name headline">Streaming Architecture Using Apache Kafka And Redis</h1>
    <p class="post-meta"><time datetime="2019-09-04T13:15:16+02:00" itemprop="datePublished">Sep 4, 2019</time></p>
  </header>

  <div class="post-content" itemprop="articleBody">
    <p>This post describes an architecture based on Apache Kafka and Redis that can be applied to building high performing, resilient streaming systems.  It applies to near-realtime systems, where a stream of events needs to be processed and the results submitted to a large list of subscribers, each of them receiving its own view of the stream.</p>

<p>Examples might include the following:</p>
<ul>
  <li>Streaming bookmaker odds - different users are browsing different parts of the site and have different markets added to their bet slips</li>
  <li>Realtime games - based on player input and game rules, for each player a different world view is computed</li>
  <li>Subscription-based data distribution, each consumer receiving a partition of the total data set</li>
</ul>

<p>The architecture assumes large data volumes, potentially with a computationally intensive step required to compute the individual view. The architecture assumes the reducers, the components responsible for computation, can scale independently and recover from failures by restarts. Their stateless nature and dynamic scaling makes them very suitable for a deploying in a Kubernetes cluster.</p>

<p><img src="https://alexandrugris.github.io/assets/arch_1.png" alt="Architecture" /></p>

<p>The picture above uses the terminology of a streaming system for distributing bookmaker odds to all connected web and mobile clients.</p>

<p>Overall, the system is composed of several parts working together and scaling independently:</p>

<ul>
  <li>A stream control API: can be implemented as a normal REST service, load balanced.</li>
  <li>A stream publisher: it accepts WebSockets connections, load balanced, individual connections can land on any machine.</li>
  <li>A Redis PUB-SUB component where channels reside. This can, eventually, be sharded or replaced with a RabbitMQ cluster. The stream publisher and the Redis PUB-SUB can be replaced with <code class="language-plaintext highlighter-rouge">socket.io</code>. It uses the same principle underneath.</li>
  <li>A Kafka queue with two topics: one for stream commands, which are distributed to all reducers, and a partitioned topic, from which all reducers consume their individual partitions without overlap (let’s call it the data topic). This topic receives the largest amount of data. For good load balancing, it is recommended that it has a high number of partitions.</li>
  <li>The reducers themselves, consuming non-overlapping partitions from the data topic.</li>
  <li>A state store, can be either a HA Redis cluster, MongoDB or any very fast key-value sture.</li>
</ul>

<h3 id="apache-kafka">Apache Kafka</h3>

<p>Apache Kafka allows to store the incoming data stream and computation results used for later stages in the pipeline in a fault tolerant, distributed way. Apache Kafka also allows brining new servers to the system in case of high data load.</p>

<ul>
  <li>In case of replication, each partition will have one server as the leader and configurable other as followers. Leader is managing read / write requests for a specific partition, while followers are managing replication of the leader.</li>
  <li>In Kafka, leadership is defined per partition. That means that a server can be leader for a partition but a follower on another.</li>
  <li>Zookeeper stores consumer offsets per topics.</li>
</ul>

<p>A simple way to use start with Kafka for fun projects at home is to use a docker-compose with a setup similar to the following:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>version : '3.5'

services:
  zookeeper:
    image: "confluentinc/cp-zookeeper"
    environment:
      - ZOOKEEPER_CLIENT_PORT=2181 

  kafka:
    image: "confluentinc/cp-kafka"
    environment:
      - KAFKA_ZOOKEEPER_CONNECT=zookeeper:2181
      - KAFKA_ADVERTISED_LISTENERS=PLAINTEXT://kafka:9092
      - KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR=1

  kafka_rest:
    image: "confluentinc/cp-kafka-rest"
    ports:
      - "8082:8082"
    environment: 
      - KAFKA_REST_ZOOKEEPER_CONNECT=zookeeper:2181
      - KAFKA_REST_LISTENERS=http://0.0.0.0:8082
      - KAFKA_REST_HOST_NAME=kafka_rest

networks:
  default:
    name: oda
</code></pre></div></div>

<p>Please note that a race condition in the dockerfile above (Zookeeper starts slower than Kafka) leads Kafka service to fail to start and it must me restarted with <code class="language-plaintext highlighter-rouge">docker-compose scale kafka=1</code>. Once the service is up, we can test the configuration as follows:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>docker run --net=oda --rm confluentinc/cp-kafka bash -c "seq 42 | kafka-console-producer --request-required-acks 1 --broker-list kafka:9092 --topic foo &amp;&amp; echo 'Produced 42 messages.'"

docker run --net=oda --rm confluentinc/cp-kafka kafka-console-consumer --bootstrap-server kafka:9092 --topic foo --from-beginning --max-messages 42
</code></pre></div></div>

<p>Below, a short node script to simplify playing with kafka console tools by providing default parameters to work with the dockerfile presented above:</p>

<div class="language-javascript highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="cp">#!/usr/local/bin/node
</span>
<span class="kd">let</span> <span class="nx">child_process</span> <span class="o">=</span> <span class="nx">require</span><span class="p">(</span><span class="dl">'</span><span class="s1">child_process</span><span class="dl">'</span><span class="p">);</span>

<span class="kd">let</span> <span class="nx">cmds</span> <span class="o">=</span> <span class="p">{</span>
    <span class="dl">'</span><span class="s1">topics</span><span class="dl">'</span> <span class="p">:</span> <span class="p">[</span><span class="dl">'</span><span class="s1">kafka-topics</span><span class="dl">'</span><span class="p">,</span> <span class="dl">'</span><span class="s1">--zookeeper</span><span class="dl">'</span><span class="p">,</span> <span class="dl">'</span><span class="s1">zookeeper:2181</span><span class="dl">'</span><span class="p">],</span>
    <span class="dl">'</span><span class="s1">produce</span><span class="dl">'</span> <span class="p">:</span> <span class="p">[</span><span class="dl">'</span><span class="s1">kafka-console-producer</span><span class="dl">'</span><span class="p">,</span> <span class="dl">'</span><span class="s1">--broker-list</span><span class="dl">'</span><span class="p">,</span> <span class="dl">'</span><span class="s1">kafka:9092</span><span class="dl">'</span><span class="p">],</span>
    <span class="dl">'</span><span class="s1">consume</span><span class="dl">'</span> <span class="p">:</span> <span class="p">[</span><span class="dl">'</span><span class="s1">kafka-console-consumer</span><span class="dl">'</span><span class="p">,</span> <span class="dl">'</span><span class="s1">--bootstrap-server</span><span class="dl">'</span><span class="p">,</span> <span class="dl">'</span><span class="s1">kafka:9092</span><span class="dl">'</span><span class="p">]</span>
<span class="p">}</span>

<span class="kd">let</span> <span class="nx">params</span> <span class="o">=</span> <span class="kc">null</span><span class="p">;</span>

<span class="nx">process</span><span class="p">.</span><span class="nx">argv</span><span class="p">.</span><span class="nx">forEach</span><span class="p">((</span><span class="nx">arg</span><span class="p">)</span> <span class="o">=&gt;</span> <span class="p">{</span>
    <span class="c1">// first with command</span>
    <span class="k">if</span> <span class="p">(</span><span class="nx">params</span> <span class="o">==</span> <span class="kc">null</span> <span class="o">&amp;&amp;</span> <span class="nx">cmds</span><span class="p">[</span><span class="nx">arg</span><span class="p">]</span> <span class="o">!=</span> <span class="kc">null</span><span class="p">)</span> <span class="p">{</span>
        <span class="kd">let</span> <span class="nx">cmd</span> <span class="o">=</span> <span class="nx">cmds</span><span class="p">[</span><span class="nx">arg</span><span class="p">];</span>
        <span class="nx">params</span> <span class="o">=</span> <span class="p">[</span><span class="dl">'</span><span class="s1">run</span><span class="dl">'</span><span class="p">,</span> <span class="dl">'</span><span class="s1">--net=oda</span><span class="dl">'</span><span class="p">,</span> <span class="dl">'</span><span class="s1">--rm</span><span class="dl">'</span><span class="p">,</span> <span class="dl">'</span><span class="s1">-it</span><span class="dl">'</span><span class="p">,</span> <span class="dl">'</span><span class="s1">confluentinc/cp-kafka</span><span class="dl">'</span><span class="p">,</span> <span class="p">...</span><span class="nx">cmd</span><span class="p">]</span>
    <span class="p">}</span>
    <span class="c1">// add the rest</span>
    <span class="k">if</span> <span class="p">(</span><span class="nx">params</span> <span class="o">!=</span> <span class="kc">null</span><span class="p">){</span>
        <span class="nx">params</span><span class="p">.</span><span class="nx">push</span><span class="p">(</span><span class="nx">arg</span><span class="p">);</span>
    <span class="p">}</span>
<span class="p">});</span>

<span class="kd">let</span> <span class="nx">docker</span> <span class="o">=</span> <span class="nx">child_process</span><span class="p">.</span><span class="nx">spawn</span><span class="p">(</span><span class="dl">'</span><span class="s1">docker</span><span class="dl">'</span><span class="p">,</span> <span class="nx">params</span><span class="p">,</span> <span class="p">{</span><span class="na">stdio</span><span class="p">:</span> <span class="dl">'</span><span class="s1">inherit</span><span class="dl">'</span><span class="p">});</span>

<span class="nx">docker</span><span class="p">.</span><span class="nx">on</span><span class="p">(</span><span class="dl">'</span><span class="s1">error</span><span class="dl">'</span><span class="p">,</span> <span class="p">(</span><span class="nx">err</span><span class="p">)</span> <span class="o">=&gt;</span> <span class="p">{</span>
    <span class="nx">console</span><span class="p">.</span><span class="nx">log</span><span class="p">(</span><span class="dl">'</span><span class="s1">Failed to start docker</span><span class="dl">'</span><span class="p">);</span>
<span class="p">});</span>

<span class="nx">docker</span><span class="p">.</span><span class="nx">on</span><span class="p">(</span><span class="dl">'</span><span class="s1">close</span><span class="dl">'</span><span class="p">,</span> <span class="p">(</span><span class="nx">code</span><span class="p">)</span> <span class="o">=&gt;</span> <span class="p">{</span>
    <span class="nx">console</span><span class="p">.</span><span class="nx">log</span><span class="p">(</span><span class="s2">`Child process exited with code </span><span class="p">${</span><span class="nx">code</span><span class="p">}</span><span class="s2">`</span><span class="p">);</span>
<span class="p">});</span>
</code></pre></div></div>

<p>This allows easy testing as follows:</p>

<ul>
  <li>To create a topic named <code class="language-plaintext highlighter-rouge">test</code>: <code class="language-plaintext highlighter-rouge">./kafka.js topics --create --topic test --replication-factor 3 --partitions 3</code></li>
  <li>To list the topics: <code class="language-plaintext highlighter-rouge">./kafka.js topics --list</code></li>
  <li>To produce some messages: <code class="language-plaintext highlighter-rouge">./kafka.js produce --topic test</code></li>
  <li>To read the messages from the beginning: <code class="language-plaintext highlighter-rouge">./kafka.js consume --topic test --from-beginning</code></li>
</ul>

<p>Zookeeper can and should be scaled also. Due the quorum required, the formula for determining the number of Zookeeper instances to run is <code class="language-plaintext highlighter-rouge">2 * F + 1</code> where <code class="language-plaintext highlighter-rouge">F</code> is the desired fault tolerance factor.</p>

<p>To test that indeed data is moving through the system, let’s write the code for the Kafka Producer. I will use a very basic NodeJS package which connects to Kafka’s REST API. For production use, the more advanced packages like <code class="language-plaintext highlighter-rouge">kafka-node</code> and the fast growing at the moment of writing this <code class="language-plaintext highlighter-rouge">kafkajs</code> should be preferred. I use here the <code class="language-plaintext highlighter-rouge">kafka-rest</code> package for simplicity and convenience.</p>

<div class="language-javascript highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kd">const</span> <span class="nx">KafkaRest</span> <span class="o">=</span> <span class="nx">require</span><span class="p">(</span><span class="dl">'</span><span class="s1">kafka-rest</span><span class="dl">'</span><span class="p">);</span>

<span class="c1">// kafka rest is exposed from docker-compose on 8082</span>
<span class="kd">const</span> <span class="nx">kafka</span> <span class="o">=</span> <span class="k">new</span> <span class="nx">KafkaRest</span><span class="p">({</span> <span class="dl">'</span><span class="s1">url</span><span class="dl">'</span><span class="p">:</span> <span class="dl">'</span><span class="s1">http://localhost:8082</span><span class="dl">'</span> <span class="p">});</span>

<span class="c1">// make sure the topic is created before</span>
<span class="kd">const</span> <span class="nx">target</span> <span class="o">=</span> <span class="nx">kafka</span><span class="p">.</span><span class="nx">topic</span><span class="p">(</span><span class="dl">'</span><span class="s1">random_walk</span><span class="dl">'</span><span class="p">);</span>

<span class="kd">const</span> <span class="nx">randWalker</span> <span class="o">=</span> <span class="kd">function</span><span class="p">(){</span>

    <span class="kd">function</span> <span class="nx">clamp</span><span class="p">(</span><span class="nx">v</span><span class="p">,</span> <span class="nx">min</span><span class="p">,</span> <span class="nx">max</span><span class="p">){</span>
        <span class="k">if</span> <span class="p">(</span><span class="nx">v</span> <span class="o">&lt;</span> <span class="nx">min</span><span class="p">)</span> <span class="k">return</span> <span class="nx">min</span><span class="p">;</span>
        <span class="k">if</span> <span class="p">(</span><span class="nx">v</span> <span class="o">&gt;</span> <span class="nx">max</span><span class="p">)</span> <span class="k">return</span> <span class="nx">max</span><span class="p">;</span>
        <span class="k">return</span> <span class="nx">v</span><span class="p">;</span>
    <span class="p">}</span>

    <span class="kd">let</span> <span class="nx">dir</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span>
    <span class="kd">let</span> <span class="nx">prevStep</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span>
    <span class="kd">const</span> <span class="nx">rnd</span> <span class="o">=</span> <span class="nx">require</span><span class="p">(</span><span class="dl">'</span><span class="s1">random</span><span class="dl">'</span><span class="p">);</span>

    <span class="k">return</span> <span class="p">{</span> 
        <span class="nx">randomWalk</span><span class="p">(){</span>
            <span class="nx">dir</span> <span class="o">=</span> <span class="nx">clamp</span><span class="p">(</span><span class="nx">rnd</span><span class="p">.</span><span class="nx">normal</span><span class="p">(</span><span class="nx">dir</span><span class="p">,</span> <span class="mi">1</span><span class="p">)(),</span> <span class="o">-</span><span class="mf">0.75</span><span class="p">,</span> <span class="mf">0.75</span><span class="p">);</span>
            <span class="nx">prevStep</span> <span class="o">+=</span> <span class="nx">dir</span><span class="p">;</span>
            <span class="k">return</span> <span class="nx">prevStep</span><span class="p">;</span>
        <span class="p">}</span>
    <span class="p">}</span>
<span class="p">}();</span>

<span class="nx">setInterval</span><span class="p">(()</span><span class="o">=&gt;</span> <span class="p">{</span>
    <span class="c1">// very basic producer, no key, no partitions, just straight round robin</span>
    <span class="nx">target</span><span class="p">.</span><span class="nx">produce</span><span class="p">(</span> <span class="nx">randWalker</span><span class="p">.</span><span class="nx">randomWalk</span><span class="p">().</span><span class="nx">toFixed</span><span class="p">(</span><span class="mi">4</span><span class="p">)</span> <span class="p">);</span>
<span class="p">},</span> <span class="mi">1000</span><span class="p">);</span>
</code></pre></div></div>

<p>A simple consumer can be implemented as follows:</p>

<div class="language-javascript highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kd">const</span> <span class="nx">KafkaRest</span> <span class="o">=</span> <span class="nx">require</span><span class="p">(</span><span class="dl">'</span><span class="s1">kafka-rest</span><span class="dl">'</span><span class="p">);</span>

<span class="kd">const</span> <span class="nx">kafka</span> <span class="o">=</span> <span class="k">new</span> <span class="nx">KafkaRest</span><span class="p">({</span> <span class="dl">'</span><span class="s1">url</span><span class="dl">'</span><span class="p">:</span> <span class="dl">'</span><span class="s1">http://localhost:8082</span><span class="dl">'</span> <span class="p">});</span>

 <span class="c1">// start reading from the beginning</span>
<span class="kd">let</span> <span class="nx">consumerConfig</span> <span class="o">=</span> <span class="p">{</span>
    <span class="dl">'</span><span class="s1">auto.offset.reset</span><span class="dl">'</span> <span class="p">:</span> <span class="dl">'</span><span class="s1">smallest</span><span class="dl">'</span>
<span class="p">};</span>

<span class="c1">// join a consumer group on the matches topic</span>
<span class="nx">kafka</span><span class="p">.</span><span class="nx">consumer</span><span class="p">(</span><span class="dl">"</span><span class="s2">consumer-group</span><span class="dl">"</span><span class="p">).</span><span class="nx">join</span><span class="p">(</span><span class="nx">consumerConfig</span><span class="p">,</span> <span class="kd">function</span><span class="p">(</span><span class="nx">err</span><span class="p">,</span> <span class="nx">instance</span><span class="p">)</span> <span class="p">{</span>
      <span class="k">if</span> <span class="p">(</span><span class="nx">err</span><span class="p">)</span> 
        <span class="k">return</span> <span class="nx">console</span><span class="p">.</span><span class="nx">log</span><span class="p">(</span><span class="dl">"</span><span class="s2">Failed to create instance in consumer group: </span><span class="dl">"</span> <span class="o">+</span> <span class="nx">err</span><span class="p">);</span>

      <span class="nx">console</span><span class="p">.</span><span class="nx">log</span><span class="p">(</span><span class="dl">"</span><span class="s2">Consumer instance initialized: </span><span class="dl">"</span> <span class="o">+</span> <span class="nx">instance</span><span class="p">.</span><span class="nx">toString</span><span class="p">());</span>
      <span class="kd">const</span> <span class="nx">stream</span> <span class="o">=</span> <span class="nx">instance</span><span class="p">.</span><span class="nx">subscribe</span><span class="p">(</span><span class="dl">"</span><span class="s2">random_walk</span><span class="dl">"</span><span class="p">);</span>

      <span class="nx">stream</span><span class="p">.</span><span class="nx">on</span><span class="p">(</span><span class="dl">'</span><span class="s1">data</span><span class="dl">'</span><span class="p">,</span> <span class="kd">function</span><span class="p">(</span><span class="nx">msgs</span><span class="p">)</span> <span class="p">{</span>

          <span class="k">for</span><span class="p">(</span><span class="kd">var</span> <span class="nx">i</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="nx">i</span> <span class="o">&lt;</span> <span class="nx">msgs</span><span class="p">.</span><span class="nx">length</span><span class="p">;</span> <span class="nx">i</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
              <span class="kd">let</span> <span class="nx">key</span> <span class="o">=</span> <span class="nx">msgs</span><span class="p">[</span><span class="nx">i</span><span class="p">].</span><span class="nx">key</span><span class="p">.</span><span class="nx">toString</span><span class="p">(</span><span class="dl">'</span><span class="s1">utf8</span><span class="dl">'</span><span class="p">);</span>
              <span class="kd">let</span> <span class="nx">value</span> <span class="o">=</span> <span class="nx">msgs</span><span class="p">[</span><span class="nx">i</span><span class="p">].</span><span class="nx">value</span><span class="p">.</span><span class="nx">toString</span><span class="p">(</span><span class="dl">'</span><span class="s1">utf8</span><span class="dl">'</span><span class="p">);</span>
              <span class="nx">console</span><span class="p">.</span><span class="nx">log</span><span class="p">(</span><span class="s2">`</span><span class="p">${</span><span class="nx">key</span><span class="p">}</span><span class="s2"> : </span><span class="p">${</span><span class="nx">value</span><span class="p">}</span><span class="s2">`</span><span class="p">);</span>

          <span class="p">}</span>

      <span class="p">});</span>

  <span class="p">});</span>
</code></pre></div></div>

<h3 id="consumer-groups">Consumer Groups</h3>

<p>Consumer Groups provide the core abstraction on which the proposed architecture is built. Consumer Groups allow a set of concurrent processes to consume messages from a Kafka topic while, at the same time, guaranteeing that no two consumers will be allocated the same list of partitions. This allows seamless scaling up and down of the consumer group as the result of fluctuating traffic, as well as due to restarts caused by consumer crashes. Consumers from a consumer group receive a <code class="language-plaintext highlighter-rouge">rebalance</code> notification callback with a list of partitions that are allocated to them, and they can resume consuming either from the beginning of the partition, from the last committed offset by another member of the group or from a self-managed offset.</p>

<p>The code above allows a very easy way to check how consumer groups work. Just start several competing consumers and kill one of them or restart it later. Since restart policy is set to the beginning of the topic, <code class="language-plaintext highlighter-rouge">'auto.offset.reset' : 'smallest'</code> consuming will start every time from the beginning of each partition.</p>

<h3 id="note-on-redis">Note on Redis</h3>

<p>The architecture can achieve its highest throughput only by using Redis pipelines to batch updates to Redis as much as possible.</p>

<p>For additional scenarios, Kafka offsets might be committed only after the Redis is updated, trading throughput for correctness.</p>

<h3 id="main-command-and-data-flows">Main Command and Data Flows</h3>

<p>In this section we will take the odds update scenario (sportsbook), in which the updates need to be pushed to the listening front-ends as fast as possible.</p>

<p><em>The simple scenario: the user wants to subscribe to changes to a single market</em></p>

<ol>
  <li>Subscriber issues a “subscribe” command to the REST control API and he is issued a unique channel ID (for example, a GUID)</li>
  <li>The subscribe command is issued further to all the reducers through a fan-out mechanism (command topic).</li>
  <li>The subscriber opens a websocket to the Stream Publisher and requests that its connection is mapped to the channel id. The Stream Publisher subscribes to the Redis PUB-SUB channel with that specific connection-id. So far no data has been published to the Redis PUB-SUB.</li>
  <li>Once the subscribed receives ACK that the connection has been established, it issues another command, “begin stream” to the command API. This is done to instruct the reducers to compute the initial state and send it through the pub-sub after the subscriber has opened the connection, so no updates are lost.</li>
  <li>Reducers maintain two maps: market ids to chanel id and channel ids to markets, so that for each incoming market it is directed to its subscribing channels and the disconnects are managed properly without leaving memory leaks behind.</li>
</ol>

<p>Reducers might maintain a copy of the market values updated in memory for fast access and in-process cache, but for each subscribed market, its value must also be saved in an out-of-process Redis HA cluster or sharded MongoDB. On subscription, if the market is not already in the memory of the reducer, that is no other subscriber has subscribed to it yet, it must be looked up first in the shared Redis or Mongo. On new incoming market updates from the Kafka topic, Redis / Mongo must be updated first. If updates cannot be skipped, the offsets are committed only after the Redis / Mongo write succeeds. Subscriptions are also saved in Redis / Mongo to account reducer restarts, scaling up or down.</p>

<p>To handle reducer restarts, partitioning logic is made known to the reducer, so upon reading the subscription information it knows which class of market ids it will serve and which to ignore. If the ACK to Kafka is sent after the message has been published to the subscribing clients, the reducer might opt for lazy initialization and not read the current state from Redis for its assigned markets. If the rate of subscription is very high, the reducer might opt to store in a separate collection in Redis a list of popular markets to eager read upon initialization, but, in my opinion, this is more an optimization than a functionality that must be implemented from the start.</p>

<p><em>Variations</em></p>

<ul>
  <li>
    <p>Subscribers are able to subscribe to yet unknown markets - e.g. all football matches about to start. In this approach, the reducer is cascaded in two steps: one to compute market IDs which match the query and act as a virtual subscriber on behalf of the client, and a second step, the one described above, where markets are sent to subscribing clients.</p>
  </li>
  <li>
    <p>Views for complex queries (e.g. upcoming matches page) are published through REST and then through a CDN for a quick initial load, together with a timestamp. The subscriber thus a) knows which markets to subscribe to as they are already in the page and, b), can use the timestamp to start receiving only the updates newer than what it already has. This approach reduces greatly the time to first load.</p>
  </li>
  <li>
    <p>The reducer is not publishing markets, but changes to a game state, driven by user actions and match events. In this case, the command queue becomes the queue for match events which are distributed to all reducers, the odds queue becomes the receiver for user actions, but the general pattern remains the same.</p>
  </li>
</ul>

<h3 id="alternative-approach">Alternative Approach</h3>

<p>An alternative approach that can be used for prototyping and a relatively solid MVP is to use something like MongoDB Streams (or Firebase, or RethinkDB) to listen to changes to a collection where the states are stored and modified in place. Each document is the atomic unit of change. A customer can subscribe to one to many documents.  All changes are propagated to all publishers and the publisher decides which are the correct recipients.</p>

<p>This take on the architecture allows a simpler model where the developers are freed from managing complex problems, like persistence, synchronization and restores, while reducing the number of dependencies and simplifying operations.</p>


  </div>

  
</article>

      </div>
    </main>

    <footer class="site-footer">

  <div class="wrapper">

    <h2 class="footer-heading">From The Trenches - The Code</h2>

    <div class="footer-col-wrapper">
      <div class="footer-col footer-col-1">
        <ul class="contact-list">
          <li>
            
              From The Trenches - The Code
            
            </li>
            
            <li><a href="mailto:alexandru.gris2006@gmail.com">alexandru.gris2006@gmail.com</a></li>
            
        </ul>
      </div>

      <div class="footer-col footer-col-2">
        <ul class="social-media-list">
          
          <li>
            <a href="https://github.com/alexandrugris"><span class="icon icon--github"><svg viewBox="0 0 16 16" width="16px" height="16px"><path fill="#828282" d="M7.999,0.431c-4.285,0-7.76,3.474-7.76,7.761 c0,3.428,2.223,6.337,5.307,7.363c0.388,0.071,0.53-0.168,0.53-0.374c0-0.184-0.007-0.672-0.01-1.32 c-2.159,0.469-2.614-1.04-2.614-1.04c-0.353-0.896-0.862-1.135-0.862-1.135c-0.705-0.481,0.053-0.472,0.053-0.472 c0.779,0.055,1.189,0.8,1.189,0.8c0.692,1.186,1.816,0.843,2.258,0.645c0.071-0.502,0.271-0.843,0.493-1.037 C4.86,11.425,3.049,10.76,3.049,7.786c0-0.847,0.302-1.54,0.799-2.082C3.768,5.507,3.501,4.718,3.924,3.65 c0,0,0.652-0.209,2.134,0.796C6.677,4.273,7.34,4.187,8,4.184c0.659,0.003,1.323,0.089,1.943,0.261 c1.482-1.004,2.132-0.796,2.132-0.796c0.423,1.068,0.157,1.857,0.077,2.054c0.497,0.542,0.798,1.235,0.798,2.082 c0,2.981-1.814,3.637-3.543,3.829c0.279,0.24,0.527,0.713,0.527,1.437c0,1.037-0.01,1.874-0.01,2.129 c0,0.208,0.14,0.449,0.534,0.373c3.081-1.028,5.302-3.935,5.302-7.362C15.76,3.906,12.285,0.431,7.999,0.431z"/></svg>
</span><span class="username">alexandrugris</span></a>

          </li>
          

          
          <li>
            <a href="https://twitter.com/alexandrugris"><span class="icon icon--twitter"><svg viewBox="0 0 16 16" width="16px" height="16px"><path fill="#828282" d="M15.969,3.058c-0.586,0.26-1.217,0.436-1.878,0.515c0.675-0.405,1.194-1.045,1.438-1.809c-0.632,0.375-1.332,0.647-2.076,0.793c-0.596-0.636-1.446-1.033-2.387-1.033c-1.806,0-3.27,1.464-3.27,3.27 c0,0.256,0.029,0.506,0.085,0.745C5.163,5.404,2.753,4.102,1.14,2.124C0.859,2.607,0.698,3.168,0.698,3.767 c0,1.134,0.577,2.135,1.455,2.722C1.616,6.472,1.112,6.325,0.671,6.08c0,0.014,0,0.027,0,0.041c0,1.584,1.127,2.906,2.623,3.206 C3.02,9.402,2.731,9.442,2.433,9.442c-0.211,0-0.416-0.021-0.615-0.059c0.416,1.299,1.624,2.245,3.055,2.271 c-1.119,0.877-2.529,1.4-4.061,1.4c-0.264,0-0.524-0.015-0.78-0.046c1.447,0.928,3.166,1.469,5.013,1.469 c6.015,0,9.304-4.983,9.304-9.304c0-0.142-0.003-0.283-0.009-0.423C14.976,4.29,15.531,3.714,15.969,3.058z"/></svg>
</span><span class="username">alexandrugris</span></a>

          </li>
          
        </ul>
      </div>

      <div class="footer-col footer-col-3">
        <p>Alexandru Gris - Personal Blog
</p>
      </div>
    </div>

  </div>

</footer>


  </body>

</html>
