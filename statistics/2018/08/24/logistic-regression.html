<!DOCTYPE html>
<html lang="en">

  <head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <title>Logistic Regression</title>
  <meta name="description" content="While linear regression is about predicting effects given a set of causes, logistic regression predicts the probability of certain effects. This way, its mai...">

  <link rel="stylesheet" href="/assets/main.css">
  <link rel="canonical" href="https://alexandrugris.github.io/statistics/2018/08/24/logistic-regression.html">
  <link rel="alternate" type="application/rss+xml" title="From The Trenches - The Code" href="/feed.xml">
  
  
</head>


  <body>

    <header class="site-header" role="banner">

  <div class="wrapper">

    <div class="site-nav"><a class="site-title" href="/">From The Trenches - The Code</a> </div>

    <nav class="site-nav">
      <span class="menu-icon">        
      </span>

      <div class="trigger">

        <a class="page-link" href="https://alexandrugris.github.io">Home</a>

        
          
          <a class="page-link" href="/about/">About</a>
          
        
          
        
          
        
          
          <a class="page-link" href="/sm/">Social Media</a>
          
        
          
        
          
        
      </div>
    </nav>

  </div>

</header>


    <main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title" itemprop="name headline">Logistic Regression</h1>
    <p class="post-meta"><time datetime="2018-08-24T14:15:16+03:00" itemprop="datePublished">Aug 24, 2018</time></p>
  </header>

  <div class="post-content" itemprop="articleBody">
    <p>While linear regression is about predicting effects given a set of causes, logistic regression predicts the probability of certain effects. This way, its main applications are classification and forecasting. Logistic regression helps find how probabilities are changed by our actions or by various changes in the factors included in the regression.</p>

<h3 id="the-problem">The problem</h3>

<p>The problem we want to solve is: given a vector of factors <code class="highlighter-rouge">X=[X1 ... Xn]</code>, find a model that predicts the probability of a binary outcome to occur, <code class="highlighter-rouge">P(X, outcome = 1)</code>. The input for the model are the previous observations of <code class="highlighter-rouge">X</code> and whether the expected outcome was <code class="highlighter-rouge">0</code> or <code class="highlighter-rouge">1</code>.</p>

<p>One can obviosly try to reduce the problem directly to a linear regression problem, as exemplified in the PDF file <a href="https://alexandrugris.github.io/assets/logistic_regression_1.pdf">here</a> but the shortcomings are immediately apparent:</p>

<ul>
  <li>R^2 is low</li>
  <li>There is not an obvious way to link the results to probability, the regression line goes above 1 and below 0.</li>
  <li>Residuals are obviously not normally distributed</li>
</ul>

<p>If in this precise case, one predictor variable, one result, one can simply find a cut-off point, the problem becomes more difficult if we include several predictor variables in the equation.</p>

<h3 id="the-logistic-regression-approach">The Logistic Regression Approach</h3>

<p>We notice that the Logistic function, <code class="highlighter-rouge">f(x) = 1 / (1+e^-(ax+b))</code>, has properties that naturally map to our problem:</p>

<ul>
  <li>it grows assimptotically from 0 to 1 - which are the natural boundaries of probabilty</li>
  <li>it is continuous</li>
  <li>has an inflection point from which <code class="highlighter-rouge">f(x)</code> grows quickly from close to 0 to close to 1</li>
</ul>

<p><img src="https://alexandrugris.github.io/assets/logistic_regression_8.png" alt="Logistic function" /></p>

<p>And the wikipedia link: <a href="https://en.wikipedia.org/wiki/Logistic_function">Logistic function</a></p>

<p>Therefore, we will find a mathematical model that uses the logistic function to map from causes to probabilities. The model aims to find a vector <code class="highlighter-rouge">B</code> that, when plugged into the logistic function multiplied by the factors <code class="highlighter-rouge">X</code>, the observed results match as closely as possible to the results determined by our model. In this case, our logistic function looks like <code class="highlighter-rouge">f(X_i) = 1 / (1+e^-(b1 * xi_1 + ... bn * xi_n)</code>, where <code class="highlighter-rouge">Xi = [xi_1, ... xi_n]</code> are the observed <code class="highlighter-rouge">X</code> factors at point <code class="highlighter-rouge">i</code>.</p>

<p>We will further define our problem like this:</p>

<ul>
  <li><code class="highlighter-rouge">Xi = [xi_1 ... xi_n]</code> - factors, with i between 1 and k. Attention, one of the factors needs to be <code class="highlighter-rouge">1</code>, the intercept - see the excel example below</li>
  <li><code class="highlighter-rouge">B = [b1 ... bn]</code> - coefficients</li>
  <li><code class="highlighter-rouge">Y = [y1 ... yk]</code> - a vector of 1 and 0 of k elements</li>
  <li><code class="highlighter-rouge">dot(Xi, B) == Xi_1 * b1 + ... Xi_n * bn</code></li>
</ul>

<p>This is a fancy way of describing a table with n colums defined by X, the regression factors, and 1 column defined by Y, the observed result, with k rows.</p>

<p>The core of the model is assuming that the probability of Y being 1 is given by the logistic function:</p>
<ul>
  <li><code class="highlighter-rouge">P(1|Xi) = f(Xi) = 1 / (1+e^-dot(Xi,B))</code></li>
</ul>

<p>Which makes:</p>
<ul>
  <li><code class="highlighter-rouge">P(0|Xi) = 1 - P(1|Xi)</code></li>
</ul>

<p>So, in compact form:</p>
<ul>
  <li><code class="highlighter-rouge">P(yi | Xi) = (P(1 | Xi) ^ yi) * (P(0 | Xi) ^ (1 - yi)</code>), since <code class="highlighter-rouge">yi</code> is either <code class="highlighter-rouge">0</code> or <code class="highlighter-rouge">1</code></li>
</ul>

<p>We conclude that <code class="highlighter-rouge">P(yi | X)</code> is another column, that of probabilities, but we cannot fully compute it because we are missing the vector <code class="highlighter-rouge">B</code>.</p>

<p>A perfect fit means <code class="highlighter-rouge">P(yi | Xi) == 1</code>, whatever <code class="highlighter-rouge">i=1..k</code>, which reads <em>given the set of inputs Xi we estimate with absolute certainty that the result is yi, 0 or 1, just as observed in the real world.</em> This leads to a <em>real world</em> equation of <code class="highlighter-rouge">PRODUCT(P(yi | Xi), for i = 0..k) == 1</code></p>

<p>But since we only aim to model the real world, we are happy if this product is merely maximized by our estimated probability function (logistic) with <code class="highlighter-rouge">B</code> plugged in. That is,</p>

<p><code class="highlighter-rouge">PRODUCT(P(yi | Xi), for i = 0..k) = P(y1 | X1) * P(y2 | X2) * ...</code> is maximized, as close as possible to 1.</p>

<p>which is equivalent to</p>

<p><code class="highlighter-rouge">log(PRODUCT) = SUM(log(P(yi | Xi)) = SUM( yi * log(f(Xi)) + (1-yi) * log(1-f(Xi)))</code> where <code class="highlighter-rouge">f(Xi) = 1 / (1+e^-dot(Xi,B))</code> is maximized as close as possible to 0.</p>

<p>Now we can use gradient descent to find the vector <code class="highlighter-rouge">B</code>. Unfortunately this method might be slow for large datasets because, for every incremental change in any <code class="highlighter-rouge">B</code>, all the exponentials and the logarithms for each line in the dataset need to be recomputed.</p>

<h3 id="implementation-in-excel-on-the-iris-dataset">Implementation in Excel on the Iris dataset</h3>

<ol>
  <li>We download the Iris dataset.</li>
  <li>We setup 3 variables, one for each type of flower. They are 1 if the flower is of that type and 0 otherwise.</li>
  <li>We split the data into trainig and test dataset.</li>
  <li>Add a new column for intercept.</li>
  <li>Add a new row for the <code class="highlighter-rouge">B</code> vector. These are the values we want to find.</li>
  <li>Add a new column for <code class="highlighter-rouge">P(setosa)</code>, the probabilities we want as result from the logistic regression. <code class="highlighter-rouge">P(setosa) = 1/1+EXP(-SUMPRODUCT(B, X))</code></li>
  <li>Add a new column, the log column, <code class="highlighter-rouge">log(P(yi | Xi)</code>, the column for which the sum we want to optimize as close as possible to 0. The formula for each of the elements in the column is <code class="highlighter-rouge">yi *LN(P(setosa)+0.0001) + (1-yi)*LN(1-P(setosa)+0.0001)</code>. I added a <code class="highlighter-rouge">0.0001</code> correction factor because, as the maximization algorithm goes on, we might end up with <code class="highlighter-rouge">NaN</code> values due to computing <code class="highlighter-rouge">P(setosa) = 0</code> which leads to <code class="highlighter-rouge">LN(0)</code> which is <code class="highlighter-rouge">-infinity</code>.</li>
  <li>Add a summary value for the column at 7, as <code class="highlighter-rouge">SUM(log(P(yi | Xi)))</code>.</li>
  <li>Use Excel Solver plugin to maximize the value of this summary value to as close to 0 as possible.</li>
  <li>Because the setosa is strictly separated from the other two types, the solver steps continues until the regression becomes very tight and the coefficients very large. In order for the <code class="highlighter-rouge">1/1+EXP(-SUMPRODUCT)</code> not to overflow, I added a constraint for the <code class="highlighter-rouge">-SUMPRODUCT()</code> to be less or equal to 50. This does not seem to impact the quality of the regression.</li>
  <li>Check the test data.</li>
</ol>

<p>Excel file <a href="https://alexandrugris.github.io/assets/iris.xlsx">here</a></p>

<p><img src="https://alexandrugris.github.io/assets/logistic_regression_1.png" alt="Solver Dialog" /></p>

<p><em>Note:</em></p>

<p>Performing the same steps for <em>virginica</em> leads also to very good results, while for <em>versicolor</em> the results are barely above random choice.</p>

<p>In the second iteration of the file, I have modified the regression so that it operates on standardized factors. This way, we can interpret the coefficients and see which factor contributes the most for classification. The picture below also contains the <code class="highlighter-rouge">log(odds)</code> vs <code class="highlighter-rouge">most important factor</code> chart, sorted by <code class="highlighter-rouge">odds = P(1) / P(0)</code>, the ratio between the probability of <code class="highlighter-rouge">1</code> and <code class="highlighter-rouge">0</code>. This is also a reminder that data for logistic regression should pe pepared in a similar manner to that for linear regression.</p>

<p><img src="https://alexandrugris.github.io/assets/logistic_regression_2.png" alt="Standardized Data" /></p>

<h3 id="linear-regression-and-logistic-regression">Linear Regression and Logistic Regression</h3>

<p>By the choice for the logistic function, <code class="highlighter-rouge">p = 1 / (1+EXP(-SUMPROD(Beta, X)))</code>, with <code class="highlighter-rouge">Beta</code> the coefficients for each of the factors X included in the regression, including the intercept, Logistic Regression hints stongly towards the linear regression.</p>

<p>We introduce the function <code class="highlighter-rouge">Odds(p) = p / 1-p</code> with its inverse, <code class="highlighter-rouge">p = Odds / (1+Odds)</code>.</p>

<p>By simple arithmetic, we conclude that <code class="highlighter-rouge">Odds(p) = EXP(SUMPRODUCT(Beta, X))</code> which leads to <code class="highlighter-rouge">LN(Odds(p)) = SUMPRODUCT(Beta, X)</code> which is a linear function.</p>

<p>Please remember that <code class="highlighter-rouge">p</code> is the probability for the event to happen, that is proability for the flower to be setosa, in the example above.</p>

<p>This function, <code class="highlighter-rouge">ln(Odds(p)) = ln(p/1-p)</code>, called the <em>logit function</em>, maps the probability space <code class="highlighter-rouge">[0,1]</code> to a linear space <code class="highlighter-rouge">[-inf, inf]</code> in which we can do the regression.</p>

<p><img src="https://alexandrugris.github.io/assets/logistic_regression_3.png" alt="Logit function" /></p>

<p>In order to use linear regression to compute the probabilities and thus build our classifier, we need in our input data probabilities as well. The Iris dataset, as presented above, has only 0 and 1 inputs, so we need to collapse it to intervals on which meaninful probabilities can be computed.</p>

<p>In this example we will use another dataset which is already collapsed:</p>

<p><img src="https://alexandrugris.github.io/assets/logistic_regression_4.png" alt="Hypertensive men" /></p>

<p>The data is described as follows:</p>
<ul>
  <li>Number of men analysed</li>
  <li>Number of men with hypertension</li>
  <li>Wether the man is smoking, obese or snores</li>
</ul>

<p>We want to:</p>
<ul>
  <li>Find the real probabilities for a man to be hypertensive given the factors above - please note that we are talking about a small sample of the population, so the real probabilities are not simply the division between men with hypertension and men.</li>
  <li>Map these probabilities to individual conditions of a patient - he may be fat but not obese or he might just be a casual smoker, metrics which can be represented as fractions of the smoking, obese, snores variables above.</li>
</ul>

<h3 id="steps">Steps</h3>

<p><em>Linear regression on the raw, unprocessed, original data, just to have a benchmark</em></p>

<p>If we include an intercept, the intercept will reflect the amount of men included in the test, so, for a single man, the results for the prediction will be completely off. We will see that these results are pretty close to what we will obtain from logistic regression, because the probabilities themselves are in the lower part of the spectrum, where the logistic regression itself is quite linear. However, we expect that, as the risk factors increase significantly, for instance by codifying a “heavy smoker” or a “highly obese”, the results from the linear regression to diverge from the real probabilities.</p>

<p>Please see how the variables for smoking, obese and snoring are codified. We want them to affect the slope of the regression, not the intercept.</p>

<p><img src="https://alexandrugris.github.io/assets/logistic_regression_5.png" alt="Linear Regression" /></p>

<p><em>Logistic Regression</em></p>

<p>We will do two types of logistic regression - one that does not account for the amount of men included in the sample and one which does. The problem with not ballancing the regression for the amount of men is equivalent to discarding the precision of the initial estimation of probability given by the sample (no of hypertensives / no of men). E.g., the more men we include, the more confident one can be that the expected value of the sample is closer to the actual expected value of the population.</p>

<p>First step for both regression is to remove from the regression the line where only 2 men are counted. This line does not contain enough information to be able to codify a probability out of it or, better said, the margin of error is too high.</p>

<p>The second step, also for both regressions, is to add the following columns:</p>

<ul>
  <li><code class="highlighter-rouge">Log(Odds Observed) = LN(P_observed / 1 - (P_observed))</code> where <code class="highlighter-rouge">P_observed = hypertensives / total number of men in that category</code>.</li>
  <li>Smoking, obesity, snoring, codified as 1 if the person is smoking, obese or snoring.</li>
  <li>Intercept, all values equal to 1.</li>
</ul>

<p>As result, we have:</p>
<ul>
  <li><code class="highlighter-rouge">Log(Odds regressed)</code>, the result of the linear regression</li>
  <li><code class="highlighter-rouge">P Logistic</code> which is the probability as computed from the logistic regression.</li>
</ul>

<p>We also added two more tables:</p>

<ul>
  <li>One under the logistic regression for drawing and validating how probabilities change for various fractions of Snoring, Smoking and Obese predictor variables.</li>
  <li>One under the linear regression to validate how many standard deviations the observed ratio (probability) is from the theoretical probability obtained from the logistic regression. That is, assuming the probabilities from the regression are correct, how likely it is to have observed the value we have observed. For this, we used the theoretical mean and variance for the <em>binomial distribution</em>, <code class="highlighter-rouge">mean = sample_size * p</code> and <code class="highlighter-rouge">variance = sample_size * p * (1-p)</code></li>
</ul>

<p>First run, the results without taking into consideration the sample size: <a href="https://alexandrugris.github.io/assets/logistic_regression_6.png">Logistic Regression</a></p>

<p><img src="https://alexandrugris.github.io/assets/logistic_regression_6.png" alt="Logistic Regression" /></p>

<p>The second time we took into consideration the sample size by weightning the square of the residuals with the sample size when computing the square sum of the residuals we want to minimize. This is equivalent to having 1 row in the regression for each men that was considered in the regression. We see that this also minimizes the sum of the square standard scores, meaning that the results are now closer to the reality that was observed in the field. As a side note, I tried computing the <code class="highlighter-rouge">Bs</code> by minimizing directly the sum of the square standard scores and the results were very close to the ones predicted by the logistic regression. <a href="https://alexandrugris.github.io/assets/logistic_regression_7.png">Weighted Logistic Regression</a></p>

<p><img src="https://alexandrugris.github.io/assets/logistic_regression_7.png" alt="Weighted Logistic Regression" /></p>

<p>Excel file <a href="(https://alexandrugris.github.io/assets/hypertensives.xlsx)">here</a></p>

<h3 id="conclusion">Conclusion</h3>

<p>We used logistic regression to build a clasifier on the Iris dataset and to predict the probability of a person having hypertension given a set of predictors. We used two ways to compute the regression coefficients: one by maximizing directly a probability function using the gradient descent, the other by applying linear regression to the log-odds function and then computing the probabilities from it.</p>

  </div>

  
</article>

      </div>
    </main>

    <footer class="site-footer">

  <div class="wrapper">

    <h2 class="footer-heading">From The Trenches - The Code</h2>

    <div class="footer-col-wrapper">
      <div class="footer-col footer-col-1">
        <ul class="contact-list">
          <li>
            
              From The Trenches - The Code
            
            </li>
            
            <li><a href="mailto:alexandru.gris2006@gmail.com">alexandru.gris2006@gmail.com</a></li>
            
        </ul>
      </div>

      <div class="footer-col footer-col-2">
        <ul class="social-media-list">
          
          <li>
            <a href="https://github.com/alexandrugris"><span class="icon icon--github"><svg viewBox="0 0 16 16" width="16px" height="16px"><path fill="#828282" d="M7.999,0.431c-4.285,0-7.76,3.474-7.76,7.761 c0,3.428,2.223,6.337,5.307,7.363c0.388,0.071,0.53-0.168,0.53-0.374c0-0.184-0.007-0.672-0.01-1.32 c-2.159,0.469-2.614-1.04-2.614-1.04c-0.353-0.896-0.862-1.135-0.862-1.135c-0.705-0.481,0.053-0.472,0.053-0.472 c0.779,0.055,1.189,0.8,1.189,0.8c0.692,1.186,1.816,0.843,2.258,0.645c0.071-0.502,0.271-0.843,0.493-1.037 C4.86,11.425,3.049,10.76,3.049,7.786c0-0.847,0.302-1.54,0.799-2.082C3.768,5.507,3.501,4.718,3.924,3.65 c0,0,0.652-0.209,2.134,0.796C6.677,4.273,7.34,4.187,8,4.184c0.659,0.003,1.323,0.089,1.943,0.261 c1.482-1.004,2.132-0.796,2.132-0.796c0.423,1.068,0.157,1.857,0.077,2.054c0.497,0.542,0.798,1.235,0.798,2.082 c0,2.981-1.814,3.637-3.543,3.829c0.279,0.24,0.527,0.713,0.527,1.437c0,1.037-0.01,1.874-0.01,2.129 c0,0.208,0.14,0.449,0.534,0.373c3.081-1.028,5.302-3.935,5.302-7.362C15.76,3.906,12.285,0.431,7.999,0.431z"/></svg>
</span><span class="username">alexandrugris</span></a>

          </li>
          

          
          <li>
            <a href="https://twitter.com/alexandrugris"><span class="icon icon--twitter"><svg viewBox="0 0 16 16" width="16px" height="16px"><path fill="#828282" d="M15.969,3.058c-0.586,0.26-1.217,0.436-1.878,0.515c0.675-0.405,1.194-1.045,1.438-1.809c-0.632,0.375-1.332,0.647-2.076,0.793c-0.596-0.636-1.446-1.033-2.387-1.033c-1.806,0-3.27,1.464-3.27,3.27 c0,0.256,0.029,0.506,0.085,0.745C5.163,5.404,2.753,4.102,1.14,2.124C0.859,2.607,0.698,3.168,0.698,3.767 c0,1.134,0.577,2.135,1.455,2.722C1.616,6.472,1.112,6.325,0.671,6.08c0,0.014,0,0.027,0,0.041c0,1.584,1.127,2.906,2.623,3.206 C3.02,9.402,2.731,9.442,2.433,9.442c-0.211,0-0.416-0.021-0.615-0.059c0.416,1.299,1.624,2.245,3.055,2.271 c-1.119,0.877-2.529,1.4-4.061,1.4c-0.264,0-0.524-0.015-0.78-0.046c1.447,0.928,3.166,1.469,5.013,1.469 c6.015,0,9.304-4.983,9.304-9.304c0-0.142-0.003-0.283-0.009-0.423C14.976,4.29,15.531,3.714,15.969,3.058z"/></svg>
</span><span class="username">alexandrugris</span></a>

          </li>
          
        </ul>
      </div>

      <div class="footer-col footer-col-3">
        <p>Alexandru Gris - Personal Blog
</p>
      </div>
    </div>

  </div>

</footer>


  </body>

</html>
