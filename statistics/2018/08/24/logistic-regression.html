<!DOCTYPE html>
<html lang="en">

  <head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <title>Logistic Regression</title>
  <meta name="description" content="While linear regression is about predicting effects given a set of causes, logistic regression predicts the probability of certain effects. This way, its mai...">

  <link rel="stylesheet" href="/assets/main.css">
  <link rel="canonical" href="https://alexandrugris.github.io/statistics/2018/08/24/logistic-regression.html">
  <link rel="alternate" type="application/rss+xml" title="From The Trenches - The Code" href="/feed.xml">
  
  
</head>


  <body>

    <header class="site-header" role="banner">

  <div class="wrapper">

    <div class="site-nav"><a class="site-title" href="/">From The Trenches - The Code</a> </div>

    <nav class="site-nav">
      <span class="menu-icon">        
      </span>

      <div class="trigger">

        <a class="page-link" href="https://alexandrugris.github.io">Home</a>

        
          
          <a class="page-link" href="/about/">About</a>
          
        
          
        
          
        
          
          <a class="page-link" href="/sm/">Social Media</a>
          
        
          
        
      </div>
    </nav>

  </div>

</header>


    <main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title" itemprop="name headline">Logistic Regression</h1>
    <p class="post-meta"><time datetime="2018-08-24T14:15:16+03:00" itemprop="datePublished">Aug 24, 2018</time></p>
  </header>

  <div class="post-content" itemprop="articleBody">
    <p>While linear regression is about predicting effects given a set of causes, logistic regression predicts the probability of certain effects. This way, its main applications are classification and forecasting. Logistic regression helps find how probabilities are changed by our actions or by various changes in the factors included in the regression.</p>

<h3 id="the-problem">The problem</h3>

<p>The problem we want to solve is: given a vector of factors <code class="highlighter-rouge">X=[X1 ... Xn]</code>, find a model that predicts the probability of a binary outcome to occur, <code class="highlighter-rouge">P(X, outcome = 1)</code>. The input for the model are the previous observations of <code class="highlighter-rouge">X</code> and whether the expected outcome was <code class="highlighter-rouge">0</code> or <code class="highlighter-rouge">1</code>.</p>

<p>One can obviosly try to reduce the problem directly to a linear regression problem, as exemplified in the PDF file <a href="https://alexandrugris.github.io/assets/logistic_regression_1.pdf">here</a> but the shortcomings are immediately apparent:</p>

<ul>
  <li>R^2 is low</li>
  <li>There is not an obvious way to link the results to probability, the regression line goes above 1 and below 0.</li>
  <li>Residuals are obviously not normally distributed</li>
</ul>

<p>If in this precise case, one predictor variable, one result, one can simply find a cut-off point, the problem becomes more difficult if we include several predictor variables in the equation.</p>

<h3 id="the-logistic-regression-approach">The Logistic Regression Approach</h3>

<p>We notice that the Logistic function, <code class="highlighter-rouge">f(x) = 1 / (1+e^-(ax+b))</code>, has properties that naturally map to our problem:</p>

<ul>
  <li>it grows assimptotically from 0 to 1 - which are the natural boundaries of probabilty</li>
  <li>it is continuous</li>
  <li>has an inflection point from which <code class="highlighter-rouge">f(x)</code> grows quickly from close to 0 to close to 1</li>
</ul>

<p><img src="https://alexandrugris.github.io/assets/logistic_regression_8.png" alt="Logistic function" /></p>

<p>And the wikipedia link: <a href="https://en.wikipedia.org/wiki/Logistic_function">Logistic function</a></p>

<p>Therefore, we will find a mathematical model that uses the logistic function to map from causes to probabilities. The model aims to find a vector <code class="highlighter-rouge">B</code> that, when plugged into the logistic function multiplied by the factors <code class="highlighter-rouge">X</code>, the observed results match as closely as possible to the results determined by our model. In this case, our logistic function looks like <code class="highlighter-rouge">f(X_i) = 1 / (1+e^-(b1 * xi_1 + ... bn * xi_n)</code>, where <code class="highlighter-rouge">Xi = [xi_1, ... xi_n]</code> are the observed <code class="highlighter-rouge">X</code> factors at point <code class="highlighter-rouge">i</code>.</p>

<p>We will further define our problem like this:</p>

<ul>
  <li><code class="highlighter-rouge">Xi = [xi_1 ... xi_n]</code> - factors, with i between 1 and k. Attention, one of the factors needs to be <code class="highlighter-rouge">1</code>, the intercept - see the excel example below</li>
  <li><code class="highlighter-rouge">B = [b1 ... bn]</code> - coefficients</li>
  <li><code class="highlighter-rouge">Y = [y1 ... yk]</code> - a vector of 1 and 0 of k elements</li>
  <li><code class="highlighter-rouge">dot(Xi, B) == Xi_1 * b1 + ... Xi_n * bn</code></li>
</ul>

<p>This is a fancy way of describing a table with n colums defined by X, the regression factors, and 1 column defined by Y, the observed result, with k rows.</p>

<p>The core of the model is assuming that the probability of Y being 1 is given by the logistic function:</p>
<ul>
  <li><code class="highlighter-rouge">P(1|Xi) = f(Xi) = 1 / (1+e^-dot(Xi,B))</code></li>
</ul>

<p>Which makes:</p>
<ul>
  <li><code class="highlighter-rouge">P(0|Xi) = 1 - P(1|Xi)</code></li>
</ul>

<p>So, in compact form:</p>
<ul>
  <li><code class="highlighter-rouge">P(yi | Xi) = (P(1 | Xi) ^ yi) * (P(0 | Xi) ^ (1 - yi)</code>), since <code class="highlighter-rouge">yi</code> is either <code class="highlighter-rouge">0</code> or <code class="highlighter-rouge">1</code></li>
</ul>

<p>We conclude that <code class="highlighter-rouge">P(yi | X)</code> is another column, that of probabilities, but we cannot fully compute it because we are missing the vector <code class="highlighter-rouge">B</code>.</p>

<p>A perfect fit means <code class="highlighter-rouge">P(yi | Xi) == 1</code>, whatever <code class="highlighter-rouge">i=1..k</code>, which reads <em>given the set of inputs Xi we estimate with absolute certainty that the result is yi, 0 or 1, just as observed in the real world.</em> This leads to a <em>real world</em> equation of <code class="highlighter-rouge">PRODUCT(P(yi | Xi), for i = 0..k) == 1</code></p>

<p>But since we only aim to model the real world, we are happy if this product is merely maximized by our estimated probability function (logistic) with <code class="highlighter-rouge">B</code> plugged in. That is,</p>

<p><code class="highlighter-rouge">PRODUCT(P(yi | Xi), for i = 0..k) = P(y1 | X1) * P(y2 | X2) * ...</code> is maximized, as close as possible to 1.</p>

<p>which is equivalent to</p>

<p><code class="highlighter-rouge">log(PRODUCT) = SUM(log(P(yi | Xi)) = SUM(yi * log(f(Xi)) + (1-yi) * log(1-f(Xi)))</code> where <code class="highlighter-rouge">f(Xi) = 1 / (1+e^-dot(Xi,B))</code> is maximized as close as possible to 0.</p>

<p>Now we can use gradient descent to find the vector <code class="highlighter-rouge">B</code>. Unfortunately this method might be slow for large datasets because, for every incremental change in any <code class="highlighter-rouge">B</code>, all the exponentials and the logarithms for each line in the dataset need to be recomputed.</p>

<h3 id="implementation-in-excel-on-the-iris-dataset">Implementation in Excel on the Iris dataset</h3>

<ol>
  <li>We download the Iris dataset.</li>
  <li>We setup 3 variables, one for each type of flower. They are 1 if the flower is of that type and 0 otherwise.</li>
  <li>We split the data into trainig and test dataset.</li>
  <li>Add a new column for intercept.</li>
  <li>Add a new row for the <code class="highlighter-rouge">B</code> vector. These are the values we want to find.</li>
  <li>Add a new column for <code class="highlighter-rouge">P(setosa)</code>, the probabilities we want as result from the logistic regression. <code class="highlighter-rouge">P(setosa) = 1 / (1 + EXP(-SUMPRODUCT(B, X))</code></li>
  <li>Add a new column, the log column, <code class="highlighter-rouge">log(P(yi | Xi)</code>, the column for which the sum we want to optimize as close as possible to 0. The formula for each of the elements in the column is <code class="highlighter-rouge">yi * LN(P(setosa) + 0.0001) + (1 - yi) * LN( 1 - P(setosa) + 0.0001)</code>. I added a <code class="highlighter-rouge">0.0001</code> correction factor because, as the maximization algorithm goes on, we might end up with <code class="highlighter-rouge">NaN</code> values due to computing <code class="highlighter-rouge">P(setosa) = 0</code> which leads to <code class="highlighter-rouge">LN(0)</code> which is <code class="highlighter-rouge">-infinity</code>.</li>
  <li>Add a summary value for the column at 7, as <code class="highlighter-rouge">SUM(log(P(yi | Xi)))</code>.</li>
  <li>Use Excel Solver plugin to maximize the value of this summary value to as close to 0 as possible.</li>
  <li>Because the setosa is strictly separated from the other two types, the solver steps continues until the regression becomes very tight and the coefficients very large. In order for the <code class="highlighter-rouge">1/1+EXP(-SUMPRODUCT)</code> not to overflow, I added a constraint for the <code class="highlighter-rouge">-SUMPRODUCT()</code> to be less or equal to 50. This does not seem to impact the quality of the regression.</li>
  <li>Check the test data.</li>
</ol>

<p>Excel file <a href="https://alexandrugris.github.io/assets/iris.xlsx">here</a></p>

<p><img src="https://alexandrugris.github.io/assets/logistic_regression_1.png" alt="Solver Dialog" /></p>

<p><em>Note:</em></p>

<p>Performing the same steps for <em>virginica</em> leads also to very good results, while for <em>versicolor</em> the results are barely above random choice.</p>

<p>In the second iteration of the file, I have modified the regression so that it operates on standardized factors. This way, we can interpret the coefficients and see which factor contributes the most for classification. The picture below also contains the <code class="highlighter-rouge">log(odds)</code> vs <code class="highlighter-rouge">most important factor</code> chart, sorted by <code class="highlighter-rouge">odds = P(1) / P(0)</code>, the ratio between the probability of <code class="highlighter-rouge">1</code> and <code class="highlighter-rouge">0</code>. This is also a reminder that data for logistic regression should pe pepared in a similar manner to that for linear regression.</p>

<p><img src="https://alexandrugris.github.io/assets/logistic_regression_2.png" alt="Standardized Data" /></p>

<p><em>Note:</em></p>

<p>The factor we are minimizing, <code class="highlighter-rouge">ce = -(y * ln(p) + (1 - y) * ln(1-p))</code>, where y is the observed value (0 or 1) and <code class="highlighter-rouge">p</code> is the probability given by our model, is called <em>cross entropy</em>. <a href="https://www.quora.com/Whats-an-intuitive-way-to-think-of-cross-entropy">Awesome explanation here</a></p>

<h3 id="linear-regression-and-logistic-regression">Linear Regression and Logistic Regression</h3>

<p>By the choice for the logistic function, <code class="highlighter-rouge">p = 1 / (1+EXP(-SUMPROD(Beta, X)))</code>, with <code class="highlighter-rouge">Beta</code> the coefficients for each of the factors X included in the regression, including the intercept, Logistic Regression hints stongly towards the linear regression.</p>

<p>We introduce the function <code class="highlighter-rouge">Odds(p) = p / 1-p</code> with its inverse, <code class="highlighter-rouge">p = Odds / (1+Odds)</code>.</p>

<p>By simple arithmetic, we conclude that <code class="highlighter-rouge">Odds(p) = EXP(SUMPRODUCT(Beta, X))</code> which leads to <code class="highlighter-rouge">LN(Odds(p)) = SUMPRODUCT(Beta, X)</code> which is a linear function.</p>

<p>Please remember that <code class="highlighter-rouge">p</code> is the probability for the event to happen, that is proability for the flower to be setosa, in the example above.</p>

<p>This function, <code class="highlighter-rouge">ln(Odds(p)) = ln(p/1-p)</code>, called the <em>logit function</em>, maps the probability space <code class="highlighter-rouge">[0,1]</code> to a linear space <code class="highlighter-rouge">[-inf, inf]</code> in which we can do the regression.</p>

<p><img src="https://alexandrugris.github.io/assets/logistic_regression_3.png" alt="Logit function" /></p>

<p>In order to use linear regression to compute the probabilities and thus build our classifier, we need in our input data probabilities as well. The Iris dataset, as presented above, has only 0 and 1 inputs, so we need to collapse it to intervals on which meaninful probabilities can be computed.</p>

<p>In this example we will use another dataset which is already collapsed:</p>

<p><img src="https://alexandrugris.github.io/assets/logistic_regression_4.png" alt="Hypertensive men" /></p>

<p>The data is described as follows:</p>
<ul>
  <li>Number of men analysed</li>
  <li>Number of men with hypertension</li>
  <li>Wether the man is smoking, obese or snores</li>
</ul>

<p>We want to:</p>
<ul>
  <li>Find the real probabilities for a man to be hypertensive given the factors above - please note that we are talking about a small sample of the population, so the real probabilities are not simply the division between men with hypertension and men.</li>
  <li>Map these probabilities to individual conditions of a patient - he may be fat but not obese or he might just be a casual smoker, metrics which can be represented as fractions of the smoking, obese, snores variables above.</li>
</ul>

<h3 id="steps">Steps</h3>

<p><em>Linear regression on the raw, unprocessed, original data, just to have a benchmark</em></p>

<p>If we include an intercept, the intercept will reflect the amount of men included in the test, so, for a single man, the results for the prediction will be completely off. We will see that these results are pretty close to what we will obtain from logistic regression, because the probabilities themselves are in the lower part of the spectrum, where the logistic regression itself is quite linear. However, we expect that, as the risk factors increase significantly, for instance by codifying a “heavy smoker” or a “highly obese”, the results from the linear regression to diverge from the real probabilities.</p>

<p>Please see how the variables for smoking, obese and snoring are codified. We want them to affect the slope of the regression, not the intercept.</p>

<p><img src="https://alexandrugris.github.io/assets/logistic_regression_5.png" alt="Linear Regression" /></p>

<p><em>Logistic Regression</em></p>

<p>We will do two types of logistic regression - one that does not account for the amount of men included in the sample and one which does. The problem with not ballancing the regression for the amount of men is equivalent to discarding the precision of the initial estimation of probability given by the sample (no of hypertensives / no of men). E.g., the more men we include, the more confident one can be that the expected value of the sample is closer to the actual expected value of the population.</p>

<p>First step for both regression is to remove from the regression the line where only 2 men are counted. This line does not contain enough information to be able to codify a probability out of it or, better said, the margin of error is too high.</p>

<p>The second step, also for both regressions, is to add the following columns:</p>

<ul>
  <li><code class="highlighter-rouge">Log(Odds Observed) = LN(P_observed / 1 - (P_observed))</code> where <code class="highlighter-rouge">P_observed = hypertensives / total number of men in that category</code>.</li>
  <li>Smoking, obesity, snoring, codified as 1 if the person is smoking, obese or snoring.</li>
  <li>Intercept, all values equal to 1.</li>
</ul>

<p>As result, we have:</p>
<ul>
  <li><code class="highlighter-rouge">Log(Odds regressed)</code>, the result of the linear regression</li>
  <li><code class="highlighter-rouge">P Logistic</code> which is the probability as computed from the logistic regression.</li>
</ul>

<p>We also added two more tables:</p>

<ul>
  <li>One under the logistic regression for drawing and validating how probabilities change for various fractions of Snoring, Smoking and Obese predictor variables.</li>
  <li>One under the linear regression to validate how many standard deviations the observed ratio (probability) is from the theoretical probability obtained from the logistic regression. That is, assuming the probabilities from the regression are correct, how likely it is to have observed the value we have observed. For this, we used the theoretical mean and variance for the <em>binomial distribution</em>, <code class="highlighter-rouge">mean = sample_size * p</code> and <code class="highlighter-rouge">variance = sample_size * p * (1-p)</code></li>
</ul>

<p>First run, the results without taking into consideration the sample size: <a href="https://alexandrugris.github.io/assets/logistic_regression_6.png">Logistic Regression</a></p>

<p><img src="https://alexandrugris.github.io/assets/logistic_regression_6.png" alt="Logistic Regression" /></p>

<p>The second time we took into consideration the sample size by weightning the square of the residuals with the sample size when computing the square sum of the residuals we want to minimize. This is equivalent to having 1 row in the regression for each men that was considered in the regression. We see that this also minimizes the sum of the square standard scores, meaning that the results are now closer to the reality that was observed in the field. As a side note, I tried computing the <code class="highlighter-rouge">Bs</code> by minimizing directly the sum of the square standard scores and the results were very close to the ones predicted by the logistic regression. <a href="https://alexandrugris.github.io/assets/logistic_regression_7.png">Weighted Logistic Regression</a></p>

<p><img src="https://alexandrugris.github.io/assets/logistic_regression_7.png" alt="Weighted Logistic Regression" /></p>

<p>Excel file <a href="(https://alexandrugris.github.io/assets/hypertensives.xlsx)">here</a></p>

<h3 id="using-of-linear-regression-to-determe-the-coefficients-for-iris-dataset">Using of linear regression to determe the coefficients for Iris dataset</h3>

<p>In order to use linear regression to determine the coefficients, the data needs to be binned, put in a form from which we can extract probabilities. For the Iris dataset, I did this with the following Python script:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># -*- coding: utf-8 -*-</span>
<span class="s">"""
Created on Fri Aug 26 15:07:47 2018

@author: alexandru gris
"""</span>

<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="n">pd</span>
<span class="kn">from</span> <span class="nn">functools</span> <span class="kn">import</span> <span class="n">partial</span><span class="p">,</span> <span class="nb">reduce</span>

<span class="c"># columns used as predictors</span>
<span class="n">X</span> <span class="o">=</span> <span class="p">[</span><span class="s">'sepal_length'</span><span class="p">,</span>
 <span class="s">'sepal_width'</span><span class="p">,</span>
 <span class="s">'petal_length'</span><span class="p">,</span>
 <span class="s">'petal_width'</span><span class="p">]</span>

<span class="c"># columns for the result on which we train the algorithm</span>
<span class="n">Y</span> <span class="o">=</span> <span class="p">[</span><span class="s">'setosa'</span><span class="p">,</span>
 <span class="s">'versicolor'</span><span class="p">,</span>
 <span class="s">'virginica'</span><span class="p">]</span>

<span class="n">data</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s">"iris.csv"</span><span class="p">)[</span><span class="n">X</span> <span class="o">+</span> <span class="n">Y</span><span class="p">]</span>

<span class="c"># split each dimension intro two groups, low and high</span>
<span class="c"># each group should have a representative number of elements in it</span>
<span class="c"># had we had more items in the dataset, we could have simply increased the group count to 3</span>
<span class="c"># by binning, part of the information is lost but we hope to compensate through </span>
<span class="c"># the curvy form of the logistic function</span>
<span class="n">groups</span> <span class="o">=</span> <span class="mi">2</span>

<span class="c"># 1e-10 below is a small hack to include the last item in the last group</span>
<span class="c"># the split could / should have been done based on percentiles, not range</span>
<span class="c"># we will do by range just to keep the code simple</span>
<span class="n">stats</span> <span class="o">=</span> <span class="p">{</span> <span class="n">x</span> <span class="p">:</span> <span class="p">{</span><span class="s">'range'</span><span class="p">:</span> <span class="n">mx</span><span class="o">-</span><span class="n">mn</span><span class="p">,</span> <span class="s">'min'</span><span class="p">:</span> <span class="n">mn</span><span class="p">,</span> <span class="s">'grp_range'</span><span class="p">:</span> <span class="p">(</span><span class="n">mx</span><span class="o">-</span><span class="n">mn</span><span class="p">)</span> <span class="o">/</span> <span class="n">groups</span> <span class="p">}</span> \
            <span class="k">for</span> <span class="n">x</span><span class="p">,</span> <span class="n">mx</span><span class="p">,</span> <span class="n">mn</span> <span class="ow">in</span> <span class="p">(</span> <span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">data</span><span class="p">[</span><span class="n">x</span><span class="p">]</span><span class="o">.</span><span class="nb">max</span><span class="p">()</span> <span class="o">+</span> <span class="mf">1e-10</span><span class="p">,</span> <span class="n">data</span><span class="p">[</span><span class="n">x</span><span class="p">]</span><span class="o">.</span><span class="nb">min</span><span class="p">())</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">X</span><span class="p">)</span> <span class="p">}</span>

<span class="c"># the function which will compute the new index for grouping</span>
<span class="k">def</span> <span class="nf">fn</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">idx</span><span class="p">):</span> 
    <span class="s">"""x - column. idx - current index for the row"""</span>
    <span class="n">idx</span> <span class="o">=</span> <span class="nb">int</span><span class="p">((</span><span class="n">data</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="n">idx</span><span class="p">][</span><span class="n">x</span><span class="p">]</span> <span class="o">-</span> <span class="n">stats</span><span class="p">[</span><span class="n">x</span><span class="p">][</span><span class="s">'min'</span><span class="p">])</span> <span class="o">*</span> <span class="n">groups</span> <span class="o">/</span> <span class="n">stats</span><span class="p">[</span><span class="n">x</span><span class="p">][</span><span class="s">'range'</span><span class="p">])</span>
    <span class="k">return</span> <span class="p">(</span><span class="n">idx</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">*</span>  <span class="n">stats</span><span class="p">[</span><span class="n">x</span><span class="p">][</span><span class="s">'grp_range'</span><span class="p">]</span> <span class="o">+</span> <span class="n">stats</span><span class="p">[</span><span class="n">x</span><span class="p">][</span><span class="s">'min'</span><span class="p">]</span>
    
<span class="c"># create the summary table</span>
<span class="n">agg</span> <span class="o">=</span> <span class="p">{</span><span class="n">y</span> <span class="p">:</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="o">.</span><span class="nb">sum</span> <span class="k">for</span> <span class="n">y</span> <span class="ow">in</span> <span class="n">Y</span><span class="p">}</span>
<span class="n">x</span> <span class="o">=</span>  <span class="n">data</span><span class="o">.</span><span class="n">groupby</span><span class="p">([</span><span class="n">partial</span><span class="p">(</span><span class="n">fn</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">X</span><span class="p">])</span><span class="o">.</span><span class="n">agg</span><span class="p">(</span><span class="n">agg</span><span class="p">)</span>

<span class="c"># add the totals and the probabilities column for each vector</span>
<span class="c"># totals:</span>
<span class="n">x</span><span class="p">[</span><span class="s">'totals'</span><span class="p">]</span> <span class="o">=</span> <span class="nb">reduce</span><span class="p">(</span><span class="k">lambda</span> <span class="n">c1</span><span class="p">,</span> <span class="n">c2</span><span class="p">:</span> <span class="n">c1</span> <span class="o">+</span> <span class="n">c2</span><span class="p">,</span> <span class="p">[</span><span class="n">x</span><span class="p">[</span><span class="n">c</span><span class="p">]</span> <span class="k">for</span> <span class="n">c</span> <span class="ow">in</span> <span class="n">Y</span><span class="p">])</span>
<span class="c"># probabilities for setosa, versicolor, virginica:</span>
<span class="k">for</span> <span class="n">prob</span><span class="p">,</span> <span class="n">col</span> <span class="ow">in</span> <span class="p">((</span><span class="s">"p_"</span> <span class="o">+</span> <span class="n">y</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span> <span class="k">for</span> <span class="n">y</span> <span class="ow">in</span> <span class="n">Y</span><span class="p">):</span>
    <span class="n">x</span><span class="p">[</span><span class="n">prob</span><span class="p">]</span> <span class="o">=</span> <span class="n">x</span><span class="p">[</span><span class="n">col</span><span class="p">]</span> <span class="o">/</span> <span class="n">x</span><span class="p">[</span><span class="s">'totals'</span><span class="p">]</span>

<span class="c"># print a csv file for further processing in excel</span>
<span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="s">"iris_processed.csv"</span><span class="p">,</span> <span class="s">"w"</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>    
    <span class="n">pf</span> <span class="o">=</span> <span class="n">partial</span><span class="p">(</span><span class="k">print</span><span class="p">,</span> <span class="nb">file</span><span class="o">=</span><span class="n">f</span><span class="p">)</span>

    <span class="n">pf</span><span class="p">(</span><span class="s">","</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">X</span><span class="o">+</span><span class="nb">list</span><span class="p">(</span><span class="n">x</span><span class="p">)))</span>
    
    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">r</span> <span class="ow">in</span> <span class="n">x</span><span class="o">.</span><span class="n">iterrows</span><span class="p">():</span>
        <span class="n">pf</span><span class="p">(</span><span class="s">","</span><span class="o">.</span><span class="n">join</span><span class="p">([</span><span class="nb">str</span><span class="p">(</span><span class="n">s</span><span class="p">)</span> <span class="k">for</span> <span class="n">s</span> <span class="ow">in</span> <span class="p">(</span><span class="nb">list</span><span class="p">(</span><span class="n">i</span><span class="p">)</span> <span class="o">+</span> <span class="nb">list</span><span class="p">(</span><span class="n">r</span><span class="p">))]))</span>
</code></pre></div></div>

<p>Summarized data looks like this:</p>

<p><img src="https://alexandrugris.github.io/assets/logistic_regression_9.png" alt="Summarized table" /></p>

<p>The predictor rows contain the middle of the group range for that particular bin, which does not reflect the actual mean of the data inside the bin. This is highly sensitive to outliers and thus a bad choice. A better way would have been to first split each predictor in equal-sized groups, compute the median for each group, and then use this number as part of the index for each bin.</p>

<p>When we compute the odds and log(odds) to use in the regression we notice 3 possible error scenarios:</p>
<ul>
  <li>too little data in one bin - we remove that bin from the regression</li>
  <li>all data in a bin is in the category we want to predict - which leads to the odds being computed to infinity</li>
  <li>all data in a bin is not in the cateogory we want to predict - which leads to the logaritm being infinite</li>
</ul>

<p>Neither of the two last scenarios account for a usable probability, but we have enough data in those bins to draw a conclusion that we cannot simply discard. I used two approaches and the results for the regression were very similar:</p>
<ul>
  <li>Use the beta distribution and start from an a-priori value of 50%-50% (alpha=1, beta=1), which is equivalent to saying <em>just before we started to count we have observed an opposite value</em> and adjust the probability based on this formula - this gives us an upper (or lower) bound for the probability of 1 in that particular bin.</li>
  <li>Assume that the string of 0s or 1s was a random extraction from the all possible extractiobs but that it had a rather high probability to occur (of 50% in this case, a value which seemed right to me :) ). Thus consider <code class="highlighter-rouge">p^bin_size = 0.5</code> and compute <code class="highlighter-rouge">p=EXP(LN(0.5) / bin_size)</code>.</li>
</ul>

<p>Excel file <a href="https://alexandrugris.github.io/assets/iris_processed.xlsx">here</a></p>

<p>Results for the regression at a <code class="highlighter-rouge">0.5</code> threshold were not as good as with direct maximization of the log-of-probabilities, but rasing the threshhold at <code class="highlighter-rouge">0.8</code> leads to perfect prediction. Picture below:</p>

<p><img src="https://alexandrugris.github.io/assets/logistic_regression_10.png" alt="Logistic regression" /></p>

<p>And link here: <a href="https://alexandrugris.github.io/assets/logistic_regression_10.png">Logistic regression</a></p>

<h3 id="conclusions">Conclusions</h3>

<p>We used logistic regression to build a clasifier on the Iris dataset and to predict the probability of a person having hypertension given a set of predictors. We used two ways to compute the regression coefficients: one by maximizing directly a probability function using the gradient descent, the other by applying linear regression to the log-odds function and then computing the probabilities from it. The third example was to process the Iris dataset used in the first regression and to compute the coefficients in a similar manner to that used for determining hypertensives.</p>

<h3 id="additional-notes---decision-boundary">Additional notes - decision boundary</h3>

<p>If we set the classification decision to <code class="highlighter-rouge">1</code> when <code class="highlighter-rouge">p(1) &gt; 0.5</code>, given the equation of the logistic function, it is equivalent to saying <code class="highlighter-rouge">y = b0 + ... bn*xn &gt; 0</code>. <em>This is the equation of a line which divides the factor space in two sections.</em></p>

<p><em>If we want to divide the space into an enclosed area, with outside and inside spaces,</em> we can set, for instance, the regression to include squares of its factors, like <code class="highlighter-rouge">y = b0 + b1*x1 + b2*x2 + b3*x1^2 + b4 *x2^1</code>. In this example, for instance, if we set <code class="highlighter-rouge">b0 = -1, b1 = 0, b2 = 0, b3 = 1, b4 = 1</code>, at the boundary condition we arrive precisely at the equation for a cirle, <code class="highlighter-rouge">1 = x1^2 + x2^2</code>.</p>

<p>The beauty of logistic regression is that the decision boundary can be visualised by just comparing the regression term to 0, since we know that in that particular point <code class="highlighter-rouge">P=1/(1+e^0) = 0.5</code></p>


  </div>

  
</article>

      </div>
    </main>

    <footer class="site-footer">

  <div class="wrapper">

    <h2 class="footer-heading">From The Trenches - The Code</h2>

    <div class="footer-col-wrapper">
      <div class="footer-col footer-col-1">
        <ul class="contact-list">
          <li>
            
              From The Trenches - The Code
            
            </li>
            
            <li><a href="mailto:alexandru.gris2006@gmail.com">alexandru.gris2006@gmail.com</a></li>
            
        </ul>
      </div>

      <div class="footer-col footer-col-2">
        <ul class="social-media-list">
          
          <li>
            <a href="https://github.com/alexandrugris"><span class="icon icon--github"><svg viewBox="0 0 16 16" width="16px" height="16px"><path fill="#828282" d="M7.999,0.431c-4.285,0-7.76,3.474-7.76,7.761 c0,3.428,2.223,6.337,5.307,7.363c0.388,0.071,0.53-0.168,0.53-0.374c0-0.184-0.007-0.672-0.01-1.32 c-2.159,0.469-2.614-1.04-2.614-1.04c-0.353-0.896-0.862-1.135-0.862-1.135c-0.705-0.481,0.053-0.472,0.053-0.472 c0.779,0.055,1.189,0.8,1.189,0.8c0.692,1.186,1.816,0.843,2.258,0.645c0.071-0.502,0.271-0.843,0.493-1.037 C4.86,11.425,3.049,10.76,3.049,7.786c0-0.847,0.302-1.54,0.799-2.082C3.768,5.507,3.501,4.718,3.924,3.65 c0,0,0.652-0.209,2.134,0.796C6.677,4.273,7.34,4.187,8,4.184c0.659,0.003,1.323,0.089,1.943,0.261 c1.482-1.004,2.132-0.796,2.132-0.796c0.423,1.068,0.157,1.857,0.077,2.054c0.497,0.542,0.798,1.235,0.798,2.082 c0,2.981-1.814,3.637-3.543,3.829c0.279,0.24,0.527,0.713,0.527,1.437c0,1.037-0.01,1.874-0.01,2.129 c0,0.208,0.14,0.449,0.534,0.373c3.081-1.028,5.302-3.935,5.302-7.362C15.76,3.906,12.285,0.431,7.999,0.431z"/></svg>
</span><span class="username">alexandrugris</span></a>

          </li>
          

          
          <li>
            <a href="https://twitter.com/alexandrugris"><span class="icon icon--twitter"><svg viewBox="0 0 16 16" width="16px" height="16px"><path fill="#828282" d="M15.969,3.058c-0.586,0.26-1.217,0.436-1.878,0.515c0.675-0.405,1.194-1.045,1.438-1.809c-0.632,0.375-1.332,0.647-2.076,0.793c-0.596-0.636-1.446-1.033-2.387-1.033c-1.806,0-3.27,1.464-3.27,3.27 c0,0.256,0.029,0.506,0.085,0.745C5.163,5.404,2.753,4.102,1.14,2.124C0.859,2.607,0.698,3.168,0.698,3.767 c0,1.134,0.577,2.135,1.455,2.722C1.616,6.472,1.112,6.325,0.671,6.08c0,0.014,0,0.027,0,0.041c0,1.584,1.127,2.906,2.623,3.206 C3.02,9.402,2.731,9.442,2.433,9.442c-0.211,0-0.416-0.021-0.615-0.059c0.416,1.299,1.624,2.245,3.055,2.271 c-1.119,0.877-2.529,1.4-4.061,1.4c-0.264,0-0.524-0.015-0.78-0.046c1.447,0.928,3.166,1.469,5.013,1.469 c6.015,0,9.304-4.983,9.304-9.304c0-0.142-0.003-0.283-0.009-0.423C14.976,4.29,15.531,3.714,15.969,3.058z"/></svg>
</span><span class="username">alexandrugris</span></a>

          </li>
          
        </ul>
      </div>

      <div class="footer-col footer-col-3">
        <p>Alexandru Gris - Personal Blog
</p>
      </div>
    </div>

  </div>

</footer>


  </body>

</html>
