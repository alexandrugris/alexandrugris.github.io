<!DOCTYPE html>
<html lang="en">

  <head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <title>Dimensionality Reduction</title>
  <meta name="description" content="This post is about dimensionality reduction. It starts with PCA, a method for finding a set of axes (dimensions) along which to perform regression such that ...">

  <link rel="stylesheet" href="/assets/main.css">
  <link rel="canonical" href="https://alexandrugris.github.io/statistics/2018/08/22/pca.html">
  <link rel="alternate" type="application/rss+xml" title="From The Trenches - The Code" href="/feed.xml">
  
  
</head>


  <body>

    <header class="site-header" role="banner">

  <div class="wrapper">

    <div class="site-nav"><a class="site-title" href="/">From The Trenches - The Code</a> </div>

    <nav class="site-nav">
      <span class="menu-icon">        
      </span>

      <div class="trigger">

        <a class="page-link" href="https://alexandrugris.github.io">Home</a>

        
          
          <a class="page-link" href="/about/">About</a>
          
        
          
        
          
        
          
          <a class="page-link" href="/sm/">Social Media</a>
          
        
          
        
      </div>
    </nav>

  </div>

</header>


    <main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title" itemprop="name headline">Dimensionality Reduction</h1>
    <p class="post-meta"><time datetime="2018-08-22T14:15:16+03:00" itemprop="datePublished">Aug 22, 2018</time></p>
  </header>

  <div class="post-content" itemprop="articleBody">
    <p>This post is about dimensionality reduction. It starts with PCA, a method for finding a set of axes (dimensions) along which to perform regression such that the most relevant information in data is preserved, while multicollinearity is avoided. It then applies PCA and K-Means to a dataset of Premier League player performances, in order to obtain relevant groupings despite randomness in data.</p>

<h3 id="pca-principal-component-analysis">PCA (Principal Component Analysis)</h3>

<p>PCA is mathematical optimization problem. It aims to find the direction line along which the distance between projected data points is the greatest, that is it has the largest variance. We choose this direction because the largest amount of information about the value of each point is preserved in this projection. The second principal component is an axis perpendicular to the first axis and so on, for multi-dimensional coordinate spaces. This choice of axes not only preserves the most amount of information, but also avoids multicollinearity, as the principal components are always perpendicular to each other.</p>

<p>The problem can be rephrased as: given a set of highly correlated factors, <code class="highlighter-rouge">[X1 ... Xn]</code>, find a equal number of factors, <code class="highlighter-rouge">[F1 ... Fn]</code>, completely uncorrelated to each other, sorted so that <code class="highlighter-rouge">Var(F1) &gt; ... &gt; Var(Fn)</code>, on which we are able to describe our regression problem as <code class="highlighter-rouge">Y = A + B1 * F1 + ... Bn * Fn</code>. These factors satisfy the equality <code class="highlighter-rouge">Var(F) == Var(X)</code>, meaning we don’t lose information when we do the decomposition along these new axes.</p>

<p><em>It is important to notice the inequality, ``Var(F1) &gt; … &gt; Var(Fn)`. The higher the variance along that particular axis, the more information is contained in that axis.</em>.</p>

<p>The linear algebra problem that helps us find these factors is called <em>eigenvalue decomposition</em> and can be phrased as:</p>

<p>Find <code class="highlighter-rouge">F1 = a1X1 + ... + anXn</code> such that <code class="highlighter-rouge">Var(F1)</code> is maximized and the coefficients <code class="highlighter-rouge">a1 .. an</code> satisfy the constraint <code class="highlighter-rouge">a1^2 + ... + an^2 = 1</code>. <code class="highlighter-rouge">F1</code> is the principal component 1, <code class="highlighter-rouge">v1 = [a1,...,an]</code> is called <em>eigenvector 1</em> and <code class="highlighter-rouge">e1 = Var(F1)</code> is called the <em>eigenvalue</em> of the principal component <code class="highlighter-rouge">F1</code>.</p>

<p>Principal component <code class="highlighter-rouge">F2 = b1(X1 - F1) + ... + bn(Xn - F1)</code> is subject to the same constraints. Like this, we have defined a recurrent problem which allows us to find all <code class="highlighter-rouge">Fn</code> components.</p>

<p>Results from this decomposition are:</p>

<ul>
  <li>
    <p><em>eigenvalues:</em> tell us how much of the variance can be explain by this particular component</p>
  </li>
  <li>
    <p><em>the principal components themselves:</em> these can be used in regression if the eigenvalues are high enough</p>
  </li>
  <li>
    <p><em>eigenvectors:</em> which are needed to calculate the principal components as follows:
<code class="highlighter-rouge">[Fi] = [Xi] * [Vi]</code>. That is, the column matrix containing all factors is the matrix product between the column matrix of eigenvectors <code class="highlighter-rouge">Vi=[a1 ... an]</code> and the original factor matrix, <code class="highlighter-rouge">X=[X1 ... Xn]</code>, (n factors, each with k elements)</p>
  </li>
</ul>

<p>Now, dividing each eigenvalue <code class="highlighter-rouge">ei=Var(Fi)</code> to the total variance of F, <code class="highlighter-rouge">Var(F) = Var(F1) + ... Var(Fn)</code> since <code class="highlighter-rouge">Covar(Fi, Fj) = 0</code>, we obtain a vector <code class="highlighter-rouge">v</code> with <code class="highlighter-rouge">sum(vi) = 1</code>. The plot of this vector is called <em>scree plot</em> and shows how much each factor contributes to explaining the variance of the original data. This is the most important decision tool for us to decide which factors we keep in our analysis.</p>

<p>More information <a href="https://en.wikipedia.org/wiki/Eigendecomposition_of_a_matrix">here</a></p>

<h3 id="eigenvalue-decomposition">Eigenvalue Decomposition</h3>

<p>The following Python code uses the power method together with deflation for computing the eigenvectors and eigenvalues for a square matrix <code class="highlighter-rouge">A</code>.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span>

<span class="k">def</span> <span class="nf">vector_norm</span><span class="p">(</span><span class="n">v</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">v</span><span class="p">,</span> <span class="n">v</span><span class="p">))</span>

<span class="k">def</span> <span class="nf">dominant_eigenvector</span><span class="p">(</span><span class="n">A</span><span class="p">):</span>
    
    <span class="s">""" 
    Returns the principal eigenvector and its corresponding eigenvalue. 
    Uses the power method technique.
    """</span>
    
    <span class="c"># check that A is square</span>
    <span class="k">assert</span><span class="p">(</span><span class="n">A</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">==</span> <span class="n">A</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
    
    <span class="n">v</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="n">A</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
    <span class="n">v_prim</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">A</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
    
    <span class="n">lmbda</span> <span class="o">=</span> <span class="mi">0</span>
    
    <span class="c"># TODO: add raleigh for faster convergence</span>
    
    <span class="k">while</span> <span class="n">np</span><span class="o">.</span><span class="nb">max</span><span class="p">(</span><span class="n">v</span><span class="o">-</span><span class="n">v_prim</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mf">1e-10</span><span class="p">:</span>
       <span class="n">v_prim</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">A</span><span class="p">,</span> <span class="n">v</span><span class="p">)</span>
       <span class="n">lmbda</span> <span class="o">=</span> <span class="n">vector_norm</span><span class="p">(</span><span class="n">v_prim</span><span class="p">)</span>
       <span class="n">v</span><span class="p">,</span> <span class="n">v_prim</span> <span class="o">=</span> <span class="p">(</span><span class="n">v_prim</span> <span class="o">/</span> <span class="n">lmbda</span><span class="p">,</span> <span class="n">v</span><span class="p">)</span>
       
    <span class="c"># eigenvalue, normalized eigenvector</span>
    <span class="k">return</span> <span class="p">(</span><span class="n">lmbda</span><span class="p">,</span> <span class="n">v</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">eigen_decomposition</span><span class="p">(</span><span class="n">A</span><span class="p">):</span>
    
    <span class="c"># check that A is square</span>
    <span class="k">assert</span><span class="p">(</span><span class="n">A</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">==</span> <span class="n">A</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
    
    <span class="n">vectors</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">lambdas</span> <span class="o">=</span> <span class="p">[]</span>
    
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">A</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]):</span>
        
        <span class="n">lmbda</span><span class="p">,</span> <span class="n">v</span> <span class="o">=</span> <span class="n">dominant_eigenvector</span><span class="p">(</span><span class="n">A</span><span class="p">)</span>
        
        <span class="n">vectors</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">v</span><span class="p">),</span>
        <span class="n">lambdas</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">lmbda</span><span class="p">)</span>
        
        <span class="c"># power method deflation eigenvectors</span>
        <span class="c"># idea is to remove the initial contribution from the initial space</span>
        <span class="n">A</span> <span class="o">=</span> <span class="n">A</span> <span class="o">-</span> <span class="n">lmbda</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">v</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">A</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="mi">1</span><span class="p">),</span> <span class="n">v</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">A</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]))</span>
        
        <span class="c"># each vector is perpendicular to the rest</span>
       
    
    <span class="k">return</span> <span class="p">(</span><span class="n">lambdas</span><span class="p">,</span> <span class="n">vectors</span><span class="p">)</span>
</code></pre></div></div>

<p>The correctness is very simple to check. Just do the following:</p>

<ol>
  <li>Check that <code class="highlighter-rouge">A * lambda == lambda * v</code> - condition for eigenvector</li>
  <li>Check that all vectors are perpendicular to each other: <code class="highlighter-rouge">np.dot(v1, v2) == 0</code> - condition for all the eigenvectors</li>
</ol>

<h3 id="principal-factor-determination">Principal Factor Determination</h3>

<p>Let’s setup a test bed of linear-dependent variables. We will want to predict <code class="highlighter-rouge">Y</code> based on <code class="highlighter-rouge">A</code> - see below:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">from</span> <span class="nn">matplotlib</span> <span class="kn">import</span> <span class="n">pyplot</span> <span class="k">as</span> <span class="n">plt</span>

<span class="c"># 2 factors, 1000 points each</span>
<span class="n">a</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">100</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">)</span>
<span class="n">b</span> <span class="o">=</span> <span class="mi">3</span> <span class="o">*</span> <span class="n">a</span> <span class="o">+</span> <span class="mi">5</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">50</span><span class="p">,</span> <span class="n">a</span><span class="o">.</span><span class="n">size</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">)</span>

<span class="n">A</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">])</span>
<span class="n">Y</span> <span class="o">=</span> <span class="mf">1.6</span> <span class="o">*</span> <span class="n">a</span> <span class="o">-</span> <span class="mf">2.3</span> <span class="o">*</span> <span class="n">b</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">20</span><span class="p">,</span> <span class="n">a</span><span class="o">.</span><span class="n">size</span><span class="p">)</span>
</code></pre></div></div>

<p>Let’s compute the covariance of and correlation of the two vectors, <code class="highlighter-rouge">a</code> and <code class="highlighter-rouge">b</code>:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">covar</span><span class="p">(</span><span class="n">_x</span><span class="p">,</span> <span class="n">_y</span><span class="p">):</span>

    <span class="n">x</span> <span class="o">=</span> <span class="n">_x</span><span class="o">.</span><span class="n">flatten</span><span class="p">()</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">_y</span><span class="o">.</span><span class="n">flatten</span><span class="p">()</span>

    <span class="k">if</span> <span class="n">x</span><span class="o">.</span><span class="n">size</span> <span class="o">!=</span> <span class="n">y</span><span class="o">.</span><span class="n">size</span><span class="p">:</span>
        <span class="k">raise</span> <span class="nb">Exception</span><span class="p">(</span><span class="s">"x and y should have the same size"</span><span class="p">)</span>

    <span class="n">mean_x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">mean_y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>

    <span class="n">N</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">size</span>

    <span class="k">return</span> <span class="p">(</span><span class="mi">1</span> <span class="o">/</span> <span class="n">N</span><span class="p">)</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="nb">sum</span><span class="p">((</span><span class="n">x</span> <span class="o">-</span> <span class="n">mean_x</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span><span class="n">y</span> <span class="o">-</span> <span class="n">mean_y</span><span class="p">))</span>

<span class="k">def</span> <span class="nf">corr_mtx</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="n">covar</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">var</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">var</span><span class="p">(</span><span class="n">y</span><span class="p">))</span> <span class="k">for</span> <span class="n">y</span> <span class="ow">in</span> <span class="n">Y</span><span class="p">]</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">X</span><span class="p">])</span>
                      
<span class="k">def</span> <span class="nf">covar_mtx</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="n">covar</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span> <span class="k">for</span> <span class="n">y</span> <span class="ow">in</span> <span class="n">Y</span><span class="p">]</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">X</span><span class="p">])</span>

<span class="c"># high Corr above / below diagonal =&gt; the two factors are highly correlated =&gt; we should switch to PCA</span>
<span class="c"># in our case, corr above diagonal is 0.83</span>
<span class="c"># this can be seen also from plotting a vs b</span>
<span class="n">Corr_AA</span> <span class="o">=</span> <span class="n">corr_mtx</span><span class="p">(</span><span class="n">A</span><span class="p">,</span> <span class="n">A</span><span class="p">)</span>
</code></pre></div></div>

<p>Eigenvalue decomposition is applied to the covariance matrix. So let’s do just that:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># eigen value decomposition is applied on the covariance matrix</span>
<span class="n">Covar_AA</span> <span class="o">=</span> <span class="n">covar_mtx</span><span class="p">(</span><span class="n">A</span><span class="p">,</span> <span class="n">A</span><span class="p">)</span>

<span class="c"># lmbdas contain the eigenvalues and v the normalized eigenvectors</span>
<span class="n">lmbdas</span><span class="p">,</span> <span class="n">v</span> <span class="o">=</span> <span class="n">eigen_decomposition</span><span class="p">(</span><span class="n">Covar_AA</span><span class="p">)</span>
</code></pre></div></div>

<p>We can quickly check that the two <code class="highlighter-rouge">v's</code> are perpendicular to each other, by doing</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>np.dot(v[0], v[1])
Out[209]: -9.731923600320158e-11
</code></pre></div></div>

<p>Now we can call the following function which computes the principal factors (vector <code class="highlighter-rouge">F</code> described in the <em>PCA</em> section) based on the initial matrix <code class="highlighter-rouge">A</code> and the eigenvalues vector <code class="highlighter-rouge">v</code>.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">factors</span><span class="p">(</span><span class="n">A</span><span class="p">,</span> <span class="n">vectors</span><span class="p">):</span>
    <span class="c"># rough check that all vectors are normalized</span>
    <span class="k">for</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">vectors</span><span class="p">:</span>
        <span class="k">assert</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="nb">abs</span><span class="p">(</span><span class="n">vector_norm</span><span class="p">(</span><span class="n">v</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">&lt;</span> <span class="mf">1e-5</span><span class="p">)</span>

    <span class="n">Fs</span> <span class="o">=</span> <span class="p">[[</span><span class="n">np</span><span class="o">.</span><span class="nb">sum</span><span class="p">([</span><span class="n">A</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">*</span> <span class="n">v</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">v</span><span class="p">))],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)]</span> <span class="k">for</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">vectors</span><span class="p">]</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">Fs</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">A</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
</code></pre></div></div>

<p>and then</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">Fs</span> <span class="o">=</span> <span class="n">factors</span><span class="p">(</span><span class="n">A</span><span class="p">,</span> <span class="n">v</span><span class="p">)</span>
</code></pre></div></div>

<p>Now we can quickly check that, indeed, <code class="highlighter-rouge">Var(Fs[i]) == lmdas[i]</code>:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">np</span><span class="o">.</span><span class="n">var</span><span class="p">(</span><span class="n">Fs</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">Out</span><span class="p">[</span><span class="mi">226</span><span class="p">]:</span> <span class="n">array</span><span class="p">([</span><span class="mf">10333.05692141</span><span class="p">,</span>   <span class="mf">199.78928639</span><span class="p">])</span>

<span class="n">lmbdas</span>
<span class="n">Out</span><span class="p">[</span><span class="mi">227</span><span class="p">]:</span> <span class="p">[</span><span class="mf">10333.056921412048</span><span class="p">,</span> <span class="mf">199.78928639278513</span><span class="p">]</span>
</code></pre></div></div>

<p>This also shows that the fist factor contributes <code class="highlighter-rouge">98%</code> to the total variance, so we can use it alone in regression.</p>

<h3 id="final-regression-and-check">Final Regression and Check</h3>

<p>We will simply plot <code class="highlighter-rouge">Fs[0]</code> vs <code class="highlighter-rouge">Y</code> and <code class="highlighter-rouge">Fs[1]</code> vs <code class="highlighter-rouge">Y</code>. Obviously, much more of the variation is explained by <code class="highlighter-rouge">Fs[0]</code> than <code class="highlighter-rouge">Fs[1]</code>:</p>

<p><img src="https://alexandrugris.github.io/assets/pca_1.png" alt="Analysis" /></p>

<h3 id="normalization-of-data">Normalization of Data</h3>

<p>If we end up with the principal <code class="highlighter-rouge">F</code> factor explaining less of the variance any of the <code class="highlighter-rouge">X</code> initial factors, we must standardize our data. For our <code class="highlighter-rouge">A</code> matrix, the matrix of all <code class="highlighter-rouge">X</code> factors, the function is as follows:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">standardize</span><span class="p">(</span><span class="n">A</span><span class="p">):</span>
    <span class="k">return</span> <span class="p">((</span><span class="n">A</span><span class="o">.</span><span class="n">T</span> <span class="o">-</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">A</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">T</span><span class="p">)</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">var</span><span class="p">(</span><span class="n">A</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">))</span><span class="o">.</span><span class="n">T</span><span class="p">)</span><span class="o">.</span><span class="n">T</span>
</code></pre></div></div>

<p>We check this need by calculating the <a href="http://www.stat.yale.edu/Courses/1997-98/101/correl.htm">R^2 metric for each regression (RSQ in Excel)</a> - for the <code class="highlighter-rouge">X</code> factors and then for the <code class="highlighter-rouge">F</code> factors. Now we can safely use the covariance matrix to do PCA.</p>

<p>Some notes - <a href="https://alexandrugris.github.io/assets/pca.pdf">example data here</a>:</p>

<ul>
  <li>R^2 is the square of R, the Pearson coefficient, the correlation coefficient between two data series</li>
  <li>Correlation of two data series is the same before and after standardization. Standardization is a linear operator, thus does not affect the linear relationship between two variables.</li>
</ul>

<p>However, there are more points to consider:</p>

<ol>
  <li>
    <p><a href="https://stats.stackexchange.com/questions/69157/why-do-we-need-to-normalize-data-before-principal-component-analysis-pca">Why do we need to normalize data before principal component analysis?</a></p>
  </li>
  <li>
    <p><a href="https://stats.stackexchange.com/questions/53/pca-on-correlation-or-covariance">PCA on correlation or covariance?</a></p>
  </li>
</ol>

<blockquote>
  <p>Using the correlation matrix standardizes the data. In general they [corr vs covar] give different results, especially when the scales are different. If we use the correlation matrix, we don’t need to standardize the data before. Hence, my answer is to use the covariance matrix when the variance of the original variable is important, and use correlation when it is not.</p>
</blockquote>

<p>And</p>

<blockquote>
  <p>The argument against automatically using correlation matrices is that it is quite a brutal way of standardizing your data. The problem with automatically using the covariance matrix, which is very apparent with that heptathalon data, is that the variables with the highest variance will dominate the first principal component (the variance maximizing property).</p>
</blockquote>

<h3 id="additional-notes">Additional Notes:</h3>

<ol>
  <li>
    <p>Eigenvalues and eigenvectors are directly baked into numpy: <a href="https://docs.scipy.org/doc/numpy/reference/generated/numpy.linalg.eig.html">np.linalg.eig</a></p>
  </li>
  <li>
    <p>Many other numeric algorithms are already baked into numpy: <a href="https://docs.scipy.org/doc/numpy/reference/routines.linalg.html">np.linalg</a></p>
  </li>
</ol>

<p><a href="https://skymind.ai/wiki/eigenvector">PCA</a></p>

<p>So basically, just like a space transform in a 3D space, by multiplying the observed data with the coordinate system which describes best its covariance / correlation, we bring that data into the coordinate space where each axis represent, in decreasing order, its highest variation. Matrix multiplication is simply a linear (coordinate space) transformation.</p>

<h3 id="example-on-premier-league-season">Example on Premier League Season</h3>

<p>We are going to use the datasets downloaded from <a href="https://footystats.org/download-stats-csv">here</a>. The purpose of the example is to group players in similar clusters, so that we can use them to enrich scarce data. That is, we know that players don’t score in every match and that there are relatively few matches in a season. We want to group players by similarity in order to obtain a better (enriched) model for player to score probability. These clusters could further be used to predict other measures like team-expected-goals or to predict player relevance in a certain team. Obviously, this is not a fully developed model, it is intended just to show how these two techniques can be used together.</p>

<p>For the actual clustering we will use KMeans. However, KMeans is not suited to dummy data, not even z-scored, and it is even worse if the counts are not balanced.</p>

<p>There are latent correlations within the data as well. These correlations will tend to increase the importance of certain features as well as pollute the distance metric, leading to less relevant clustering. Therefore, we will use PCA to reduce this dimensionality, as well as decrease the errors in the distance function introduced by the dummy variables.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="n">pd</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">from</span> <span class="nn">scipy.stats</span> <span class="kn">import</span> <span class="n">zscore</span>
<span class="kn">from</span> <span class="nn">sklearn.cluster</span> <span class="kn">import</span> <span class="n">KMeans</span>

<span class="c"># https://footystats.org/download-stats-csv</span>
<span class="n">pd_players</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s">"./england-premier-league-players-2018-to-2019-stats.csv"</span><span class="p">)</span>

<span class="c"># index by player</span>
<span class="n">pd_players</span> <span class="o">=</span> <span class="n">pd_players</span><span class="o">.</span><span class="n">set_index</span><span class="p">(</span><span class="s">'full_name'</span><span class="p">)</span>
<span class="c"># we know the league and the season</span>
<span class="n">pd_players</span> <span class="o">=</span> <span class="n">pd_players</span><span class="o">.</span><span class="n">drop</span><span class="p">(</span> <span class="p">[</span><span class="s">'league'</span><span class="p">,</span> <span class="s">'season'</span><span class="p">,</span> <span class="s">'nationality'</span><span class="p">]</span> <span class="p">,</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

<span class="c"># these are filled with wrong values</span>
<span class="c"># not very relevant for our task</span>
<span class="n">pd_players</span> <span class="o">=</span> <span class="n">pd_players</span><span class="o">.</span><span class="n">drop</span><span class="p">(</span> <span class="p">[</span><span class="s">'rank_in_league_top_attackers'</span><span class="p">,</span> 
                               <span class="s">'rank_in_league_top_midfielders'</span><span class="p">,</span> 
                               <span class="s">'rank_in_league_top_defenders'</span><span class="p">,</span> 
                               <span class="s">'rank_in_club_top_scorer'</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

<span class="c"># keep only the overall</span>
<span class="c"># the home and away should be used for clustering </span>
<span class="c"># when pitching teams against each other</span>
<span class="n">pd_players</span> <span class="o">=</span> <span class="n">pd_players</span><span class="p">[[</span><span class="s">'age'</span><span class="p">,</span> 
                         <span class="s">'position'</span><span class="p">,</span> 
                         <span class="s">'Current Club'</span><span class="p">,</span> 
                         <span class="s">'minutes_played_overall'</span><span class="p">,</span> 
                         <span class="s">'appearances_overall'</span><span class="p">,</span> 
                         <span class="s">'goals_overall'</span><span class="p">,</span> 
                         <span class="s">'assists_overall'</span><span class="p">,</span> 
                         <span class="s">'penalty_goals'</span><span class="p">,</span> 
                         <span class="s">'penalty_misses'</span><span class="p">,</span> 
                         <span class="s">'clean_sheets_overall'</span><span class="p">,</span> 
                         <span class="s">'conceded_overall'</span><span class="p">,</span> 
                         <span class="s">'yellow_cards_overall'</span><span class="p">,</span> 
                         <span class="s">'red_cards_overall'</span><span class="p">]]</span>


<span class="n">players_with_dummies_not_zscored</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">get_dummies</span><span class="p">(</span><span class="n">pd_players</span><span class="p">)</span>
<span class="n">players_with_dummies</span> <span class="o">=</span> <span class="n">players_with_dummies_not_zscored</span><span class="o">.</span><span class="nb">apply</span><span class="p">(</span><span class="n">zscore</span><span class="p">)</span>

<span class="c"># here we add some weights for the goal scored</span>
<span class="n">w</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">1.0</span><span class="p">]</span> <span class="o">*</span> <span class="nb">len</span><span class="p">(</span><span class="n">players_with_dummies</span><span class="o">.</span><span class="n">columns</span><span class="p">))</span>

<span class="c"># increase a little bit the weight for non-club features</span>
<span class="n">np</span><span class="o">.</span><span class="n">put</span><span class="p">(</span><span class="n">w</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">15</span><span class="p">),</span> <span class="mf">1.1</span><span class="p">)</span> 

<span class="c"># since we are looking mostly at goals, </span>
<span class="c"># we want the clustering to focus on goals as the main success measure</span>
<span class="c"># the other dimensions being used mostly for smoothing out the outliers</span>

<span class="n">w</span><span class="p">[</span><span class="mi">3</span><span class="p">]</span> <span class="o">=</span> <span class="mf">3.0</span> <span class="c"># goals</span>
<span class="n">w</span><span class="p">[</span><span class="mi">11</span><span class="p">]</span> <span class="o">=</span> <span class="mf">1.3</span> <span class="c"># position</span>
<span class="n">w</span><span class="p">[</span><span class="mi">12</span><span class="p">]</span> <span class="o">=</span> <span class="mf">1.3</span> <span class="c"># position</span>
<span class="n">w</span><span class="p">[</span><span class="mi">13</span><span class="p">]</span> <span class="o">=</span> <span class="mf">1.3</span> <span class="c"># position</span>
<span class="n">w</span><span class="p">[</span><span class="mi">14</span><span class="p">]</span> <span class="o">=</span> <span class="mf">1.3</span> <span class="c"># position</span>

<span class="n">players_with_dummies</span> <span class="o">=</span> <span class="n">players_with_dummies</span><span class="o">.</span><span class="nb">apply</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">x</span> <span class="o">*</span> <span class="n">w</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
</code></pre></div></div>

<p>This is how the data looks after preprocessing:</p>

<p><img src="https://alexandrugris.github.io/assets/pca_2.png" alt="How the data looks now" /></p>

<p>Now we are going to use PCA with a relatively low explained variance threshold, so that data is smoothed out and we only keep the most relevant features.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">sklearn.decomposition</span> <span class="kn">import</span> <span class="n">PCA</span>

<span class="n">pca</span> <span class="o">=</span> <span class="n">PCA</span><span class="p">(</span><span class="mf">0.75</span><span class="p">)</span> <span class="c"># explained variance - 80%</span>
<span class="n">pca</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">players_with_dummies</span><span class="p">)</span>

<span class="c"># number of features</span>
<span class="c"># len(pca.explained_variance_ratio_)</span>
<span class="c"># explained variance ratio</span>
<span class="c"># sum(pca.explained_variance_ratio_)</span>

<span class="c"># preserve the columns and index of the dataset</span>
<span class="n">columns</span> <span class="o">=</span> <span class="n">players_with_dummies</span><span class="o">.</span><span class="n">columns</span>
<span class="n">index</span> <span class="o">=</span> <span class="n">players_with_dummies</span><span class="o">.</span><span class="n">index</span>

<span class="n">players_with_dummies</span> <span class="o">=</span> <span class="n">pca</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">players_with_dummies</span><span class="p">)</span>
</code></pre></div></div>

<p>We see in the picture below the explained variance of <code class="highlighter-rouge">75%</code> as well as the reduction of features from <code class="highlighter-rouge">35</code> to <code class="highlighter-rouge">17</code>.</p>

<p><img src="https://alexandrugris.github.io/assets/pca_3.png" alt="Explained variance" /></p>

<p>Now we are going to perform clustering on the reduced feature set, as well as transform back the cluster values from the PCA features to the original features.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
<span class="c"># we will split in 10 clusters</span>
<span class="n">players</span> <span class="o">=</span> <span class="n">KMeans</span><span class="p">(</span><span class="n">n_clusters</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
<span class="n">players</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">players_with_dummies</span><span class="p">)</span>

<span class="c"># transform the clusters to original features</span>
<span class="n">clusters</span> <span class="o">=</span> <span class="n">pca</span><span class="o">.</span><span class="n">inverse_transform</span><span class="p">(</span><span class="n">players</span><span class="o">.</span><span class="n">cluster_centers_</span><span class="p">)</span>
<span class="n">clusters</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">clusters</span><span class="p">,</span> <span class="n">columns</span><span class="o">=</span><span class="n">columns</span><span class="p">)</span>

<span class="c"># inverse weights</span>
<span class="n">clusters</span> <span class="o">=</span> <span class="n">clusters</span><span class="o">.</span><span class="nb">apply</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">x</span> <span class="o">*</span> <span class="mi">1</span><span class="o">/</span><span class="n">w</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="c"># inverse the zscore</span>
<span class="n">clusters</span> <span class="o">=</span> <span class="n">clusters</span> <span class="o">*</span> <span class="n">players_with_dummies_not_zscored</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span> <span class="o">+</span> <span class="n">players_with_dummies_not_zscored</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="c"># set to 0 all annomalies, values less than 0</span>
<span class="n">clusters</span><span class="p">[</span><span class="n">clusters</span> <span class="o">&lt;</span> <span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="mi">0</span>
</code></pre></div></div>

<p>The centers of our clusters:</p>

<p><img src="https://alexandrugris.github.io/assets/pca_4.png" alt="Explained variance" /></p>

<p>And each player assigned to its cluster, with predicted goals for the season:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">pd_players</span><span class="p">[</span><span class="s">'Cluster'</span><span class="p">]</span> <span class="o">=</span> <span class="n">players</span><span class="o">.</span><span class="n">labels_</span>
<span class="n">pd_players</span><span class="p">[</span><span class="s">'Predicted Goals'</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="n">clusters</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="n">pd_players</span><span class="p">[</span><span class="s">'Cluster'</span><span class="p">]])[</span><span class="s">'goals_overall'</span><span class="p">]</span><span class="o">.</span><span class="n">to_numpy</span><span class="p">()</span>
<span class="n">pd_players</span><span class="p">[</span><span class="s">'Predicted Appearances'</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="n">clusters</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="n">pd_players</span><span class="p">[</span><span class="s">'Cluster'</span><span class="p">]])[</span><span class="s">'appearances_overall'</span><span class="p">]</span><span class="o">.</span><span class="n">to_numpy</span><span class="p">()</span>
<span class="n">pd_players</span><span class="p">[</span><span class="s">'Predicted Goal Rate'</span><span class="p">]</span> <span class="o">=</span> <span class="n">pd_players</span><span class="p">[</span><span class="s">'Predicted Goals'</span><span class="p">]</span> <span class="o">/</span> <span class="n">pd_players</span><span class="p">[</span><span class="s">'Predicted Appearances'</span><span class="p">]</span>
</code></pre></div></div>

<p>Our players sorted descending by the number of goals, showing the correlation with the predicted goals from the cluster:</p>

<p><img src="https://alexandrugris.github.io/assets/pca_5.png" alt="Predicted goals" /></p>

<p>The technique above is far from perfect, but it does show some relevant groups of players and could be used further as a feature in a regression or to predict the performance of teams of players.</p>

  </div>

  
</article>

      </div>
    </main>

    <footer class="site-footer">

  <div class="wrapper">

    <h2 class="footer-heading">From The Trenches - The Code</h2>

    <div class="footer-col-wrapper">
      <div class="footer-col footer-col-1">
        <ul class="contact-list">
          <li>
            
              From The Trenches - The Code
            
            </li>
            
            <li><a href="mailto:alexandru.gris2006@gmail.com">alexandru.gris2006@gmail.com</a></li>
            
        </ul>
      </div>

      <div class="footer-col footer-col-2">
        <ul class="social-media-list">
          
          <li>
            <a href="https://github.com/alexandrugris"><span class="icon icon--github"><svg viewBox="0 0 16 16" width="16px" height="16px"><path fill="#828282" d="M7.999,0.431c-4.285,0-7.76,3.474-7.76,7.761 c0,3.428,2.223,6.337,5.307,7.363c0.388,0.071,0.53-0.168,0.53-0.374c0-0.184-0.007-0.672-0.01-1.32 c-2.159,0.469-2.614-1.04-2.614-1.04c-0.353-0.896-0.862-1.135-0.862-1.135c-0.705-0.481,0.053-0.472,0.053-0.472 c0.779,0.055,1.189,0.8,1.189,0.8c0.692,1.186,1.816,0.843,2.258,0.645c0.071-0.502,0.271-0.843,0.493-1.037 C4.86,11.425,3.049,10.76,3.049,7.786c0-0.847,0.302-1.54,0.799-2.082C3.768,5.507,3.501,4.718,3.924,3.65 c0,0,0.652-0.209,2.134,0.796C6.677,4.273,7.34,4.187,8,4.184c0.659,0.003,1.323,0.089,1.943,0.261 c1.482-1.004,2.132-0.796,2.132-0.796c0.423,1.068,0.157,1.857,0.077,2.054c0.497,0.542,0.798,1.235,0.798,2.082 c0,2.981-1.814,3.637-3.543,3.829c0.279,0.24,0.527,0.713,0.527,1.437c0,1.037-0.01,1.874-0.01,2.129 c0,0.208,0.14,0.449,0.534,0.373c3.081-1.028,5.302-3.935,5.302-7.362C15.76,3.906,12.285,0.431,7.999,0.431z"/></svg>
</span><span class="username">alexandrugris</span></a>

          </li>
          

          
          <li>
            <a href="https://twitter.com/alexandrugris"><span class="icon icon--twitter"><svg viewBox="0 0 16 16" width="16px" height="16px"><path fill="#828282" d="M15.969,3.058c-0.586,0.26-1.217,0.436-1.878,0.515c0.675-0.405,1.194-1.045,1.438-1.809c-0.632,0.375-1.332,0.647-2.076,0.793c-0.596-0.636-1.446-1.033-2.387-1.033c-1.806,0-3.27,1.464-3.27,3.27 c0,0.256,0.029,0.506,0.085,0.745C5.163,5.404,2.753,4.102,1.14,2.124C0.859,2.607,0.698,3.168,0.698,3.767 c0,1.134,0.577,2.135,1.455,2.722C1.616,6.472,1.112,6.325,0.671,6.08c0,0.014,0,0.027,0,0.041c0,1.584,1.127,2.906,2.623,3.206 C3.02,9.402,2.731,9.442,2.433,9.442c-0.211,0-0.416-0.021-0.615-0.059c0.416,1.299,1.624,2.245,3.055,2.271 c-1.119,0.877-2.529,1.4-4.061,1.4c-0.264,0-0.524-0.015-0.78-0.046c1.447,0.928,3.166,1.469,5.013,1.469 c6.015,0,9.304-4.983,9.304-9.304c0-0.142-0.003-0.283-0.009-0.423C14.976,4.29,15.531,3.714,15.969,3.058z"/></svg>
</span><span class="username">alexandrugris</span></a>

          </li>
          
        </ul>
      </div>

      <div class="footer-col footer-col-3">
        <p>Alexandru Gris - Personal Blog
</p>
      </div>
    </div>

  </div>

</footer>


  </body>

</html>
