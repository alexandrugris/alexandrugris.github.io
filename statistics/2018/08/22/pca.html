<!DOCTYPE html>
<html lang="en">

  <head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <title>Principal Component Analisys</title>
  <meta name="description" content="This is a continuation on my previous post on linear regression. It talks about finding a set of axes (dimensions) along which to regress in such a way as to...">

  <link rel="stylesheet" href="/assets/main.css">
  <link rel="canonical" href="https://alexandrugris.github.io/statistics/2018/08/22/pca.html">
  <link rel="alternate" type="application/rss+xml" title="From The Trenches - The Code" href="/feed.xml">
  
  
</head>


  <body>

    <header class="site-header" role="banner">

  <div class="wrapper">

    <div class="site-nav"><a class="site-title" href="/">From The Trenches - The Code</a> </div>

    <nav class="site-nav">
      <span class="menu-icon">        
      </span>

      <div class="trigger">

        <a class="page-link" href="https://alexandrugris.github.io">Home</a>

        
          
          <a class="page-link" href="/about/">About</a>
          
        
          
        
          
        
          
          <a class="page-link" href="/sm/">Social Media</a>
          
        
          
        
          
        
      </div>
    </nav>

  </div>

</header>


    <main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title" itemprop="name headline">Principal Component Analisys</h1>
    <p class="post-meta"><time datetime="2018-08-22T14:15:16+03:00" itemprop="datePublished">Aug 22, 2018</time></p>
  </header>

  <div class="post-content" itemprop="articleBody">
    <p>This is a continuation on my previous post on linear regression. It talks about finding a set of axes (dimensions) along which to regress in such a way as to preserve the most relevant data while avoiding multicollinearity and reducing the amount of dimensions.</p>

<h3 id="pca">PCA</h3>

<p>Mathematical optimization problem to find the direction line along which the distance between projected data points is the greatest, that is it has the largest variance. We choose this direction because the largest amount of information about the value of each point is preserved in this projection. The second principal component is an axis perpendicular to the first axis and so on, for multi-dimensional coordiante spaces. This choice of axes not only preserves the most amount of information, but also avoids multicollinearity, as the principal compoments are perpendicular to each other.</p>

<p>The problem can be rephrased as: given a set of highly correlated factors, <code class="highlighter-rouge">[X1 ... Xn]</code>, find a equal number of factors, <code class="highlighter-rouge">[F1 ... Fn]</code>, completely uncorrelated to each other, sorted so that <code class="highlighter-rouge">Var(F1) &gt; ... &gt; Var(Fn)</code>, on which we are able to describe our regression problem as <code class="highlighter-rouge">Y = A + B1 * F1 + ... Bn * Fn</code>. These factors satisfy the equality <code class="highlighter-rouge">Var(F) == Var(X)</code>, meaning we don’t lose information when we do the decomposition along these new axes.</p>

<p><em>It is important to notice the inequality, ``Var(F1) &gt; … &gt; Var(Fn)`. The higher the variance along that particular axis, the more information is contained in that axis.</em>.</p>

<p>The linear algebra problem that helps us find these factors is called <em>eigenvalue decomposition</em> and can be phrased as:</p>

<blockquote>
  <p>Find <code class="highlighter-rouge">F1 = a1X1 + ... + anXn</code> such that <code class="highlighter-rouge">Var(F1)</code> is maximised and the coefficients <code class="highlighter-rouge">a1 .. an</code> satisfy the contraint <code class="highlighter-rouge">a1^2 + ... + an^2 = 1</code>. <code class="highlighter-rouge">F1</code> is the principal component 1, <code class="highlighter-rouge">v1 = [a1,...,an]</code> is called <em>eigenvector 1</em> and <code class="highlighter-rouge">e1 = Var(F1)</code> is called the <em>eigenvalue</em> of the principal component <code class="highlighter-rouge">F1</code>.</p>
</blockquote>

<blockquote>
  <p>Principal component <code class="highlighter-rouge">F2 = b1(X1 - F1) + ... + bn(Xn - F1)</code> is subject to the same constraints. Like this, we have defined a recurrent problem which allows us to find all <code class="highlighter-rouge">Fn</code> components.</p>
</blockquote>

<p>Results from decomposition are:</p>

<ul>
  <li>
    <p><em>eigenvalues:</em> tell us how much of the variance can be explain by this particular component</p>
  </li>
  <li>
    <p><em>the principal components themselves:</em> these can be used in regression if the eigenvalues are high enough</p>
  </li>
  <li>
    <p><em>eigenvectors:</em> which are needed to calculate the principal components as follows:
<code class="highlighter-rouge">[Fi] = [Xi] * [Vi]</code>. That is, the column matrix containing all factors is the matrix product between the column matrix of eigenvectors <code class="highlighter-rouge">Vi=[a1 ... an]</code> and the original factor matrix, <code class="highlighter-rouge">X=[X1 ... Xn]</code>, (n factors, each with k elements)</p>
  </li>
</ul>

<p>Now, dividing each eigenvalue <code class="highlighter-rouge">ei=Var(Fi)</code> to the total variance of F, <code class="highlighter-rouge">Var(F) = Var(F1) + ... Var(Fn)</code> since <code class="highlighter-rouge">Covar(Fi, Fj) = 0</code>, we obtain a vector <code class="highlighter-rouge">v</code> with <code class="highlighter-rouge">sum(vi) = 1</code>. The plot of this vector is called <em>scree plot</em> and shows how much each factor contributes to explaining the variance of the original data. This is the most important decision tool for us to decide which factors we keep in our analysis.</p>

<p>More information <a href="https://en.wikipedia.org/wiki/Eigendecomposition_of_a_matrix">here</a></p>

<h3 id="eigenvalue-decomposition">Eigenvalue decomposition</h3>

<p>The following Python code uses the power method together with deflation for computing the eigenvectors and eigenvalues for a <em>square</em> matrix <code class="highlighter-rouge">A</code>.</p>

<div class="language-python highlighter-rouge"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>

<span class="k">def</span> <span class="nf">vector_norm</span><span class="p">(</span><span class="n">v</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">v</span><span class="p">,</span> <span class="n">v</span><span class="p">))</span>

<span class="k">def</span> <span class="nf">dominant_eigenvector</span><span class="p">(</span><span class="n">A</span><span class="p">):</span>
    
    <span class="s">""" Returns the principal eigen vector and its corresponding eigen value. uses the power method technique """</span>
    
    <span class="c"># check that A is square</span>
    <span class="k">assert</span><span class="p">(</span><span class="n">A</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">==</span> <span class="n">A</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
    
    <span class="n">v</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="n">A</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
    <span class="n">v_prim</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">A</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
    
    <span class="n">lmbda</span> <span class="o">=</span> <span class="mi">0</span>
    
    <span class="c"># TODO: add raleigh for faster convergence</span>
    
    <span class="k">while</span> <span class="n">np</span><span class="o">.</span><span class="nb">max</span><span class="p">(</span><span class="n">v</span><span class="o">-</span><span class="n">v_prim</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mf">1e-10</span><span class="p">:</span>
       <span class="n">v_prim</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">A</span><span class="p">,</span> <span class="n">v</span><span class="p">)</span>
       <span class="n">lmbda</span> <span class="o">=</span> <span class="n">vector_norm</span><span class="p">(</span><span class="n">v_prim</span><span class="p">)</span>
       <span class="n">v</span><span class="p">,</span> <span class="n">v_prim</span> <span class="o">=</span> <span class="p">(</span><span class="n">v_prim</span> <span class="o">/</span> <span class="n">lmbda</span><span class="p">,</span> <span class="n">v</span><span class="p">)</span>
       
    <span class="c"># eigenvalue, normalized eigenvector</span>
    <span class="k">return</span> <span class="p">(</span><span class="n">lmbda</span><span class="p">,</span> <span class="n">v</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">eigen_decomposition</span><span class="p">(</span><span class="n">A</span><span class="p">):</span>
    
    <span class="c"># check that A is square</span>
    <span class="k">assert</span><span class="p">(</span><span class="n">A</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">==</span> <span class="n">A</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
    
    <span class="n">vectors</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">lambdas</span> <span class="o">=</span> <span class="p">[]</span>
    
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">A</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]):</span>
        
        <span class="n">lmbda</span><span class="p">,</span> <span class="n">v</span> <span class="o">=</span> <span class="n">dominant_eigenvector</span><span class="p">(</span><span class="n">A</span><span class="p">)</span>
        
        <span class="n">vectors</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">v</span><span class="p">),</span>
        <span class="n">lambdas</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">lmbda</span><span class="p">)</span>
        
        <span class="c"># power method deflation eigenvectors</span>
        <span class="c"># idea is to remove the initial contribution from the initial space</span>
        <span class="n">A</span> <span class="o">=</span> <span class="n">A</span> <span class="o">-</span> <span class="n">lmbda</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">v</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">A</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="mi">1</span><span class="p">),</span> <span class="n">v</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">A</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]))</span>
        
        <span class="c"># each vector is perpendicular to the rest</span>
       
    
    <span class="k">return</span> <span class="p">(</span><span class="n">lambdas</span><span class="p">,</span> <span class="n">vectors</span><span class="p">)</span>
</code></pre>
</div>

<p>Correctness is very simple to check. Do the following:</p>

<ol>
  <li>Check that <code class="highlighter-rouge">A * lambda == lambda * v</code> - condition for eigenvector</li>
  <li>Check that all vectors are perpendicular to eachother: <code class="highlighter-rouge">np.dot(v1, v2) == 0</code> - condition for all the eigenvectors</li>
</ol>

<h3 id="principal-factor-determination">Principal factor determination</h3>

<p>Let’s setup a test bed of linear-dependent variables. We will want to predict <code class="highlighter-rouge">Y</code> based on <code class="highlighter-rouge">A</code> - see below:</p>

<div class="language-python highlighter-rouge"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>
<span class="kn">from</span> <span class="nn">matplotlib</span> <span class="kn">import</span> <span class="n">pyplot</span> <span class="k">as</span> <span class="n">plt</span>

<span class="c"># 2 factors, 1000 points each</span>
<span class="n">a</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">100</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">)</span>
<span class="n">b</span> <span class="o">=</span> <span class="mi">3</span> <span class="o">*</span> <span class="n">a</span> <span class="o">+</span> <span class="mi">5</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">50</span><span class="p">,</span> <span class="n">a</span><span class="o">.</span><span class="n">size</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">)</span>

<span class="n">A</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">])</span>
<span class="n">Y</span> <span class="o">=</span> <span class="mf">1.6</span> <span class="o">*</span> <span class="n">a</span> <span class="o">-</span> <span class="mf">2.3</span> <span class="o">*</span> <span class="n">b</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">20</span><span class="p">,</span> <span class="n">a</span><span class="o">.</span><span class="n">size</span><span class="p">)</span>
</code></pre>
</div>

<p>Let’s compute the covariance of and correlation of the two vectors, <code class="highlighter-rouge">a</code> and <code class="highlighter-rouge">b</code>:</p>

<div class="language-python highlighter-rouge"><pre class="highlight"><code><span class="k">def</span> <span class="nf">covar</span><span class="p">(</span><span class="n">_x</span><span class="p">,</span> <span class="n">_y</span><span class="p">):</span>

    <span class="n">x</span> <span class="o">=</span> <span class="n">_x</span><span class="o">.</span><span class="n">flatten</span><span class="p">()</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">_y</span><span class="o">.</span><span class="n">flatten</span><span class="p">()</span>

    <span class="k">if</span> <span class="n">x</span><span class="o">.</span><span class="n">size</span> <span class="o">!=</span> <span class="n">y</span><span class="o">.</span><span class="n">size</span><span class="p">:</span>
        <span class="k">raise</span> <span class="nb">Exception</span><span class="p">(</span><span class="s">"x and y should have the same size"</span><span class="p">)</span>

    <span class="n">mean_x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">mean_y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>

    <span class="n">N</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">size</span>

    <span class="k">return</span> <span class="p">(</span><span class="mi">1</span> <span class="o">/</span> <span class="n">N</span><span class="p">)</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="nb">sum</span><span class="p">((</span><span class="n">x</span> <span class="o">-</span> <span class="n">mean_x</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span><span class="n">y</span> <span class="o">-</span> <span class="n">mean_y</span><span class="p">))</span>

<span class="k">def</span> <span class="nf">corr_mtx</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="n">covar</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">var</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">var</span><span class="p">(</span><span class="n">y</span><span class="p">))</span> <span class="k">for</span> <span class="n">y</span> <span class="ow">in</span> <span class="n">Y</span><span class="p">]</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">X</span><span class="p">])</span>
                      
<span class="k">def</span> <span class="nf">covar_mtx</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="n">covar</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span> <span class="k">for</span> <span class="n">y</span> <span class="ow">in</span> <span class="n">Y</span><span class="p">]</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">X</span><span class="p">])</span>

<span class="c"># high Corr above / below diagonal =&gt; the two factors are highly correlated =&gt; we should switch to PCA</span>
<span class="c"># in our case, corr above diagonal is 0.83</span>
<span class="c"># this can be seen also from plotting a vs b</span>
<span class="n">Corr_AA</span> <span class="o">=</span> <span class="n">corr_mtx</span><span class="p">(</span><span class="n">A</span><span class="p">,</span> <span class="n">A</span><span class="p">)</span>
</code></pre>
</div>

<p>Eigenvalue decomposition is applied to the covariance matrix. So let’s do just that:</p>

<div class="language-python highlighter-rouge"><pre class="highlight"><code><span class="c"># eigen value decomposition is applied on the covariance matrix</span>
<span class="n">Covar_AA</span> <span class="o">=</span> <span class="n">covar_mtx</span><span class="p">(</span><span class="n">A</span><span class="p">,</span> <span class="n">A</span><span class="p">)</span>

<span class="c"># lmbdas contain the eigenvalues and v the normalized eigenvectors</span>
<span class="n">lmbdas</span><span class="p">,</span> <span class="n">v</span> <span class="o">=</span> <span class="n">eigen_decomposition</span><span class="p">(</span><span class="n">Covar_AA</span><span class="p">)</span>
</code></pre>
</div>

<p>We can quickly check that the two <code class="highlighter-rouge">v's</code> are perpedicular to each other, by doing</p>

<div class="highlighter-rouge"><pre class="highlight"><code>np.dot(v[0], v[1])
Out[209]: -9.731923600320158e-11
</code></pre>
</div>

<p>We call the following function which computes the principal factors (vector <code class="highlighter-rouge">F</code> described in the <em>PCA</em> section) based on the initial matrix <code class="highlighter-rouge">A</code> and the eigenvalues vector <code class="highlighter-rouge">v</code>.</p>

<div class="language-python highlighter-rouge"><pre class="highlight"><code><span class="k">def</span> <span class="nf">factors</span><span class="p">(</span><span class="n">A</span><span class="p">,</span> <span class="n">vectors</span><span class="p">):</span>
    <span class="c"># rough check that all vectors are normalized</span>
    <span class="k">for</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">vectors</span><span class="p">:</span>
        <span class="k">assert</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="nb">abs</span><span class="p">(</span><span class="n">vector_norm</span><span class="p">(</span><span class="n">v</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">&lt;</span> <span class="mf">1e-5</span><span class="p">)</span>

    <span class="n">Fs</span> <span class="o">=</span> <span class="p">[[</span><span class="n">np</span><span class="o">.</span><span class="nb">sum</span><span class="p">([</span><span class="n">A</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">*</span> <span class="n">v</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">v</span><span class="p">))],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)]</span> <span class="k">for</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">vectors</span><span class="p">]</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">Fs</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">A</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
</code></pre>
</div>

<p>and then</p>

<div class="language-python highlighter-rouge"><pre class="highlight"><code><span class="n">Fs</span> <span class="o">=</span> <span class="n">factors</span><span class="p">(</span><span class="n">A</span><span class="p">,</span> <span class="n">v</span><span class="p">)</span>
</code></pre>
</div>

<p>Now we can quickly check that, indeed, <code class="highlighter-rouge">Var(Fs[i]) == lmdas[i]</code>:</p>

<div class="language-python highlighter-rouge"><pre class="highlight"><code><span class="n">np</span><span class="o">.</span><span class="n">var</span><span class="p">(</span><span class="n">Fs</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">Out</span><span class="p">[</span><span class="mi">226</span><span class="p">]:</span> <span class="n">array</span><span class="p">([</span><span class="mf">10333.05692141</span><span class="p">,</span>   <span class="mf">199.78928639</span><span class="p">])</span>

<span class="n">lmbdas</span>
<span class="n">Out</span><span class="p">[</span><span class="mi">227</span><span class="p">]:</span> <span class="p">[</span><span class="mf">10333.056921412048</span><span class="p">,</span> <span class="mf">199.78928639278513</span><span class="p">]</span>
</code></pre>
</div>

<p>This also shows that the fist factor contributes <code class="highlighter-rouge">98%</code> to the total variance, so we can use it alone in regression.</p>

<h3 id="final-regression-and-check">Final regression and check</h3>

<p>We will simply plot <code class="highlighter-rouge">Fs[0]</code> vs <code class="highlighter-rouge">Y</code> and <code class="highlighter-rouge">Fs[1]</code> vs <code class="highlighter-rouge">Y</code>. Obviously, much more from the variation is explained by <code class="highlighter-rouge">Fs[0]</code> than <code class="highlighter-rouge">Fs[1]</code>:</p>

<p><img src="https://alexandrugris.github.io/assets/pca_1.png" alt="Analysis" /></p>

<h3 id="additional-notes">Additional notes:</h3>

<ol>
  <li>
    <p>When to use the correlation and when to use the covariance matrix: <a href="https://stats.stackexchange.com/questions/53/pca-on-correlation-or-covariance">https://stats.stackexchange.com</a></p>
  </li>
  <li>
    <p>Why use covariance (or correlation at all) - because we are interested in finding the axes which describe the highest variation of data:</p>
  </li>
</ol>

<blockquote>
  <p>Finding the eigenvectors and eigenvalues of the covariance matrix is the equivalent of fitting those straight, principal-component lines to the variance of the data. Why? Because eigenvectors trace the principal lines of force, and the axes of greatest variance and covariance illustrate where the data is most susceptible to change.</p>
</blockquote>

<p>And then</p>

<blockquote>
  <p>Because the eigenvectors of the covariance matrix are orthogonal to each other, they can be used to reorient the data from the x and y axes to the axes represented by the principal components. You re-base the coordinate system for the dataset in a new space defined by its lines of greatest variance.</p>
</blockquote>

<p><a href="https://skymind.ai/wiki/eigenvector">PCA</a></p>

<p>So basically, just like a space transform in a 3D space, by multiplying the observed data with the coordinate system which describes best its covariance / correlation, we bring that data into the coordinate space where each axis represent, in decreasing order, its highest variation. Matrix multiplication is simply a linear (coordinate space) transformation.</p>

  </div>

  
</article>

      </div>
    </main>

    <footer class="site-footer">

  <div class="wrapper">

    <h2 class="footer-heading">From The Trenches - The Code</h2>

    <div class="footer-col-wrapper">
      <div class="footer-col footer-col-1">
        <ul class="contact-list">
          <li>
            
              From The Trenches - The Code
            
            </li>
            
            <li><a href="mailto:alexandru.gris2006@gmail.com">alexandru.gris2006@gmail.com</a></li>
            
        </ul>
      </div>

      <div class="footer-col footer-col-2">
        <ul class="social-media-list">
          
          <li>
            <a href="https://github.com/alexandrugris"><span class="icon icon--github"><svg viewBox="0 0 16 16" width="16px" height="16px"><path fill="#828282" d="M7.999,0.431c-4.285,0-7.76,3.474-7.76,7.761 c0,3.428,2.223,6.337,5.307,7.363c0.388,0.071,0.53-0.168,0.53-0.374c0-0.184-0.007-0.672-0.01-1.32 c-2.159,0.469-2.614-1.04-2.614-1.04c-0.353-0.896-0.862-1.135-0.862-1.135c-0.705-0.481,0.053-0.472,0.053-0.472 c0.779,0.055,1.189,0.8,1.189,0.8c0.692,1.186,1.816,0.843,2.258,0.645c0.071-0.502,0.271-0.843,0.493-1.037 C4.86,11.425,3.049,10.76,3.049,7.786c0-0.847,0.302-1.54,0.799-2.082C3.768,5.507,3.501,4.718,3.924,3.65 c0,0,0.652-0.209,2.134,0.796C6.677,4.273,7.34,4.187,8,4.184c0.659,0.003,1.323,0.089,1.943,0.261 c1.482-1.004,2.132-0.796,2.132-0.796c0.423,1.068,0.157,1.857,0.077,2.054c0.497,0.542,0.798,1.235,0.798,2.082 c0,2.981-1.814,3.637-3.543,3.829c0.279,0.24,0.527,0.713,0.527,1.437c0,1.037-0.01,1.874-0.01,2.129 c0,0.208,0.14,0.449,0.534,0.373c3.081-1.028,5.302-3.935,5.302-7.362C15.76,3.906,12.285,0.431,7.999,0.431z"/></svg>
</span><span class="username">alexandrugris</span></a>

          </li>
          

          
          <li>
            <a href="https://twitter.com/alexandrugris"><span class="icon icon--twitter"><svg viewBox="0 0 16 16" width="16px" height="16px"><path fill="#828282" d="M15.969,3.058c-0.586,0.26-1.217,0.436-1.878,0.515c0.675-0.405,1.194-1.045,1.438-1.809c-0.632,0.375-1.332,0.647-2.076,0.793c-0.596-0.636-1.446-1.033-2.387-1.033c-1.806,0-3.27,1.464-3.27,3.27 c0,0.256,0.029,0.506,0.085,0.745C5.163,5.404,2.753,4.102,1.14,2.124C0.859,2.607,0.698,3.168,0.698,3.767 c0,1.134,0.577,2.135,1.455,2.722C1.616,6.472,1.112,6.325,0.671,6.08c0,0.014,0,0.027,0,0.041c0,1.584,1.127,2.906,2.623,3.206 C3.02,9.402,2.731,9.442,2.433,9.442c-0.211,0-0.416-0.021-0.615-0.059c0.416,1.299,1.624,2.245,3.055,2.271 c-1.119,0.877-2.529,1.4-4.061,1.4c-0.264,0-0.524-0.015-0.78-0.046c1.447,0.928,3.166,1.469,5.013,1.469 c6.015,0,9.304-4.983,9.304-9.304c0-0.142-0.003-0.283-0.009-0.423C14.976,4.29,15.531,3.714,15.969,3.058z"/></svg>
</span><span class="username">alexandrugris</span></a>

          </li>
          
        </ul>
      </div>

      <div class="footer-col footer-col-3">
        <p>Alexandru Gris - Personal Blog
</p>
      </div>
    </div>

  </div>

</footer>


  </body>

</html>
