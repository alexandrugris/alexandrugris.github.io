<!DOCTYPE html>
<html lang="en">

  <head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <title>Stats Again</title>
  <meta name="description" content="A summary of statistics notions not found anywhere else on this blog. The post touches among others: some descriptive statistics measures, hypothesis testing...">

  <link rel="stylesheet" href="/assets/main.css">
  <link rel="canonical" href="https://alexandrugris.github.io/statistics/2019/12/24/stats-again.html">
  <link rel="alternate" type="application/rss+xml" title="From The Trenches - The Code" href="/feed.xml">
  
  
</head>


  <body>

    <header class="site-header" role="banner">

  <div class="wrapper">

    <div class="site-nav"><a class="site-title" href="/">From The Trenches - The Code</a> </div>

    <nav class="site-nav">
      <span class="menu-icon">        
      </span>

      <div class="trigger">

        <a class="page-link" href="https://alexandrugris.github.io">Home</a>

        
          
          <a class="page-link" href="/about/">About</a>
          
        
          
        
          
        
          
          <a class="page-link" href="/sm/">Social Media</a>
          
        
          
        
      </div>
    </nav>

  </div>

</header>


    <main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title" itemprop="name headline">Stats Again</h1>
    <p class="post-meta"><time datetime="2019-12-24T13:15:16+02:00" itemprop="datePublished">Dec 24, 2019</time></p>
  </header>

  <div class="post-content" itemprop="articleBody">
    <p>A summary of statistics notions not found anywhere else on this blog. The post touches among others: some descriptive statistics measures, hypothesis testing, goodness of fit, Lasso and Ridge regression.</p>

<h3 id="descriptive-statistics">Descriptive Statistics</h3>

<p>For unimodal distributions, we define skewness as a measure of whether the distribution has its its peak towards the left or the right.</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Skewness = 1/n * sum( (xi - x_mean) / stdev)^3
</code></pre></div></div>

<p>A negative value means the sample has its mode to the left, a positive, to the right. Left-skewed means the tail is on the left and the mode is on the right. Right-skewed, the opposite.</p>

<p>The following inequalities stand:</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>mode &lt; mean &lt; median if the distribution is right skewed and
mean &lt; median &lt; mode if the distribution is left skewed
</code></pre></div></div>

<p>For bivariate datasets, <code class="highlighter-rouge">X</code> and <code class="highlighter-rouge">Y</code>, we define correlation and covariance as measures of how <code class="highlighter-rouge">X</code> and <code class="highlighter-rouge">Y</code> move together monotonically.</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>sample_covariance = 1/(n-1) * sum((xi - x_mean) (yi-y_mean))
</code></pre></div></div>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>sample_correlation = r = sample_covariance / (x_stddev * y_stddev)
</code></pre></div></div>

<p>The sample correlation, <code class="highlighter-rouge">r</code>, is also called Pearson’s correlation coefficient.</p>

<p>A perfect correlation, <code class="highlighter-rouge">1</code> or <code class="highlighter-rouge">-1</code>, means that <code class="highlighter-rouge">X</code> and <code class="highlighter-rouge">Y</code> move together on a straight line. We can derive the regression line between <code class="highlighter-rouge">X</code> and <code class="highlighter-rouge">Y</code> as follows:</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Y = aX + b
a = covar(X, Y) / Var(X)
b = mean(Y) - a * mean(X) # the regression line always passes through the means of X and Y
</code></pre></div></div>

<p>For the regression, we define:</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>r_squared = r^2 = 1 - Var(residuals) / Var(Y)
</code></pre></div></div>

<p>For linear least squares regression with an intercept term and a single explanatory, this <code class="highlighter-rouge">r</code> is the same <code class="highlighter-rouge">r</code> as the Pearson’s correlation coefficient defined above. It measures the amount of explained variance of <code class="highlighter-rouge">Y</code> for this particular regression.</p>

<p>Similarly to the Pearson coefficient which is defined for the values of <code class="highlighter-rouge">X</code> and <code class="highlighter-rouge">Y</code>, we define the Spearman correlation coefficient, but instead of values we take the ranks. Spearman is more robust, thus less sensitive to outliers.</p>

<p><em>Chi2 Analysis</em></p>

<p>For nominal (categorical) values, we can introduce a measure of dependency and correlation. We use the <code class="highlighter-rouge">Chi2 statistic</code> to measure the independence of two categorical variables. The <em>Chi2 test</em> works on <code class="highlighter-rouge">r rows x c columns</code> contingency tables and assesses wether the null hypothesis of independence among variables is reasonable.</p>

<p>An example for its use would be to test how well different headlines for a website generate more clicks. Let’s assume an experiment with N headlines which generated either a click or a no-click. A contingency table to describe the problem would contain on the column axes the N headlines and on the rows axes click or no-click, with the counts for each in the table.</p>

<p>The table and the calculations below come from the <code class="highlighter-rouge">Practical Statistics for Data Scientists</code> book:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="n">pd</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span>

<span class="c"># generate some data:</span>
<span class="n">headings</span> <span class="o">=</span> <span class="p">[</span><span class="s">'H 1'</span><span class="p">]</span> <span class="o">*</span> <span class="p">(</span><span class="mi">14</span> <span class="o">+</span> <span class="mi">986</span><span class="p">)</span> <span class="o">+</span> <span class="p">[</span><span class="s">'H 2'</span><span class="p">]</span> <span class="o">*</span> <span class="p">(</span><span class="mi">8</span><span class="o">+</span><span class="mi">992</span><span class="p">)</span> <span class="o">+</span> <span class="p">[</span><span class="s">'H 3'</span><span class="p">]</span> <span class="o">*</span> <span class="p">(</span><span class="mi">12</span> <span class="o">+</span> <span class="mi">988</span><span class="p">)</span>
<span class="n">clicks_no_clicks</span> <span class="o">=</span> <span class="p">[</span><span class="s">'C'</span><span class="p">]</span> <span class="o">*</span> <span class="mi">14</span> <span class="o">+</span> <span class="p">[</span><span class="s">'NC'</span><span class="p">]</span> <span class="o">*</span> <span class="mi">986</span> <span class="o">+</span> <span class="p">[</span><span class="s">'C'</span><span class="p">]</span> <span class="o">*</span> <span class="mi">8</span> <span class="o">+</span> <span class="p">[</span><span class="s">'NC'</span><span class="p">]</span> <span class="o">*</span> <span class="mi">992</span> <span class="o">+</span> <span class="p">[</span><span class="s">'C'</span><span class="p">]</span> <span class="o">*</span> <span class="mi">12</span> <span class="o">+</span> <span class="p">[</span><span class="s">'NC'</span><span class="p">]</span> <span class="o">*</span> <span class="mi">988</span>

<span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">()</span>

<span class="n">df</span><span class="p">[</span><span class="s">'Headings'</span><span class="p">]</span> <span class="o">=</span> <span class="n">headings</span>
<span class="n">df</span><span class="p">[</span><span class="s">'Clicks'</span><span class="p">]</span> <span class="o">=</span> <span class="n">clicks_no_clicks</span>

<span class="n">contingency</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">crosstab</span><span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="s">'Headings'</span><span class="p">],</span> <span class="n">df</span><span class="p">[</span><span class="s">'Clicks'</span><span class="p">])</span><span class="o">.</span><span class="n">T</span>

<span class="c"># Expected values given the null hypothesis, </span>
<span class="c"># that all are independent trials drawn from the same distribution</span>
<span class="n">expected</span> <span class="o">=</span> <span class="n">contingency</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">values</span><span class="o">.</span><span class="n">repeat</span><span class="p">(</span><span class="mi">3</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">((</span><span class="mi">2</span><span class="p">,</span><span class="mi">3</span><span class="p">))</span>

<span class="c"># Pearson's residuals</span>
<span class="n">R</span> <span class="o">=</span> <span class="p">(</span><span class="n">contingency</span><span class="o">.</span><span class="n">values</span> <span class="o">-</span> <span class="n">expected</span><span class="p">)</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">expected</span><span class="p">)</span>

<span class="n">R</span> <span class="o">=</span> <span class="n">R</span><span class="o">.</span><span class="n">flatten</span><span class="p">()</span>
<span class="n">chi_stat</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">R</span><span class="p">,</span> <span class="n">R</span><span class="p">)</span>

<span class="kn">from</span> <span class="nn">scipy.stats</span> <span class="kn">import</span> <span class="n">chi2</span>
<span class="n">p_value</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="n">chi2</span><span class="o">.</span><span class="n">cdf</span><span class="p">(</span><span class="n">chi_stat</span><span class="p">,</span> <span class="n">df</span><span class="o">=</span><span class="p">(</span><span class="mi">3</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span><span class="mi">2</span><span class="o">-</span><span class="mi">1</span><span class="p">)))</span>
</code></pre></div></div>

<p>Or, more straightforward, same results:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">scipy.stats</span> <span class="kn">import</span> <span class="n">chi2_contingency</span>

<span class="c"># first create the contingency table based on the two categorical variables</span>
<span class="n">df_for_test</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">crosstab</span><span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="s">'Headings'</span><span class="p">],</span> <span class="n">df</span><span class="p">[</span><span class="s">'Clicks'</span><span class="p">])</span><span class="o">.</span><span class="n">T</span>
<span class="n">chi2</span><span class="p">,</span> <span class="n">p_value</span><span class="p">,</span> <span class="n">degrees_of_freedom</span><span class="p">,</span> <span class="n">expected_values</span> <span class="o">=</span> <span class="n">chi2_contingency</span><span class="p">(</span><span class="n">df_for_test</span><span class="o">.</span><span class="n">values</span><span class="p">)</span>
</code></pre></div></div>

<p><img src="https://alexandrugris.github.io/assets/stats_2.png" alt="Chi2" /></p>

<p>The return values are described below:</p>
<ul>
  <li><code class="highlighter-rouge">expected_values</code> - how would the data look like if it were independent</li>
  <li><code class="highlighter-rouge">chi2</code> - the test statistic, the higher it is, the lower the <code class="highlighter-rouge">p-value</code>, the probability that the values are independent</li>
  <li><code class="highlighter-rouge">degrees_of_freedom</code> - <code class="highlighter-rouge">dof = observed.size - sum(observed.shape) + observed.ndim - 1</code> the degrees of freedom for the <code class="highlighter-rouge">Chi2 distribution</code> from which the <code class="highlighter-rouge">p-value</code> is computed given the test statistic.</li>
</ul>

<p><em>Note:</em></p>

<p>The p-values can be computed through Monte Carlo simulation, by simply sampling <code class="highlighter-rouge">N</code> times <code class="highlighter-rouge">M</code> samples with the same distribution as the expected values and counting how many times the squared sum of Pearon’s residuals (<code class="highlighter-rouge">R</code>) exceed the value obtained from our sample. That is the <code class="highlighter-rouge">p-value</code>.</p>

<h3 id="binomial-and-hypergeometric-distributions">Binomial and Hypergeometric Distributions</h3>

<p>The binomial distribution is frequently used to model the number of successes in a sample of size <code class="highlighter-rouge">n</code> drawn with replacement from a population of size <code class="highlighter-rouge">N</code>. If the sampling is carried out without replacement, the draws are not independent and so the resulting distribution is a hypergeometric distribution, not a binomial one. However, for <code class="highlighter-rouge">N</code> much larger than <code class="highlighter-rouge">n</code>, a rule of thumb is <code class="highlighter-rouge">n</code> is less than <code class="highlighter-rouge">10%</code> of <code class="highlighter-rouge">N</code>, the binomial distribution remains a good approximation.</p>

<p>If <code class="highlighter-rouge">n</code> is large enough, then the skew of the distribution of the expected value is not too large. In this case, a reasonable approximation to <code class="highlighter-rouge">B(n, p)</code> is given by the normal distribution <code class="highlighter-rouge">N(n*p, n*p*(1-p))</code>. A commonly used rule is that both <code class="highlighter-rouge">n*p</code> and <code class="highlighter-rouge">n*(1-p)</code> are larger than <code class="highlighter-rouge">10</code>.</p>

<p>The binomial distribution converges towards the Poisson distribution as the number of trials goes to infinity while the product <code class="highlighter-rouge">n*p</code> remains fixed or at least p tends to zero. Therefore, the Poisson distribution with parameter <code class="highlighter-rouge">λ = n*p</code> can be used as an approximation to <code class="highlighter-rouge">B(n, p)</code> of the binomial distribution if <code class="highlighter-rouge">n</code> is sufficiently large and <code class="highlighter-rouge">p</code> is sufficiently small. According to two rules of thumb, this approximation is good if <code class="highlighter-rouge">n &gt;= 20</code> and <code class="highlighter-rouge">p &lt;= 0.05</code>, or if <code class="highlighter-rouge">n &gt;= 100</code> and <code class="highlighter-rouge">n*p &lt;= 10</code>.</p>

<p>The hypergeometric distribution is not to be confused with the geometric distribution, the latter giving the probability that the first occurrence of success requires <code class="highlighter-rouge">k</code> independent trials, each with success probability <code class="highlighter-rouge">p</code>.</p>

<h3 id="inferential-statistics">Inferential Statistics</h3>

<p><em>Maximum Likelihood Estimate</em></p>

<p>MLE starts from the premise that if you observe a certain sample, then its probability must be high. It is used to estimate a parameter of the distribution of the population given the observations.</p>

<p>The MLE problem is described as follows:</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>maximize(P(x1 | distrib(parameter)) * P(x2 | distrib(parameter) * .... * P(xn | distrib(parameter))))
</code></pre></div></div>

<p>which is equivalent to</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>maximize(log(P(x1 | ...)) + log(P(x2 |...)) + .... )
</code></pre></div></div>

<p>Since MLE usually uses gradient descent to find the maximum, it helps if the distribution for which we plan to estimate the parameter is continuous. Otherwise, the maximum must be found through different means.</p>

<p>As an example, let us extract <code class="highlighter-rouge">1000</code> samples from a <code class="highlighter-rouge">t</code> distribution with 5 degrees of freedom. We want to estimate the degrees of freedom knowing that the samples come from a <code class="highlighter-rouge">t</code> distribution. This problem of estimating a parameter of the source distribution is a perfect example of when to employ the MLE method.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span> 
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">import</span> <span class="nn">scipy.stats</span> <span class="k">as</span> <span class="n">stats</span>
<span class="kn">from</span> <span class="nn">scipy.optimize</span> <span class="kn">import</span> <span class="n">minimize</span>
<span class="kn">from</span> <span class="nn">functools</span> <span class="kn">import</span> <span class="n">partial</span>

<span class="c"># the samples we extract</span>
<span class="n">sample</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">standard_t</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">1000</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">hist</span><span class="p">(</span><span class="n">sample</span><span class="p">)</span>

<span class="c"># problem solved below</span>
<span class="k">def</span> <span class="nf">log_likelihood</span><span class="p">(</span><span class="n">f</span><span class="p">,</span> <span class="n">sample</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">float</span><span class="p">:</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="nb">sum</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">f</span><span class="p">(</span><span class="n">sample</span><span class="p">)))</span>

<span class="k">def</span> <span class="nf">t_distrib</span><span class="p">(</span><span class="n">df</span> <span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">sample</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">:</span>
    <span class="k">return</span> <span class="n">stats</span><span class="o">.</span><span class="n">t</span><span class="o">.</span><span class="n">pdf</span><span class="p">(</span><span class="n">sample</span><span class="p">,</span> <span class="n">df</span><span class="o">=</span><span class="n">df</span><span class="p">)</span>

<span class="n">ret</span> <span class="o">=</span> <span class="n">minimize</span><span class="p">(</span><span class="k">lambda</span> <span class="n">df</span><span class="p">:</span> <span class="o">-</span><span class="n">log_likelihood</span><span class="p">(</span><span class="n">partial</span><span class="p">(</span><span class="n">t_distrib</span><span class="p">,</span> <span class="n">df</span><span class="p">),</span> <span class="n">sample</span><span class="p">),</span> <span class="mf">10.0</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">ret</span><span class="o">.</span><span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
</code></pre></div></div>

<p>A good estimator has low variance and low bias, both preferably 0. Bias is the difference between the true value of the parameter and the expected value of its estimator.</p>

<p><em>Hypothesis Testing</em></p>

<p>We define two hypotheses:</p>

<ul>
  <li><code class="highlighter-rouge">H0</code>, the null hypothesis, is considered true until proven false. It is usually in the form ‘there is no relationship’ or ‘there is no effect’.</li>
  <li><code class="highlighter-rouge">H1</code>, the alternative hypothesis, asserts that there is a relationship or a significant effect.</li>
</ul>

<p>Hypothesis testing does not aim to prove <code class="highlighter-rouge">H1</code>, but rather to say that there is a low probability,usually under 5%, that <code class="highlighter-rouge">H0</code> can occur. There is still thus a possibility that the results of the test occur under <code class="highlighter-rouge">H0</code>, but it is considered low. The <code class="highlighter-rouge">p-values, which are used in hypothesis testing, represent the probability that we observe </code>H1<code class="highlighter-rouge"> given that </code>H0<code class="highlighter-rouge"> is true. The </code>5%` above is called significance level and its setting depends on the application.</p>

<p>For example, let’s assume that someone can tell if <a href="https://en.wikipedia.org/wiki/Lady_tasting_tea">the milk was poured before or after the tea in a cup</a>. There are 8 cups, 4 with milk poured before and 4 with milk poured after. Assuming the person manages to correctly identify which one is which in this experiment, can we conclude that the person is able to tell whether the milk was poured before or after?</p>

<p>Let’s define the two hypotheses:</p>

<ul>
  <li><code class="highlighter-rouge">H0</code>: correct identification in this experiment is purely due to chance</li>
  <li><code class="highlighter-rouge">H1</code>: the person is able to identify which is which</li>
</ul>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>p_observed_results_given_h0 = 1 / Combinations of 8 taken by 4 = 1/70 &lt; 5% 
</code></pre></div></div>

<p>We can conclude that it is unlikely that the person made the choices purely by chance.</p>

<p><em>T-Tests</em></p>

<p>T-tests are is used to learn about averages across two categories, for instance the height of males vs the height of females in a sample. It has several forms:</p>

<ul>
  <li><em>One sample location test</em> - used to test whether a population mean is significantly different from some hypothesized value. The degrees of freedom is <code class="highlighter-rouge">DF = n - 1</code> where <code class="highlighter-rouge">n</code> is the number of observations in the sample.</li>
</ul>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>t = (sample_mean - hypothesized value) / Standard Error 
t = sqrt(n) * (sample_mean - hypothesized value) / (sample_std_dev)
</code></pre></div></div>

<p>In python we use:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">scipy</span><span class="o">.</span><span class="n">stats</span><span class="o">.</span><span class="n">ttest_1samp</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">popmean</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">nan_policy</span><span class="o">=</span><span class="s">'propagate'</span><span class="p">)</span>
</code></pre></div></div>

<p>If the sample size is large (e.g. &gt; 30 observations) and the population standard deviation is known, you can assume the test statistics follows the normal distribution instead of the t-distribution with n-1 degrees of freedom, thus one can apply the Z-test.</p>

<ul>
  <li><em>Two sample location test</em> - used to test whether the population means of two samples are significantly different. E.g. mean of the the height of adults from town A is different from the mean of heights of adults from town B. The sample size should be the same and the variance in each sample should be equal. If the variances differ, one needs to apply the <em>Welch t-test</em>. To test for the equality of variances between the two populations, one can use the <em>Levene</em> test. For the <em>Levene</em> test, the null hypothesis is that the sample variances are equal, so we can use the two sample location test if we cannot discard the null hypothesis, that is the p-values from the <em>Levene</em> tests are higher than 5%.</li>
</ul>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># we assume we have the two samples, sample1 and sample2 already extracted</span>
<span class="c"># sample1 and sample2 are of the same type, same unit of measure, same scale</span>

<span class="kn">from</span> <span class="nn">sklearn.preprocessing</span> <span class="kn">import</span> <span class="n">scale</span> <span class="c"># for z-scoring</span>

<span class="n">stats</span><span class="o">.</span><span class="n">levene</span><span class="p">(</span><span class="n">sample1</span><span class="p">,</span> <span class="n">sample2</span><span class="p">)</span> <span class="c"># here we need to accept the null, which means p&gt;0.05</span>

<span class="n">diff</span> <span class="o">=</span> <span class="n">scale</span><span class="p">(</span><span class="n">sample1</span> <span class="o">-</span> <span class="n">sample2</span><span class="p">)</span> <span class="c"># this diff should be normally distributed</span>
<span class="n">plt</span><span class="o">.</span><span class="n">hist</span><span class="p">(</span><span class="n">diff</span><span class="p">)</span>
<span class="n">stats</span><span class="o">.</span><span class="n">probplot</span><span class="p">(</span><span class="n">diff</span><span class="p">,</span> <span class="n">plot</span><span class="o">=</span><span class="n">plt</span><span class="p">,</span> <span class="n">dist</span><span class="o">=</span><span class="s">'norm'</span><span class="p">)</span> <span class="c"># check the QQ plot</span>
<span class="n">stats</span><span class="o">.</span><span class="n">shapiro</span><span class="p">(</span><span class="n">diff</span><span class="p">)</span> <span class="c">#test for normality, if the test statistic is not significant, p&gt;0.05, then the population is normally distributed</span>

<span class="n">stats</span><span class="o">.</span><span class="n">ttest_ind</span><span class="p">(</span><span class="n">sample1</span><span class="p">,</span> <span class="n">sample2</span><span class="p">)</span> <span class="c"># perform the test</span>
</code></pre></div></div>

<p>If the population variances are not equal, we need to use:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">stats</span><span class="o">.</span><span class="n">ttest_ind</span><span class="p">(</span><span class="n">sample1</span><span class="p">,</span> <span class="n">sample2</span><span class="p">,</span> <span class="n">equal_var</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span> <span class="c"># Welch t-test</span>
</code></pre></div></div>

<ul>
  <li><em>Paired difference test</em> - the previous two t-tests assume the variables are independent. If they are not independent, for instance taking babies from the same town or the same sample before and after treatment to check if the treatment was successful, one needs to use the paired difference test.</li>
</ul>

<p>Taking the same assumptions as before, in python use:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">stats</span><span class="o">.</span><span class="n">ttest_rel</span><span class="p">(</span><span class="n">sample1</span><span class="p">,</span> <span class="n">sample2</span><span class="p">)</span>
</code></pre></div></div>

<ul>
  <li><em>Regression coefficient tests</em> - tests whether the coefficients from a regression are significantly different from 0.</li>
</ul>

<p>Assumptions of the t-test:</p>
<ul>
  <li>Populations are normal</li>
  <li>Samples are representative</li>
  <li>Samples randomly drawn</li>
</ul>

<p>Below is an example of how to compute the t-statistic manually in case of the regression analysis. We consider an example of a multiple regression, with the predictor variables in <code class="highlighter-rouge">X</code> and dependent variable in <code class="highlighter-rouge">y</code>.</p>

<p>Given <code class="highlighter-rouge">c_i</code> as the coefficients of a regression model,  <code class="highlighter-rouge">t-statistic(c_i) = c_i / SE(c_i)</code> and 
<code class="highlighter-rouge">SE(c_i) = sqrt(residuals_sigma^2 * diagonal((X.T * X)^-1)).</code> <code class="highlighter-rouge">SE(c_i)</code> is the  standard error of coefficient <code class="highlighter-rouge">c_i</code>.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">statsmodels.api</span> <span class="k">as</span> <span class="n">sm</span>

<span class="c"># add the intercept</span>
<span class="n">X_</span> <span class="o">=</span> <span class="n">sm</span><span class="o">.</span><span class="n">add_constant</span><span class="p">(</span><span class="n">X</span><span class="p">,</span><span class="n">prepend</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">sm</span><span class="o">.</span><span class="n">OLS</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">X_</span><span class="p">)</span>
<span class="n">results</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">()</span>
<span class="k">print</span><span class="p">(</span><span class="n">results</span><span class="o">.</span><span class="n">summary</span><span class="p">())</span>

<span class="s">"""
We will consider to compute the t-test for the coefficient of `income`
H0: income has no predictive power on the outcome of the regression
"""</span>

<span class="n">X_arr</span> <span class="o">=</span> <span class="n">X_</span><span class="o">.</span><span class="n">to_numpy</span><span class="p">()</span>

<span class="n">SE_arr</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">results</span><span class="o">.</span><span class="n">resid</span><span class="o">.</span><span class="n">var</span><span class="p">()</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">inv</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">X_arr</span><span class="o">.</span><span class="n">T</span><span class="p">,</span> <span class="n">X_arr</span><span class="p">))</span><span class="o">.</span><span class="n">diagonal</span><span class="p">())</span>
<span class="n">SE</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">SE_arr</span><span class="p">,</span> <span class="n">index</span><span class="o">=</span><span class="n">X_</span><span class="o">.</span><span class="n">columns</span><span class="p">)</span><span class="o">.</span><span class="n">T</span>

<span class="n">t_statistic</span> <span class="o">=</span> <span class="n">results</span><span class="o">.</span><span class="n">params</span><span class="p">[</span><span class="s">'income'</span><span class="p">]</span> <span class="o">/</span> <span class="n">SE</span><span class="p">[</span><span class="s">'income'</span><span class="p">]</span>
</code></pre></div></div>

<p>T-tests work well for two group comparison, for multiple groups one needs to use <em>ANOVA</em>.</p>

<p><em>Test for correlation / dependence</em></p>

<p>If sample (X, Y) come from a 2 dimensional normal distribution, <code class="highlighter-rouge">Corr(X, Y) == 0</code> means independence.</p>

<p><em>Kolmogorov-Smirnov Goodness of Fit test</em></p>

<p>Tells us if the samples come from a specified distribution. In python, we can use <code class="highlighter-rouge">stats.kstest</code>.
For instance, assuming the variable <code class="highlighter-rouge">samples</code> contains an array drawn from an unknown distribution,
we can test if the unknown distribution is normal using the following test:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">stats</span><span class="o">.</span><span class="n">kstest</span><span class="p">(</span><span class="n">samples</span><span class="p">,</span> <span class="s">'norm'</span><span class="p">)</span><span class="o">.</span><span class="n">pvalue</span> <span class="o">&lt;=</span> <span class="mf">0.05</span>
</code></pre></div></div>

<p><em>One-way ANOVA</em></p>

<p>Unlike <em>t-tests</em> which compare only two means, <em>ANOVA</em> looks at several groups within a population to produce one score and one significance value. A <em>t-test</em> will tell you if there is a significant variation between two groups. We use <em>ANOVA</em> when the population is split in more than two groups.</p>

<ul>
  <li><code class="highlighter-rouge">H0</code>: all groups have the same mean</li>
  <li><code class="highlighter-rouge">H1</code>: not all groups have the same mean</li>
</ul>

<p>The <code class="highlighter-rouge">F-statistic</code> in one-way ANOVA is a tool to help you answer the question “Is the variance between the means of two populations significantly different?”</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>F-statistic = Variance between groups / Variance within groups
</code></pre></div></div>

<p>Given <code class="highlighter-rouge">K</code> the number of groups, <code class="highlighter-rouge">N</code> the total number of samples in all groups, and <code class="highlighter-rouge">ni</code> the number of samples in each group, we have <code class="highlighter-rouge">N = sum(ni, i=1..k)</code></p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>variance_between_groups = sum((mean_group_i - mean_all)^2 ,i=1..K) / (N-K)
variance_within_groups = sum(sum((xj-mean_group_i)^2, j=1..ni ), i=1..K ) / (K-1)
</code></pre></div></div>

<p>For <em>one-way ANOVA</em>, a single categorical variable is used to split the population into these groups. <em>ANOVA</em> assumes the populations are normal. <em>One-way ANOVA</em> will tell you that at least two groups were different from each other, but it won’t tell you which groups were different.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">stats</span><span class="o">.</span><span class="n">f_oneway</span><span class="p">(</span><span class="n">sample1</span><span class="p">,</span> <span class="n">sample2</span><span class="p">,</span> <span class="n">sample3</span><span class="p">)</span>
</code></pre></div></div>

<p>To test which group is different we use  <em>Tukey Honest Significant Difference</em> (Tuckey HSD)</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">statsmodels.stats.multicomp</span> <span class="kn">import</span> <span class="n">MultiComparison</span>
<span class="n">mult_comp</span> <span class="o">=</span> <span class="n">MultiComparison</span><span class="p">(</span><span class="n">all_samples_column_df</span><span class="p">,</span> <span class="n">group_by_column_df</span><span class="p">)</span>
<span class="n">result</span> <span class="o">=</span> <span class="n">mult_comp</span><span class="o">.</span><span class="n">tukeyhsd</span><span class="p">()</span>
<span class="k">print</span><span class="p">(</span><span class="n">result</span><span class="p">)</span>
</code></pre></div></div>

<p>This will group by the <code class="highlighter-rouge">group_by_column_df</code> the data frame containing all the samples and will output the pairwise comparison. The <code class="highlighter-rouge">Reject</code> column will tell us whether the difference in the mean between the groups is statistically significant.</p>

<p><em>Two-way ANOVA</em></p>

<p>For <em>two-way ANOVA</em>, we split the population in classes based on two categorical variables (e.g. gender and age over 35). <em>Two-way ANOVA</em> brings 3 null hypotheses which are tested all at once:</p>

<ul>
  <li><code class="highlighter-rouge">H0</code>: The means of all gender groups are equal</li>
  <li>
    <p><code class="highlighter-rouge">H1</code>: The mean of at least one gender group is different</p>
  </li>
  <li><code class="highlighter-rouge">H0</code>: The means of the age groups are equal</li>
  <li>
    <p><code class="highlighter-rouge">H1</code>: The mean of at least one the age group is different</p>
  </li>
  <li><code class="highlighter-rouge">H0</code>: There is no interaction between the gender and age</li>
  <li><code class="highlighter-rouge">H1</code>: There is interaction between the gender and age</li>
</ul>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="n">pd</span>
<span class="kn">import</span> <span class="nn">statsmodels.api</span> <span class="k">as</span> <span class="n">sm</span>
<span class="kn">from</span> <span class="nn">statsmodels.formula.api</span> <span class="kn">import</span> <span class="n">ols</span>
<span class="kn">from</span> <span class="nn">statsmodels.stats.anova</span> <span class="kn">import</span> <span class="n">anova_lm</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span>

<span class="n">formula</span> <span class="o">=</span> <span class="s">'cnt ~ C(age) + C(gender) + C(age):C(gender)'</span>
<span class="c">#formula = 'cnt ~ C(age) * C(gender)'</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">ols</span><span class="p">(</span><span class="n">formula</span><span class="p">,</span> <span class="n">data</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">()</span>
<span class="n">results</span> <span class="o">=</span> <span class="n">anova_lm</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">typ</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
</code></pre></div></div>

<h3 id="regression-analysis-lasso-and-ridge">Regression analysis (Lasso and Ridge)</h3>

<p>Lasso and Ridge regression add a penalization term to the loss function (the objective function to be minimized by the regression algorithm). The difference in the two is that Lasso adds a sum of the absolute values of the regression coefficients while Ridge adds the sum of their squares. The parameter for the regression is called <code class="highlighter-rouge">lambda</code>.</p>

<ul>
  <li><em>Lasso regression:</em> <code class="highlighter-rouge">cost_function = sum((yi - sum(b_i*x_i) ** 2) + lambda * sum(abs(b_i))</code></li>
  <li><em>Ridge regression:</em> <code class="highlighter-rouge">cost_function = sum((yi - sum(b_i*x_i) ** 2) + lambda * sum(abs(b_i**2))</code></li>
</ul>

<p>The Lasso additional term is called <em>L1 regularization</em> while for the Ridge it is called <em>L2 regularization</em>. For a given regression, both L1 and L2 can be applied simultaneously, in a method called <a href="https://en.wikipedia.org/wiki/Elastic_net_regularization">ElasticNet</a>.</p>

<p>The idea for both is to shrink coefficients in order to minimize overfitting. Lasso regression allows for automatic removal of some features during the minimization process. This is not true for the Ridge regression which preserves all coefficients but in a shrunk form. Both aims to reduce the effort put in model selection and allow for more explanatory variables in the regression equation,
even in the case of multicollinearity.</p>

<p>Regularization puts constraints on the size of the coefficients associated with each predictor. The constraint will depend on the magnitude of each variable. It is therefore necessary to center and reduce, or standardize, the predictor variables. OneHot-encoded variables should be scaled so the penalization is fairly applied to all coefficients. However, you then lose the straightforward interpretability of your coefficients. If you don’t, your variables are not on an even playing field. You are essentially tipping the scales in favor of your continuous variables (most likely). So, if your primary goal is model selection then this is an egregious error. (From <a href="lasso regression and dummy variables">https://stats.stackexchange.com/questions/69568/whether-to-rescale-indicator-binary-dummy-predictors-for-lasso</a>)</p>

<p>An example below. In the first snippet we do data preparation - dummy variable encoding and standardization:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">data</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">get_dummies</span><span class="p">(</span><span class="n">original</span><span class="p">,</span> <span class="n">columns</span><span class="o">=</span><span class="p">[</span><span class="s">'marital'</span><span class="p">,</span> <span class="s">'ed'</span><span class="p">,</span> <span class="s">'retire'</span><span class="p">,</span> <span class="s">'gender'</span><span class="p">,</span> <span class="s">'churn'</span><span class="p">],</span> <span class="n">drop_first</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="n">data</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>


<span class="n">initial</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">copy</span><span class="p">(</span><span class="n">deep</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">zscore</span><span class="p">(</span><span class="n">s</span><span class="p">):</span>
    <span class="k">return</span> <span class="p">(</span><span class="n">initial</span><span class="p">[</span><span class="n">s</span><span class="p">]</span> <span class="o">-</span> <span class="n">initial</span><span class="p">[</span><span class="n">s</span><span class="p">]</span><span class="o">.</span><span class="n">mean</span><span class="p">())</span><span class="o">/</span><span class="n">initial</span><span class="p">[</span><span class="n">s</span><span class="p">]</span><span class="o">.</span><span class="n">std</span><span class="p">()</span> 

<span class="k">for</span> <span class="n">c</span> <span class="ow">in</span> <span class="n">data</span><span class="o">.</span><span class="n">columns</span><span class="p">:</span>
    <span class="n">data</span><span class="p">[</span><span class="n">c</span><span class="p">]</span> <span class="o">=</span> <span class="n">zscore</span><span class="p">(</span><span class="n">c</span><span class="p">)</span>
</code></pre></div></div>

<p>In the second snippet we do lambda parameter selection by running the Lasso regression with increasing lambda in a loop and then we plot the shinkage to 0 of each parameter to demonstrate the feature selection ability of the Lasso regression.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">Lasso</span>

<span class="c">#y = initial['tenure'] # not with standardization for dependent variable</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">data</span><span class="p">[</span><span class="s">'tenure'</span><span class="p">]</span> <span class="c"># standardized dependent variable</span>

<span class="n">cols</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">data</span><span class="o">.</span><span class="n">columns</span><span class="p">)</span>
<span class="n">cols</span><span class="o">.</span><span class="n">remove</span><span class="p">(</span><span class="s">'tenure'</span><span class="p">)</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">data</span><span class="p">[</span><span class="n">cols</span><span class="p">]</span>

<span class="n">coefs</span> <span class="o">=</span> <span class="p">{}</span>

<span class="k">for</span> <span class="n">lmbda</span> <span class="ow">in</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mf">0.001</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">step</span><span class="o">=</span><span class="mf">0.001</span><span class="p">):</span>
    <span class="n">l</span> <span class="o">=</span> <span class="n">Lasso</span><span class="p">(</span><span class="n">lmbda</span><span class="p">)</span>
    <span class="n">l</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
    <span class="n">coefs</span><span class="p">[</span><span class="n">lmbda</span><span class="p">]</span> <span class="o">=</span> <span class="n">l</span><span class="o">.</span><span class="n">coef_</span>
    
<span class="n">coefs</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="o">.</span><span class="n">from_dict</span><span class="p">(</span><span class="n">coefs</span><span class="p">)</span><span class="o">.</span><span class="n">T</span>
<span class="n">coefs</span><span class="o">.</span><span class="n">columns</span> <span class="o">=</span> <span class="n">cols</span>

<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span> <span class="mi">20</span><span class="p">))</span>
<span class="n">coefs</span><span class="o">.</span><span class="n">plot</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>    
</code></pre></div></div>

<p><img src="https://alexandrugris.github.io/assets/stats_1.png" alt="Lasso feature selection" /></p>

<p>Before finishing, here is how to automatically compute the <code class="highlighter-rouge">lambda</code> regularization parameter by using the Akaike Information Criterion and then by employing Cross Validation.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
<span class="s">"""
Using AIC
"""</span>

<span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">LassoLarsIC</span>
<span class="n">model_aic</span> <span class="o">=</span> <span class="n">LassoLarsIC</span><span class="p">(</span><span class="n">criterion</span><span class="o">=</span><span class="s">'aic'</span><span class="p">)</span>
<span class="n">model_aic</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">model_aic</span><span class="o">.</span><span class="n">alphas_</span><span class="p">,</span> <span class="n">model_aic</span><span class="o">.</span><span class="n">criterion_</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>

<span class="k">print</span><span class="p">(</span><span class="n">f</span><span class="s">"Selected lambda AIC = {model_aic.alpha_}"</span><span class="p">)</span>

<span class="s">"""
And with cross validation
"""</span>
<span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">LassoLarsCV</span>

<span class="n">model_cv</span> <span class="o">=</span> <span class="n">LassoLarsCV</span><span class="p">(</span><span class="n">cv</span><span class="o">=</span><span class="mi">20</span><span class="p">)</span>
<span class="n">model_cv</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">f</span><span class="s">"Selected lambda CV = {model_cv.alpha_}"</span><span class="p">)</span>
</code></pre></div></div>

<p>In a future post I will write more about feature selection for Machine Learning.</p>

  </div>

  
</article>

      </div>
    </main>

    <footer class="site-footer">

  <div class="wrapper">

    <h2 class="footer-heading">From The Trenches - The Code</h2>

    <div class="footer-col-wrapper">
      <div class="footer-col footer-col-1">
        <ul class="contact-list">
          <li>
            
              From The Trenches - The Code
            
            </li>
            
            <li><a href="mailto:alexandru.gris2006@gmail.com">alexandru.gris2006@gmail.com</a></li>
            
        </ul>
      </div>

      <div class="footer-col footer-col-2">
        <ul class="social-media-list">
          
          <li>
            <a href="https://github.com/alexandrugris"><span class="icon icon--github"><svg viewBox="0 0 16 16" width="16px" height="16px"><path fill="#828282" d="M7.999,0.431c-4.285,0-7.76,3.474-7.76,7.761 c0,3.428,2.223,6.337,5.307,7.363c0.388,0.071,0.53-0.168,0.53-0.374c0-0.184-0.007-0.672-0.01-1.32 c-2.159,0.469-2.614-1.04-2.614-1.04c-0.353-0.896-0.862-1.135-0.862-1.135c-0.705-0.481,0.053-0.472,0.053-0.472 c0.779,0.055,1.189,0.8,1.189,0.8c0.692,1.186,1.816,0.843,2.258,0.645c0.071-0.502,0.271-0.843,0.493-1.037 C4.86,11.425,3.049,10.76,3.049,7.786c0-0.847,0.302-1.54,0.799-2.082C3.768,5.507,3.501,4.718,3.924,3.65 c0,0,0.652-0.209,2.134,0.796C6.677,4.273,7.34,4.187,8,4.184c0.659,0.003,1.323,0.089,1.943,0.261 c1.482-1.004,2.132-0.796,2.132-0.796c0.423,1.068,0.157,1.857,0.077,2.054c0.497,0.542,0.798,1.235,0.798,2.082 c0,2.981-1.814,3.637-3.543,3.829c0.279,0.24,0.527,0.713,0.527,1.437c0,1.037-0.01,1.874-0.01,2.129 c0,0.208,0.14,0.449,0.534,0.373c3.081-1.028,5.302-3.935,5.302-7.362C15.76,3.906,12.285,0.431,7.999,0.431z"/></svg>
</span><span class="username">alexandrugris</span></a>

          </li>
          

          
          <li>
            <a href="https://twitter.com/alexandrugris"><span class="icon icon--twitter"><svg viewBox="0 0 16 16" width="16px" height="16px"><path fill="#828282" d="M15.969,3.058c-0.586,0.26-1.217,0.436-1.878,0.515c0.675-0.405,1.194-1.045,1.438-1.809c-0.632,0.375-1.332,0.647-2.076,0.793c-0.596-0.636-1.446-1.033-2.387-1.033c-1.806,0-3.27,1.464-3.27,3.27 c0,0.256,0.029,0.506,0.085,0.745C5.163,5.404,2.753,4.102,1.14,2.124C0.859,2.607,0.698,3.168,0.698,3.767 c0,1.134,0.577,2.135,1.455,2.722C1.616,6.472,1.112,6.325,0.671,6.08c0,0.014,0,0.027,0,0.041c0,1.584,1.127,2.906,2.623,3.206 C3.02,9.402,2.731,9.442,2.433,9.442c-0.211,0-0.416-0.021-0.615-0.059c0.416,1.299,1.624,2.245,3.055,2.271 c-1.119,0.877-2.529,1.4-4.061,1.4c-0.264,0-0.524-0.015-0.78-0.046c1.447,0.928,3.166,1.469,5.013,1.469 c6.015,0,9.304-4.983,9.304-9.304c0-0.142-0.003-0.283-0.009-0.423C14.976,4.29,15.531,3.714,15.969,3.058z"/></svg>
</span><span class="username">alexandrugris</span></a>

          </li>
          
        </ul>
      </div>

      <div class="footer-col footer-col-3">
        <p>Alexandru Gris - Personal Blog
</p>
      </div>
    </div>

  </div>

</footer>


  </body>

</html>
